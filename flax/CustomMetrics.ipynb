{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8JngLum3eKCUdV7nv1n9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenh17/jaxLearning/blob/main/flax/CustomMetrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUAfhJeb-vgD",
        "outputId": "4d816bff-343d-49ae-cb50-21c5ec410188",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Collecting flax\n",
            "  Downloading flax-0.10.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.9.tar.gz (1.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jax\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.4)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.68)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.4.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.10.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.87)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.87->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.21.0)\n",
            "Downloading flax-0.10.2-py3-none-any.whl (424 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.2/424.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: orbax\n",
            "  Building wheel for orbax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for orbax: filename=orbax-0.1.9-py3-none-any.whl size=1493 sha256=d6de1e1bc08e989b3adbd7b1cdd2ce93ae37f586af7b04dc3cc03c82c40b7e57\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/7a/98/b955a4db98b54317c311ee32367994ca530721c62a87ec56a7\n",
            "Successfully built orbax\n",
            "Installing collected packages: jaxlib, jax, orbax, flax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "Successfully installed flax-0.10.2 jax-0.4.35 jaxlib-0.4.35 orbax-0.1.9\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade flax orbax jax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from flax import nnx\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import orbax.checkpoint as ocp\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import io\n",
        "\n"
      ],
      "metadata": {
        "id": "JFO7NNqQ_Dtc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate a dataset with 100 samples, 10 features, 5 informative, 5 redundant, and 2 classes\n",
        "X, y = make_classification(n_samples=10000, n_features=10, n_informative=8,\n",
        "                          n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
        "df['Class'] = y\n",
        "csv_data = df.to_csv(index=True)\n"
      ],
      "metadata": {
        "id": "W4I2hnqUodtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Class for Custom  Data\n",
        "This section defines a CustomDataset class, which inherits from torch.utils.data.Dataset. It is designed to load and preprocess data from a CSV file for machine learning tasks.\n",
        "\n",
        "python\n",
        "Copy code\n"
      ],
      "metadata": {
        "id": "UJReXJOQliDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    # Initialize the dataset by loading, scaling, and organizing features and labels\n",
        "    def __init__(self, dataset, transform=None, target_transform=None):\n",
        "        # Load the CSV file into a DataFrame, using the first column as the index\n",
        "        completeDF = pd.read_csv(dataset, index_col=0)\n",
        "\n",
        "        # Extract the 'Class' column as labels\n",
        "        self.labels = pd.DataFrame(completeDF['Class'])\n",
        "\n",
        "        # Extract all other columns as features, excluding 'Class'\n",
        "        features_raw = pd.DataFrame(completeDF.drop('Class', axis=1))\n",
        "\n",
        "        # Scale the feature values to the range [0, 1] using Min-Max Scaling\n",
        "        scaler = MinMaxScaler()\n",
        "        self.features = pd.DataFrame(scaler.fit_transform(features_raw))\n",
        "\n",
        "        # Assign optional transformations for features and labels\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    # Return the total number of data samples in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    # Retrieve a specific sample by index and apply transformations if needed\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract feature values as a NumPy array for the given index\n",
        "        features = np.array(self.features.iloc[idx, :])\n",
        "\n",
        "        # Extract the corresponding label for the given index\n",
        "        label = self.labels.iloc[idx, 0]\n",
        "\n",
        "        # Apply transformation to the features, if provided\n",
        "        if self.transform:\n",
        "            features = self.transform(features)\n",
        "\n",
        "        # Apply transformation to the label, if provided\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        # Return the processed features and label\n",
        "        return features, label\n"
      ],
      "metadata": {
        "id": "ZBPFELboo4e1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"mssmartypants/rice-type-classification\")\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWv5r05xBvNk",
        "outputId": "e4bbdb93-9256-4f08-8444-be47e57d9caa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mssmartypants/rice-type-classification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 888k/888k [00:00<00:00, 63.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "/root/.cache/kagglehub/datasets/mssmartypants/rice-type-classification/versions/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /root/.cache/kagglehub/datasets/mssmartypants/rice-type-classification/versions/2/\n",
        "! mv /root/.cache/kagglehub/datasets/mssmartypants/rice-type-classification/versions/2/riceClassification.csv riceClassification.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXec3_GJByPc",
        "outputId": "b6379089-652a-4c0b-ed8a-3abed3ebaa92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "riceClassification.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = CustomImageDataset(dataset=io.StringIO(csv_data))\n",
        "dataset = CustomImageDataset(dataset=\"riceClassification.csv\")\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [.9, 0.05,0.05])\n",
        "data_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "for features, labels in data_loader:\n",
        "    print(\"Batch of features has shape: \",features.shape)\n",
        "    print(\"Batch of labels has shape: \", labels.shape)\n",
        "    print(features)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s3fFrv-otNR",
        "outputId": "8090b532-8e91-48bf-b7a4-1ce683487522"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of features has shape:  torch.Size([4, 10])\n",
            "Batch of labels has shape:  torch.Size([4])\n",
            "tensor([[0.9278, 0.8798, 0.7983, 0.7825, 0.8645, 0.9452, 0.7908, 0.6603, 0.7855,\n",
            "         0.3826],\n",
            "        [0.7616, 0.7653, 0.7139, 0.7690, 0.7097, 0.8127, 0.3696, 0.5730, 0.7834,\n",
            "         0.3655],\n",
            "        [0.7029, 0.6865, 0.7110, 0.7272, 0.6536, 0.7636, 0.3922, 0.5227, 0.8144,\n",
            "         0.3184],\n",
            "        [0.4820, 0.7577, 0.3588, 0.9218, 0.4533, 0.5646, 0.2687, 0.4809, 0.6520,\n",
            "         0.6560]], dtype=torch.float64)\n",
            "tensor([0, 0, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Implementation\n",
        "This section implements a customizable MLP model with dropout and batch normalization at each layer. The architecture is parameterized by input dimensions, hidden dimensions, and the number of classes"
      ],
      "metadata": {
        "id": "ZpJhAZGomiBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nnx.Module):\n",
        "    def __init__(self, din, dm1, dm2, dm3, num_classes: int, rngs: nnx.Rngs):\n",
        "        # Define dropout, linear, and batch normalization layers for each stage\n",
        "        self.dp1 = nnx.Dropout(rate=0.4, rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(din, dm1, rngs=rngs)\n",
        "        self.bn1 = nnx.BatchNorm(dm1, rngs=rngs)\n",
        "\n",
        "        self.dp2 = nnx.Dropout(rate=0.2, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(dm1, dm2, rngs=rngs)\n",
        "        self.bn2 = nnx.BatchNorm(dm2, rngs=rngs)\n",
        "\n",
        "        self.dp3 = nnx.Dropout(rate=0.1, rngs=rngs)\n",
        "        self.linear3 = nnx.Linear(dm2, dm3, rngs=rngs)\n",
        "        self.bn3 = nnx.BatchNorm(dm3, rngs=rngs)\n",
        "\n",
        "        # Output layer without batch normalization\n",
        "        self.linear4 = nnx.Linear(dm3, num_classes, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Apply dropout, linear transformation, activation, and batch normalization for each layer\n",
        "        x = self.dp1(x)\n",
        "        x = self.linear1(x)\n",
        "        x = nnx.gelu(x)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        x = self.dp2(x)\n",
        "        x = self.linear2(x)\n",
        "        x = nnx.gelu(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        x = self.dp3(x)\n",
        "        x = self.linear3(x)\n",
        "        x = nnx.gelu(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        # Final linear transformation\n",
        "        x = self.linear4(x)\n",
        "\n",
        "        # Sigmoid activation for binary classification (uncomment if needed)\n",
        "        # return nnx.sigmoid(x)\n",
        "        return x\n",
        "# Instantiate the model with given dimensions\n",
        "model = MLP(din=10, dm1=16, dm2=32, dm3=16, num_classes=1, rngs=nnx.Rngs(0))\n",
        "\n",
        "# Test the model with a sample input tensor of shape (3, 10)\n",
        "y = model(x=jnp.ones((3, 10)))\n",
        "\n",
        "# Display the model architecture and output\n",
        "nnx.display(model)\n",
        "nnx.display(y)\n",
        "\n",
        "# Output the predictions\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C2PTOR3_Abz",
        "outputId": "cb6fcdd3-305f-49aa-ffb3-c7813d25d3e8",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  dp1=Dropout(rate=0.4, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
            "    default=RngStream(\n",
            "      key=RngKey(\n",
            "        value=Array((), dtype=key<fry>) overlaying:\n",
            "        [0 0],\n",
            "        tag='default'\n",
            "      ),\n",
            "      count=RngCount(\n",
            "        value=Array(17, dtype=uint32),\n",
            "        tag='default'\n",
            "      )\n",
            "    )\n",
            "  )),\n",
            "  linear1=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(10, 16), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    in_features=10,\n",
            "    out_features=16,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7b65847f8430>,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    dot_general=<function dot_general at 0x7b6585a8bb50>\n",
            "  ),\n",
            "  bn1=BatchNorm(\n",
            "    mean=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    var=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    scale=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    num_features=16,\n",
            "    use_running_average=False,\n",
            "    axis=-1,\n",
            "    momentum=0.99,\n",
            "    epsilon=1e-05,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    use_bias=True,\n",
            "    use_scale=True,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    scale_init=<function ones at 0x7b6585156c20>,\n",
            "    axis_name=None,\n",
            "    axis_index_groups=None,\n",
            "    use_fast_variance=True\n",
            "  ),\n",
            "  dp2=Dropout(rate=0.2, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(...)),\n",
            "  linear2=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(16, 32), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    in_features=16,\n",
            "    out_features=32,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7b65847f8430>,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    dot_general=<function dot_general at 0x7b6585a8bb50>\n",
            "  ),\n",
            "  bn2=BatchNorm(\n",
            "    mean=BatchStat(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    var=BatchStat(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    scale=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    num_features=32,\n",
            "    use_running_average=False,\n",
            "    axis=-1,\n",
            "    momentum=0.99,\n",
            "    epsilon=1e-05,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    use_bias=True,\n",
            "    use_scale=True,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    scale_init=<function ones at 0x7b6585156c20>,\n",
            "    axis_name=None,\n",
            "    axis_index_groups=None,\n",
            "    use_fast_variance=True\n",
            "  ),\n",
            "  dp3=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(...)),\n",
            "  linear3=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(32, 16), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    in_features=32,\n",
            "    out_features=16,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7b65847f8430>,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    dot_general=<function dot_general at 0x7b6585a8bb50>\n",
            "  ),\n",
            "  bn3=BatchNorm(\n",
            "    mean=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    var=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    scale=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    num_features=16,\n",
            "    use_running_average=False,\n",
            "    axis=-1,\n",
            "    momentum=0.99,\n",
            "    epsilon=1e-05,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    use_bias=True,\n",
            "    use_scale=True,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    scale_init=<function ones at 0x7b6585156c20>,\n",
            "    axis_name=None,\n",
            "    axis_index_groups=None,\n",
            "    use_fast_variance=True\n",
            "  ),\n",
            "  linear4=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(16, 1), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array([0.], dtype=float32)\n",
            "    ),\n",
            "    in_features=16,\n",
            "    out_features=1,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7b65847f8430>,\n",
            "    bias_init=<function zeros at 0x7b6585156a70>,\n",
            "    dot_general=<function dot_general at 0x7b6585a8bb50>\n",
            "  )\n",
            ")\n",
            "[[-0.46538347]\n",
            " [-0.11259228]\n",
            " [ 0.57797575]]\n",
            "[[-0.46538347]\n",
            " [-0.11259228]\n",
            " [ 0.57797575]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Metrics"
      ],
      "metadata": {
        "id": "1Mxqo-0MoVSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomMetrics(nnx.metrics.Metric):\n",
        "    def __init__(self):\n",
        "        # Initialize counters for true positives, false positives, and false negatives\n",
        "        self.true_positives = 0\n",
        "        self.false_positives = 0\n",
        "        self.false_negatives = 0\n",
        "\n",
        "    def update(self, loss, logits, labels):\n",
        "        \"\"\"\n",
        "        Update the metric counters based on the predictions and labels.\n",
        "        Assumes logits are probabilities (e.g., from a sigmoid activation).\n",
        "        \"\"\"\n",
        "        # Convert logits to binary predictions\n",
        "        predictions = jnp.where(jnp.array(logits) > 0.5, 1, 0)\n",
        "\n",
        "        predictions = predictions.ravel()\n",
        "        labels = jnp.array(labels).ravel()\n",
        "\n",
        "\n",
        "        # Compute metrics\n",
        "        tp = jnp.sum((labels == 1) & (predictions == 1))\n",
        "        fp = jnp.sum((labels == 0) & (predictions == 1))\n",
        "        fn = jnp.sum((labels == 1) & (predictions == 0))\n",
        "\n",
        "        # Update counters\n",
        "        self.true_positives += tp\n",
        "        self.false_positives += fp\n",
        "        self.false_negatives += fn\n",
        "\n",
        "    def compute(self):\n",
        "        \"\"\"\n",
        "        Compute precision, recall, and F1-score from the accumulated counters.\n",
        "        \"\"\"\n",
        "        precision = self.true_positives / (self.true_positives + self.false_positives + 1e-7)\n",
        "        recall = self.true_positives / (self.true_positives + self.false_negatives + 1e-7)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
        "        return {\"f1_score\": f1_score, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the metric counters.\n",
        "        \"\"\"\n",
        "        self.true_positives = 0\n",
        "        self.false_positives = 0 #jnp.array(0)\n",
        "        self.false_negatives = 0\n"
      ],
      "metadata": {
        "id": "mLuplI5goY_D"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAccuracy(nnx.metrics.Metric):\n",
        "    def __init__(self):\n",
        "        self.correct_count= nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))\n",
        "        self.total_count= nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))\n",
        "\n",
        "    def update(self, loss, logits, labels):\n",
        "        # Convert logits to binary predictions (0 or 1) based on a 0.5 threshold\n",
        "        predictions = jnp.where(jnp.array(logits) > 0.5, 1, 0)\n",
        "        # Flatten if necessary\n",
        "        predictions = predictions.ravel()\n",
        "        labels = jnp.array(labels).ravel()\n",
        "\n",
        "        # Calculate number of correct predictions in the current batch\n",
        "        self.correct_count += jnp.sum(predictions == labels)\n",
        "        self.total_count += len(labels)\n",
        "\n",
        "    def compute(self):\n",
        "        # Calculate accuracy over all batches seen so far\n",
        "        if self.total_count == 0:\n",
        "            return 0  # Avoid division by zero if no samples are seen\n",
        "        return self.correct_count / self.total_count\n",
        "    def reset(self):\n",
        "        # Reset counters\n",
        "        self.correct_count = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))\n",
        "        self.total_count = nnx.metrics.MetricState(jnp.array(0, dtype=jnp.int32))"
      ],
      "metadata": {
        "id": "B1kVQ_DFIQQh"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test class"
      ],
      "metadata": {
        "id": "nlXFKOWbps1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = CustomMetrics()\n",
        "logits = jnp.array([0.9, 0.7, 0.2, 0.4, 0.8, 0.1])\n",
        "labels = jnp.array([1, 1, 0, 0, 1, 0])\n",
        "\n",
        "metric.update(None, logits, labels)\n",
        "metrics = metric.compute()\n",
        "\n",
        "print(metrics[\"precision\"], 1)\n",
        "print(metrics[\"recall\"], 1)\n",
        "print(metrics[\"f1_score\"], 1)\n",
        "\n",
        "metric.reset()\n",
        "\n",
        "logits = jnp.array([0.9, 0.7, 0.2, 0.8, 0.1, 0.3])\n",
        "labels = jnp.array([1, 0, 1, 1, 0, 0])\n",
        "\n",
        "# Expected binary predictions: [1, 1, 0, 1, 0, 0]\n",
        "# TP = 2 (Correct positive predictions: [1, 1])\n",
        "# FP = 1 (False positive prediction: [1])\n",
        "# FN = 1 (False negative prediction: [1])\n",
        "metric.update(None, logits, labels)\n",
        "metrics = metric.compute()\n",
        "\n",
        "expected_precision = 2 / (2 + 1)  # TP / (TP + FP)\n",
        "expected_recall = 2 / (2 + 1)     # TP / (TP + FN)\n",
        "expected_f1 = 2 * (expected_precision * expected_recall) / (expected_precision + expected_recall)\n",
        "\n",
        "print(metrics[\"precision\"], expected_precision)\n",
        "print(metrics[\"recall\"], expected_recall)\n",
        "print(metrics[\"f1_score\"], expected_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlUoD37jprj3",
        "outputId": "aa2f93c1-2ebf-4ec4-cafe-318a23f3b934"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 1\n",
            "1.0 1\n",
            "1.0 1\n",
            "0.6666667 0.6666666666666666\n",
            "0.6666667 0.6666666666666666\n",
            "0.6666666 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up an Optimizer and Metrics"
      ],
      "metadata": {
        "id": "BFME6UD3m88I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Optax for optimizer configuration\n",
        "import optax\n",
        "\n",
        "# Learning rate and momentum for the optimizer\n",
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "\n",
        "# Instantiate the MLP model\n",
        "model = MLP(10, 16, 32, 16, 1, rngs=nnx.Rngs(0))\n",
        "\n",
        "# Set up the optimizer using Adam with the specified learning rate\n",
        "optimizer = nnx.Optimizer(model, optax.adam(learning_rate))\n",
        "\n",
        "# Initialize metrics for tracking training performance\n",
        "metrics = nnx.MultiMetric(\n",
        "    accuracy=CustomAccuracy(),\n",
        "    loss=nnx.metrics.Average('loss'),  # Tracks the average loss\n",
        ")\n"
      ],
      "metadata": {
        "id": "ASj5AcVZqpXN"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Defining Loss, Training, and Evaluation Functions"
      ],
      "metadata": {
        "id": "thvdNah4nV8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model: MLP, batch):\n",
        "    # Forward pass: Compute logits (raw predictions) using the model\n",
        "    logits = model(batch['features'])\n",
        "\n",
        "    # Compute binary cross-entropy loss for classification\n",
        "    loss = optax.sigmoid_binary_cross_entropy(\n",
        "        logits=logits, labels=batch['labels'].reshape(-1, 1)\n",
        "    ).mean()\n",
        "\n",
        "    # Optionally, use a custom loss function like mean squared error\n",
        "    # loss = (logits - batch['labels'])**2\n",
        "\n",
        "    # Return the computed loss and logits\n",
        "    return loss, logits\n",
        "@nnx.jit\n",
        "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    \"\"\"Train for a single step.\"\"\"\n",
        "    # Compute loss and gradients using a differentiable function\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "\n",
        "    # Update metrics in-place with loss and logits\n",
        "    metrics.update(loss=loss, logits=logits, labels=batch['labels'])\n",
        "\n",
        "    # Apply the computed gradients to update model parameters\n",
        "    optimizer.update(grads)\n",
        "\n",
        "    # Return the loss and logits after applying sigmoid for interpretation\n",
        "    return loss, nnx.sigmoid(logits)\n",
        "@nnx.jit\n",
        "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
        "    # Compute loss and logits for the batch\n",
        "    loss, logits = loss_fn(model, batch)\n",
        "\n",
        "    # Update metrics in-place with evaluation results\n",
        "    metrics.update(loss=loss, logits=logits, labels=batch['labels'])\n",
        "\n",
        "    # Return the loss for further aggregation or monitoring\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "HsPjwULJq8GS"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation and DataLoader Creation"
      ],
      "metadata": {
        "id": "p6jJmprenxrn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    # Transpose batch to group features and labels separately\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    # Convert labels and features into NumPy arrays\n",
        "    labels = np.array(transposed_data[1])\n",
        "    features = np.array(transposed_data[0])\n",
        "\n",
        "    # Return a dictionary with features and labels\n",
        "    return {\"features\": features, \"labels\": labels}\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.7, 0.1, 0.2])\n",
        "\n",
        "train_ds = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True, collate_fn=custom_collate_fn)\n",
        "val_ds = DataLoader(val_set, batch_size=4, shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
        "test_ds = DataLoader(test_set, batch_size=4, shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Fetch a single batch from the training DataLoader\n",
        "batch_data = next(iter(train_ds))\n",
        "imgs = batch_data['features']\n",
        "lbls = batch_data['labels']\n",
        "\n",
        "# Print shapes and data types for verification\n",
        "print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)\n",
        "print(lbls)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fioF1mHarC8_",
        "outputId": "0f4ef03c-6114-4298-c3d2-1a0df655e75a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10) float64 (64,) int64\n",
            "[0 1 1 0 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1\n",
            " 1 0 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop for Neural Network with Metrics Tracking"
      ],
      "metadata": {
        "id": "HN4uqJhHok23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "#model.train()\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'train_accuracy': [],\n",
        "  'train_precision': [],\n",
        "  'train_recall': [],\n",
        "  'train_f1': [],\n",
        "  'val_loss': [],\n",
        "  'val_accuracy': [],\n",
        "  'val_precision': [],\n",
        "  'val_recall': [],\n",
        "  'val_f1': [],\n",
        "  'test_loss': [],\n",
        "  'test_accuracy': [],\n",
        "  'test_precision': [],\n",
        "  'test_recall': [],\n",
        "  'test_f1': [],\n",
        "}\n",
        "\n",
        "num_epochs = 40  # Number of training epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Train on each batch in the training dataset\n",
        "    for batch in train_ds:\n",
        "        loss, logits = train_step(model, optimizer, metrics, batch)\n",
        "\n",
        "    # Compute and log training metrics for this epoch\n",
        "    for metric, value in metrics.compute().items():\n",
        "        metrics_history[f'train_{metric}'].append(value)\n",
        "\n",
        "    # Reset metrics for the next epoch\n",
        "    metrics.reset()\n",
        "\n",
        "    # Log training performance\n",
        "    print(\n",
        "        f\"[train] epoch: {epoch + 1}/{num_epochs}, \"\n",
        "        f\"loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
        "        f\"accuracy: {metrics_history['train_accuracy'][-1]:.4f}, \"\n",
        "    )\n",
        "\n",
        "# later stuff\n",
        "\"\"\"\n",
        "for batch in val_ds:\n",
        "    loss = eval_step(model, metrics, batch)\n",
        "\n",
        "for metric, value in metrics.compute().items():\n",
        "    metrics_history[f'val_{metric}'].append(value)\n",
        "\n",
        "metrics.reset()\n",
        "print(\n",
        "    f\"[val] epoch: {epoch + 1}/{num_epochs}, \"\n",
        "    f\"loss: {metrics_history['val_loss'][-1]:.4f}, \"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"[train] epoch: {epoch + 1}/{num_epochs}, \"\n",
        "    f\"loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
        "    f\"accuracy: {metrics_history['train_accuracy'][-1]:.2f}\"\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "g_cxxkNtrH0O",
        "outputId": "28450e53-a041-4f00-ce86-dab41ad02835"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] epoch: 1/40, loss: 0.3668, accuracy: 0.8147, \n",
            "[train] epoch: 2/40, loss: 0.1872, accuracy: 0.9179, \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-14ff9f2f88f5>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Train on each batch in the training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6c07f6b505a1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Extract feature values as a NumPy array for the given index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Extract the corresponding label for the given index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1063\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1754\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3998\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3999\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4000\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4001\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4002\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_constructor_sliced_from_mgr\u001b[0;34m(self, mgr, axes)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_constructor_sliced_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# caller is responsible for setting real name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_from_mgr\u001b[0;34m(cls, mgr, axes)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mevent\u001b[0m \u001b[0mthat\u001b[0m \u001b[0maxes\u001b[0m \u001b[0mare\u001b[0m \u001b[0mrefactored\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mManager\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction and Evaluation Pipeline"
      ],
      "metadata": {
        "id": "L0iX4KVcqTfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def pred_step(model: MLP, batch):\n",
        "    logits = model(batch['features'])\n",
        "    return nnx.sigmoid(logits)\n",
        "\n",
        "test_ds = DataLoader(test_set, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "ypred = []  # List to store predicted probabilities\n",
        "label = []  # List to store true labels\n",
        "\n",
        "for test_batch in test_ds:\n",
        "    logits = pred_step(model, test_batch)\n",
        "    ypred.extend(np.ravel(logits))  # Flatten and collect predictions\n",
        "    label.extend(np.ravel(test_batch[\"labels\"]))  # Flatten and collect true labels\n",
        "\n",
        "binary_ypred = np.where(np.array(ypred) > 0.5, 1, 0)\n",
        "\n",
        "accuracy = sum([1 for pred, true in zip(binary_ypred, label) if pred == true]) / len(label)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JdDFipvrj4L",
        "outputId": "c6525ca3-74ed-497e-9914-6520e040551a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint! save model"
      ],
      "metadata": {
        "id": "U4eF72y2q150"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from orbax.checkpoint.type_handlers import TypeHandler\n",
        "from orbax.checkpoint.type_handlers import register_type_handler\n",
        "from copy import deepcopy\n",
        "\n",
        "state = nnx.state(model)  # Retrieve the model's state (parameters, PRNGs, etc.)\n",
        "_, state = nnx.split(model)  # Split the model into parameters and state (useful for checkpoints)\n",
        "stateOrg = deepcopy(state)  # Deep copy the state to avoid modifying the original state\n",
        "nnx.display(state)  # Display the current state (for inspection)\n",
        "\n",
        "prng_key_value = state[\"dp1\"][\"rngs\"][\"default\"][\"key\"].value  # Access the PRNG key for dp1\n",
        "state[\"dp1\"][\"rngs\"][\"default\"][\"key\"] = nnx.VariableState(\n",
        "    type=nnx.Param, value=jax.random.key_data(prng_key_value), tag='default'\n",
        ")  # Modify the PRNG key for dp1\n",
        "\n",
        "ckpt_dir = ocp.test_utils.erase_and_create_empty('/content/my-checkpoints/')  # Erase and create a new empty checkpoint directory\n",
        "checkpointer = ocp.PyTreeCheckpointer()  # Initialize the checkpointing system\n",
        "checkpointer.save('/content/my-checkpoints/state', state)  # Save the modified state to the checkpoint directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jeAzhDPJYSIp",
        "outputId": "8e43d8c7-4376-4cd6-df48-541d976a42ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State({\n",
            "  'bn1': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.00725206,  0.23271044,  0.33379307, -0.10293782,  0.16999696,\n",
            "             -0.02470776,  0.1803976 ,  0.3354114 ,  0.38076717,  0.7533252 ,\n",
            "              0.31849438, -0.16884422, -0.18173328,  0.63423645,  0.6223282 ,\n",
            "              0.29654378], dtype=float32)\n",
            "    ),\n",
            "    'mean': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([-0.06623889, -0.06290819,  0.072702  , -0.02497913, -0.05046811,\n",
            "              0.04938643,  0.4643559 ,  1.2697079 ,  0.07811453,  0.16194636,\n",
            "              1.714218  ,  0.02059288, -0.06510378,  1.6044323 ,  0.18827136,\n",
            "              0.8350833 ], dtype=float32)\n",
            "    ),\n",
            "    'scale': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([1.0840424 , 0.7646256 , 1.3338696 , 0.6091287 , 1.1535928 ,\n",
            "             1.1255418 , 1.7002821 , 1.2452723 , 1.2851168 , 1.99684   ,\n",
            "             1.1272197 , 1.0468231 , 0.76659524, 0.83290005, 1.5162948 ,\n",
            "             1.0240717 ], dtype=float32)\n",
            "    ),\n",
            "    'var': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([0.00258621, 0.00174741, 0.0654055 , 0.00212151, 0.00278427,\n",
            "             0.01876172, 0.6959272 , 1.5372598 , 0.09440131, 0.1894538 ,\n",
            "             1.4516977 , 0.01520831, 0.00283754, 1.4974139 , 0.20506674,\n",
            "             0.7845007 ], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'bn2': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.2734482 , -0.20424846,  0.2273801 ,  0.29812312,  0.45474792,\n",
            "              0.10120319, -0.02791155,  0.18237743,  0.46708924,  0.17185016,\n",
            "              0.09426275, -0.7207849 , -0.524336  , -0.00307447, -0.25506517,\n",
            "             -0.08624966,  0.43810502, -0.12513179, -0.12127531,  0.264251  ,\n",
            "              0.3581237 ,  0.45515606,  0.17919037,  0.13200226,  0.62273306,\n",
            "              0.00121778,  0.24922419,  0.3958186 , -0.86991197, -0.00815355,\n",
            "              0.53813916,  0.07604666], dtype=float32)\n",
            "    ),\n",
            "    'mean': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([0.9900592 , 0.31843528, 1.0849493 , 1.2139533 , 1.852126  ,\n",
            "             1.0009613 , 1.4924536 , 0.6492783 , 2.5459602 , 1.1670551 ,\n",
            "             1.6711509 , 2.476072  , 1.7111752 , 1.1960285 , 0.5323073 ,\n",
            "             0.7195688 , 1.4841663 , 2.7187636 , 1.3386242 , 2.6470072 ,\n",
            "             1.9365801 , 1.4165573 , 1.4188936 , 2.2764912 , 1.9749904 ,\n",
            "             1.8562994 , 2.1073987 , 3.5426917 , 3.4391873 , 2.1366374 ,\n",
            "             2.2183316 , 2.0100331 ], dtype=float32)\n",
            "    ),\n",
            "    'scale': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([1.3239793 , 1.3478454 , 1.5106597 , 1.229434  , 1.4142108 ,\n",
            "             1.3625079 , 1.0306854 , 1.2635882 , 1.2933959 , 1.4380144 ,\n",
            "             1.2953155 , 1.5605978 , 0.77518684, 1.3008138 , 1.0611371 ,\n",
            "             0.90571326, 1.1193839 , 1.2917702 , 1.1102585 , 1.1745013 ,\n",
            "             1.1593901 , 1.3237327 , 1.2469317 , 1.1633134 , 1.4325724 ,\n",
            "             1.0112848 , 1.2891715 , 1.201534  , 1.3482783 , 1.2667085 ,\n",
            "             1.4486701 , 1.1546135 ], dtype=float32)\n",
            "    ),\n",
            "    'var': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([ 2.7160432,  0.6812064,  3.2048092,  6.9554   , 11.025535 ,\n",
            "              5.0568614,  6.6872387,  1.5655491, 11.0934   ,  4.6113877,\n",
            "              8.006646 ,  5.736517 ,  3.7042866,  9.480041 ,  1.2734028,\n",
            "              2.043938 ,  8.32312  , 14.517059 ,  3.7600493, 15.777767 ,\n",
            "              8.210551 ,  6.370378 ,  5.222251 , 12.173861 , 13.278882 ,\n",
            "              9.681715 ,  8.502867 , 37.137745 ,  9.166534 ,  9.777032 ,\n",
            "             12.7474   ,  9.966864 ], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'bn3': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([0.06658668, 0.1776781 , 0.07701101, 0.15727554, 0.16311063,\n",
            "             0.0376824 , 0.24992977, 0.049815  , 0.26160982, 0.1489026 ,\n",
            "             0.10644737, 0.13533807, 0.16280818, 0.20748907, 0.16481046,\n",
            "             0.24201003], dtype=float32)\n",
            "    ),\n",
            "    'mean': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([2.3660097, 1.3547657, 6.651172 , 2.77342  , 2.6797843, 2.553579 ,\n",
            "             1.3552355, 2.3446763, 2.6200354, 1.6965499, 6.4862585, 2.9681773,\n",
            "             3.3838015, 3.0536153, 2.1094546, 4.78101  ], dtype=float32)\n",
            "    ),\n",
            "    'scale': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([1.496329  , 1.6496295 , 1.9240032 , 1.2012777 , 1.1232064 ,\n",
            "             1.263517  , 1.5588706 , 0.81375676, 1.7514136 , 1.0397816 ,\n",
            "             1.4534097 , 1.6515164 , 1.4873545 , 1.8458147 , 1.2225394 ,\n",
            "             1.7677306 ], dtype=float32)\n",
            "    ),\n",
            "    'var': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([10.966568 , 13.429475 , 81.33854  , 20.26948  , 19.950783 ,\n",
            "             23.704803 ,  8.973683 , 10.786844 , 21.793455 , 15.850107 ,\n",
            "             46.76792  , 31.430948 , 32.520252 , 21.40776  ,  9.7975645,\n",
            "             37.73215  ], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'dp1': {\n",
            "    'rngs': {\n",
            "      'default': {\n",
            "        'count': VariableState(\n",
            "          type=RngCount,\n",
            "          value=Array(49574, dtype=uint32),\n",
            "          tag='default'\n",
            "        ),\n",
            "        'key': VariableState(\n",
            "          type=RngKey,\n",
            "          value=Array((), dtype=key<fry>) overlaying:\n",
            "          [0 0],\n",
            "          tag='default'\n",
            "        )\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  'linear1': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.04356459, -0.18516815, -1.9319565 ,  0.04788419, -0.20270282,\n",
            "             -2.600933  , -2.0476656 ,  1.1484954 , -1.8780309 , -1.9383091 ,\n",
            "             -0.07272667, -2.6124706 , -0.45336953,  2.7053208 , -1.9011153 ,\n",
            "              1.0150479 ], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 3.82315628e-02,  1.36259580e-02, -1.05955921e-01,\n",
            "              -2.60965204e+00, -9.76753607e-02,  6.90265670e-02,\n",
            "               1.48807725e-04, -1.18319243e-01, -8.11690688e-02,\n",
            "              -3.78066003e-02, -4.73917909e-02,  1.79973617e-02,\n",
            "              -2.64736675e-02,  1.29721820e-01,  2.38168144e+00,\n",
            "              -2.02238902e-01],\n",
            "             [ 3.53830615e-06,  6.08984455e-02, -1.64239593e-02,\n",
            "               7.39997253e-02, -9.07388516e-03, -2.54820473e-02,\n",
            "               1.93559881e-02, -4.23077717e-02, -6.07034080e-02,\n",
            "               5.91667481e-02, -4.64961119e-02, -3.12203038e-02,\n",
            "               1.09748458e-02, -4.33537550e-02, -6.82592690e-02,\n",
            "               1.74881686e-02],\n",
            "             [ 4.79375664e-03, -7.32985288e-02, -1.60693422e-01,\n",
            "              -2.32386780e+00, -2.69750450e-02,  6.59193238e-03,\n",
            "               3.03814650e+00, -1.63276941e-02, -1.93076760e-01,\n",
            "               2.36947322e+00,  1.91219604e+00, -5.96273839e-02,\n",
            "              -1.23255095e-02,  1.29746377e-01, -2.30500698e-01,\n",
            "               1.56796360e+00],\n",
            "             [-1.85300186e-01, -1.67677291e-02,  1.10317171e-01,\n",
            "              -1.70872267e-02, -5.10374382e-02,  1.97329903e+00,\n",
            "              -3.74234840e-02,  1.28030377e-02,  1.20500326e-01,\n",
            "              -9.38072102e-04,  1.11032891e+00,  1.98791111e+00,\n",
            "              -1.25418305e+00,  8.34659040e-02,  1.26496732e-01,\n",
            "              -8.21379185e-01],\n",
            "             [ 3.27922478e-02, -2.18011260e+00, -6.95432276e-02,\n",
            "              -3.12036239e-02,  2.75275614e-02, -2.90769860e-02,\n",
            "               4.38816026e-02,  2.02913240e-01,  2.27117538e+00,\n",
            "              -6.95235804e-02,  3.37888598e-02, -4.17792462e-02,\n",
            "              -5.12551926e-02,  8.76427963e-02, -5.41664995e-02,\n",
            "              -7.91884065e-02],\n",
            "             [-5.91099486e-02, -8.99893567e-02,  1.96044457e+00,\n",
            "              -1.17174372e-01, -1.35950196e+00,  2.07667402e-03,\n",
            "              -2.58961376e-02,  1.66285419e+00, -8.54520500e-02,\n",
            "               2.51918584e-02, -6.88065514e-02, -2.63709873e-02,\n",
            "               3.75620089e-02, -2.77455021e-02,  1.81812830e-02,\n",
            "              -7.21550062e-02],\n",
            "             [-5.58137745e-02, -2.50146855e-02,  5.97205572e-02,\n",
            "               4.71110083e-02, -2.08543167e-02,  4.14746925e-02,\n",
            "               1.39782295e-01, -1.33713065e-02,  9.29603279e-02,\n",
            "               2.64136307e-02, -4.18166891e-02,  2.21328791e-02,\n",
            "              -3.85719240e-02, -1.68622017e-03,  6.37530312e-02,\n",
            "              -5.09184971e-02],\n",
            "             [ 2.41540968e-02,  5.18928021e-02, -7.91338235e-02,\n",
            "               8.41528773e-02, -5.89121170e-02,  1.03458464e-02,\n",
            "              -8.09881277e-03,  8.13971460e-03, -1.13904677e-01,\n",
            "              -6.25336692e-02,  5.54989986e-02, -3.73355299e-02,\n",
            "              -1.57999750e-02,  3.19701880e-02, -1.96445674e-01,\n",
            "              -4.27494710e-03],\n",
            "             [ 2.03371253e-02, -7.70132538e-05,  1.18351728e-02,\n",
            "              -3.94776128e-02, -7.25014657e-02,  3.06905224e-03,\n",
            "               4.78501320e-02,  2.91805714e-02,  4.21889424e-02,\n",
            "               1.09952092e-02,  2.58200783e-02, -7.03320093e-03,\n",
            "              -3.37831676e-02,  3.77530009e-02,  6.76558912e-02,\n",
            "              -4.55425680e-03],\n",
            "             [-2.26432824e+00,  1.71458237e-02,  6.13972768e-02,\n",
            "              -1.12069502e-01, -2.05819893e+00, -5.31100482e-02,\n",
            "               8.56482890e-03, -2.26862884e+00, -3.90494168e-02,\n",
            "              -2.62104310e-02, -1.03752844e-01, -3.66764553e-02,\n",
            "               4.96737093e-01, -2.79855847e+00, -4.43820246e-02,\n",
            "              -2.67824560e-01]], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'linear2': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.19738197, -1.5538434 , -0.30284053, -0.25650272, -0.51091284,\n",
            "             -0.8474912 ,  0.38853577, -0.60373354, -0.02563347, -0.9821146 ,\n",
            "             -0.11046591,  1.8898432 ,  1.4789627 , -0.62982947, -1.0760281 ,\n",
            "             -0.08576827, -0.485173  ,  1.0476723 ,  0.34953922, -0.29376155,\n",
            "              1.4921119 , -0.9415802 , -0.5803366 ,  0.3478054 , -0.39766416,\n",
            "              0.675403  ,  0.6111003 ,  0.39386255,  3.095503  ,  1.1656095 ,\n",
            "             -0.59270644,  1.2245536 ], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 1.71038580e+00, -7.71369338e-02,  1.60943627e-01,\n",
            "              -3.57483059e-01,  2.08217070e-01,  2.66679078e-01,\n",
            "              -1.12836219e-01,  6.49533033e-01,  2.21381728e-02,\n",
            "              -3.05328697e-01, -3.91700953e-01,  1.36478066e-01,\n",
            "               6.75452799e-02, -7.13425055e-02, -9.61930007e-02,\n",
            "              -2.02454543e+00,  3.46050948e-01, -7.74480999e-01,\n",
            "               8.39543521e-01, -2.54160523e-01,  1.04656565e+00,\n",
            "              -2.35181488e-02, -2.10452676e+00,  1.47689670e-01,\n",
            "               2.49759644e-01, -2.04597756e-01, -9.78301823e-01,\n",
            "              -1.63832635e-01,  1.53857753e-01,  8.43748674e-02,\n",
            "               1.26792133e-01,  5.81416078e-02],\n",
            "             [ 5.64532936e-01, -1.52862036e+00,  6.86894134e-02,\n",
            "              -2.71159802e-02, -8.17067456e-03,  9.00873780e-01,\n",
            "               8.29207659e-01,  2.15426669e-01,  3.90264690e-01,\n",
            "               7.16253579e-01,  8.90262306e-01, -2.75278032e-01,\n",
            "              -3.66426587e-01, -2.76460558e-01, -1.23590863e+00,\n",
            "               3.03850502e-01, -3.08664352e-01,  6.98068738e-01,\n",
            "               1.24394469e-01,  5.50094128e-01,  6.05662391e-02,\n",
            "              -1.34521618e-01, -2.84444034e-01,  5.74291825e-01,\n",
            "               2.30300769e-01,  5.20099178e-02, -1.23368308e-01,\n",
            "              -1.65944934e-01, -2.14409471e-01,  7.72360563e-02,\n",
            "               5.71648404e-02, -4.65172231e-02],\n",
            "             [ 2.66216904e-01, -3.97762418e-01,  6.60122335e-01,\n",
            "              -6.43437088e-01, -6.62942290e-01,  4.11858261e-01,\n",
            "              -1.42586648e-01,  9.35918540e-02,  3.45404059e-01,\n",
            "               5.67248523e-01,  6.59983993e-01, -4.01548147e-01,\n",
            "              -1.76524377e+00,  2.40411505e-01,  3.59632671e-01,\n",
            "               4.68988419e-01, -8.61667633e-01,  9.36801076e-01,\n",
            "               9.72026885e-02,  8.60586703e-01, -3.32814693e-01,\n",
            "              -1.19755335e-01, -6.39602900e-01,  2.86627293e-01,\n",
            "              -5.42092741e-01,  4.17941883e-02,  1.19687915e+00,\n",
            "              -7.65136257e-02, -1.20540309e+00, -4.33980942e-01,\n",
            "               8.98998857e-01,  1.39104605e-01],\n",
            "             [ 2.56222099e-01,  1.15902446e-01,  9.33591723e-02,\n",
            "               4.81814951e-01,  4.98213589e-01,  7.20861971e-01,\n",
            "               5.78202546e-01,  4.11167890e-01,  5.27802646e-01,\n",
            "               7.67217278e-01,  7.26569474e-01, -5.79845130e-01,\n",
            "               6.29587233e-01, -4.34185751e-02, -1.04182756e+00,\n",
            "               3.43599141e-01,  4.48576137e-02,  7.39619970e-01,\n",
            "               2.50202864e-02,  1.73169479e-01, -4.81784016e-01,\n",
            "               4.43099409e-01, -2.17584088e-01,  6.47018194e-01,\n",
            "               5.91046453e-01,  1.18341111e-01,  1.12671494e-01,\n",
            "               3.55941266e-01, -7.69949138e-01, -1.57622218e-01,\n",
            "               6.64219772e-03,  4.32652801e-01],\n",
            "             [-5.56642234e-01, -1.90206274e-01,  2.05344939e+00,\n",
            "              -6.88870132e-01,  2.91182429e-01,  1.45999128e-02,\n",
            "               5.59645183e-02,  6.26636684e-01,  4.53752339e-01,\n",
            "              -2.75147315e-02, -8.20035338e-01, -9.13218781e-02,\n",
            "              -5.42503119e-01, -9.86506790e-02, -4.39345017e-02,\n",
            "              -7.13343322e-01, -9.12315547e-02,  7.15626180e-01,\n",
            "               9.13412273e-01,  8.91777039e-01,  5.35522163e-01,\n",
            "              -2.24230677e-01,  4.28569943e-01, -8.92960906e-01,\n",
            "               2.06264585e-01, -3.81259948e-01,  8.85105252e-01,\n",
            "              -4.34117913e-01, -1.14010528e-01, -2.49284014e-01,\n",
            "               1.71000016e+00, -9.43198148e-03],\n",
            "             [ 3.55111748e-01,  2.59469599e-01, -4.41195965e-01,\n",
            "               1.27563703e+00, -1.31663326e-02, -1.38329595e-01,\n",
            "              -5.35958052e-01, -2.03960881e-01, -9.31432724e-01,\n",
            "              -1.56378835e-01, -2.84096479e-01, -2.89771259e-01,\n",
            "              -1.50360316e-01,  5.38835712e-02, -3.02028209e-01,\n",
            "              -6.74457312e-01, -8.80595669e-02, -1.07225299e+00,\n",
            "              -4.81261253e-01, -5.25242805e-01,  5.33951283e-01,\n",
            "               2.42172956e-01, -1.53950143e+00, -5.84154308e-01,\n",
            "              -3.94082755e-01,  1.33932805e+00,  2.09994882e-01,\n",
            "               4.53349650e-01, -2.62229890e-01,  1.34000564e+00,\n",
            "              -3.43763471e-01,  1.30024672e+00],\n",
            "             [-1.47680432e-01,  8.29483420e-02, -5.15396073e-02,\n",
            "              -8.55201203e-03,  1.16729510e+00, -9.27725017e-01,\n",
            "              -4.37634997e-02,  3.46411616e-01,  2.78033525e-01,\n",
            "              -1.64806056e+00,  3.36274326e-01, -9.02261794e-01,\n",
            "              -5.05803943e-01, -1.30261123e-01, -1.80240333e-01,\n",
            "              -1.66687593e-01,  5.00442088e-01, -1.57789171e-01,\n",
            "               1.97880864e-01,  3.99642348e-01, -3.28012675e-01,\n",
            "               9.82825398e-01, -1.56584214e-02,  4.15058523e-01,\n",
            "               1.45368290e+00, -3.59733224e-01, -2.97353268e-01,\n",
            "               1.84457946e+00, -8.05631697e-01, -3.73598970e-02,\n",
            "              -1.41735524e-01, -3.41112256e-01],\n",
            "             [ 4.74489510e-01,  1.04410239e-01,  4.88378167e-01,\n",
            "               7.05277264e-01,  2.77808160e-02, -1.12631202e+00,\n",
            "               5.81985451e-02, -7.01517045e-01,  1.21040845e+00,\n",
            "               2.26994663e-01, -1.62447393e+00,  2.67201271e-02,\n",
            "              -4.92141962e-01, -5.30651867e-01, -1.82639048e-01,\n",
            "              -4.01337534e-01, -3.84821333e-02,  2.80341506e-01,\n",
            "              -1.23011887e+00,  1.52469444e+00, -7.53987491e-01,\n",
            "               6.19786978e-02,  1.20665133e-01, -1.10623717e+00,\n",
            "              -7.26172552e-02,  2.43750006e-01,  9.34386969e-01,\n",
            "               2.66609341e-02,  7.79392198e-02,  7.60600120e-02,\n",
            "               3.91472280e-01,  1.52543485e-01],\n",
            "             [-4.74998355e-01, -2.35506490e-01, -1.44207388e-01,\n",
            "              -2.59915441e-01, -8.92014384e-01,  1.36443365e+00,\n",
            "              -9.94530976e-01, -1.51714444e-01, -8.10840309e-01,\n",
            "               8.68017077e-02,  8.29923511e-01, -7.38238573e-01,\n",
            "              -1.04544151e+00, -1.62419856e-01,  1.35745227e-01,\n",
            "              -6.01501405e-01, -6.77563667e-01,  6.72132850e-01,\n",
            "              -2.44161324e-03, -1.59343213e-01, -3.39706093e-01,\n",
            "              -6.86561525e-01, -1.37446970e-01,  1.22953713e+00,\n",
            "              -6.98796690e-01,  4.61566634e-02,  4.91007008e-02,\n",
            "               1.25653625e+00, -5.82262933e-01, -3.13965887e-01,\n",
            "               5.83382308e-01,  2.08412372e-02],\n",
            "             [-1.09832311e+00, -2.31558657e+00, -4.87197727e-01,\n",
            "              -3.42440546e-01,  6.28545880e-01, -2.54345745e-01,\n",
            "               1.12001456e-01,  6.07431114e-01,  4.52945948e-01,\n",
            "               1.08584404e+00,  5.03084779e-01, -2.29237127e+00,\n",
            "              -5.67137182e-01, -3.40830535e-01, -2.10089803e+00,\n",
            "              -2.53645271e-01,  4.67384636e-01, -3.22234601e-01,\n",
            "               5.16252697e-01,  5.01387715e-01, -4.85027879e-01,\n",
            "               2.02654794e-01,  3.80669355e-01,  4.52763736e-01,\n",
            "               6.87315345e-01, -6.68893218e-01, -2.16509610e-01,\n",
            "               1.09424484e+00, -2.43211293e+00,  5.04471362e-01,\n",
            "               9.52654004e-01, -2.96562582e-01],\n",
            "             [-1.06715679e+00, -2.31042147e-01, -7.11038291e-01,\n",
            "               1.71428010e-01,  1.29479885e+00, -2.89934874e-01,\n",
            "              -8.21158051e-01, -3.29255849e-01, -3.44075561e-01,\n",
            "              -1.04698455e+00, -3.92567188e-01, -6.25251532e-01,\n",
            "               2.95244213e-02, -2.36350656e-01,  4.36416358e-01,\n",
            "               8.59149620e-02,  4.60247368e-01, -4.49563533e-01,\n",
            "               5.91156669e-02, -4.99667108e-01,  3.33328396e-01,\n",
            "               1.53480232e+00,  5.45546174e-01,  1.83904823e-02,\n",
            "               6.06252849e-01,  4.40022439e-01,  3.57510626e-01,\n",
            "               7.68198192e-01, -1.68648869e-01,  1.01497602e+00,\n",
            "               1.96890011e-01,  5.76124310e-01],\n",
            "             [ 4.70592156e-02,  3.39764208e-01, -5.97856700e-01,\n",
            "               1.38083899e+00,  1.26742601e-01, -4.58101094e-01,\n",
            "              -3.63098055e-01, -2.88743883e-01, -1.26698709e+00,\n",
            "              -2.87557244e-01, -5.39138079e-01, -2.84869909e-01,\n",
            "              -1.85635701e-01, -7.96718597e-02, -2.81592786e-01,\n",
            "              -5.63627779e-01, -6.48222268e-02, -7.39526749e-01,\n",
            "              -1.76874563e-01, -1.26822487e-01,  1.97933733e-01,\n",
            "               3.08470488e-01, -9.01515603e-01, -4.05357718e-01,\n",
            "              -1.62396580e-01,  1.52612102e+00,  5.54071367e-01,\n",
            "              -3.13197146e-04, -2.46140927e-01,  1.45905137e+00,\n",
            "               1.07209302e-01,  1.51782191e+00],\n",
            "             [ 3.04892898e-01,  1.38166726e-01,  2.55034447e-01,\n",
            "               1.86751866e+00, -6.08170852e-02, -4.54161987e-02,\n",
            "              -2.41965055e-01,  7.81772017e-01, -1.51554585e+00,\n",
            "              -6.40055180e-01,  7.89705813e-02, -5.69889024e-02,\n",
            "              -7.29384124e-01, -2.91548729e-01, -5.02705395e-01,\n",
            "               3.19112897e-01, -8.62214938e-02,  3.80147815e-01,\n",
            "               4.80530977e-01, -4.47046787e-01,  8.02978039e-01,\n",
            "              -9.63121355e-02,  2.56381810e-01,  2.00017169e-01,\n",
            "               1.21715300e-01,  1.93173873e+00,  2.30194092e-01,\n",
            "              -1.37704507e-01, -1.74462765e-01,  1.31808531e+00,\n",
            "              -4.98262560e-03,  1.10654402e+00],\n",
            "             [-2.84442842e-01,  2.00638827e-02, -7.47324646e-01,\n",
            "               2.38870159e-01, -5.73728234e-02,  2.02403441e-01,\n",
            "              -9.13339481e-02, -2.53066516e+00,  5.80134273e-01,\n",
            "               7.56605482e-03, -2.62260288e-01,  1.14212260e-01,\n",
            "              -1.43651724e-01, -1.90612614e-01,  3.85565087e-02,\n",
            "              -3.59823018e-01, -5.27362339e-02,  1.16468862e-01,\n",
            "              -2.69274974e+00,  4.52171326e-01, -1.21625662e+00,\n",
            "              -9.71334241e-03,  7.00953975e-02,  6.87135816e-01,\n",
            "              -2.00873874e-02,  2.21614897e-01,  4.80091989e-01,\n",
            "               3.58771741e-01,  1.92478895e-01,  1.94938123e-01,\n",
            "               7.29824305e-02,  1.56411737e-01],\n",
            "             [-5.01241982e-02, -4.68639642e-01, -6.04771316e-01,\n",
            "              -1.21617973e-01, -1.77771926e-01,  2.18876660e-01,\n",
            "               1.58417928e+00, -2.08527446e-01, -8.00803304e-01,\n",
            "               3.09143364e-01,  1.40636712e-01, -3.57104808e-01,\n",
            "              -1.04674339e+00,  2.29302835e+00, -1.72156692e+00,\n",
            "              -1.51408359e-01,  1.50636983e+00,  7.06287086e-01,\n",
            "              -1.83230013e-01,  3.82860512e-01, -5.32764792e-01,\n",
            "              -1.78750858e-01, -2.41258740e-01,  3.14498127e-01,\n",
            "              -5.61772466e-01,  1.37939200e-01,  6.96818978e-02,\n",
            "              -4.06725556e-01, -7.48782933e-01, -7.85110965e-02,\n",
            "               2.04371497e-01,  5.38644679e-02],\n",
            "             [ 1.20446241e+00, -3.57435286e-01, -2.59099960e-01,\n",
            "              -1.49251446e-01,  8.86260092e-01, -9.00243223e-01,\n",
            "              -6.57357752e-01, -1.33753732e-01,  3.99826378e-01,\n",
            "              -1.10391581e+00,  1.29889280e-01,  4.08497691e-01,\n",
            "              -2.14324430e-01, -7.22175598e-01,  3.90048742e-01,\n",
            "              -7.34011889e-01,  6.80490375e-01, -5.89477181e-01,\n",
            "              -3.80282581e-01, -2.39135176e-01, -1.54334918e-01,\n",
            "               9.16918278e-01, -3.35977972e-01, -4.73154306e-01,\n",
            "               9.81723785e-01, -3.48684400e-01, -1.38609123e+00,\n",
            "               2.66948819e-01,  1.73762381e-01, -5.17413795e-01,\n",
            "              -2.56845057e-01, -7.60606468e-01]], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'linear3': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([ 0.7151826 , -0.40927982,  0.68577397, -0.13949122, -0.17425233,\n",
            "             -0.36873627, -0.5072177 ,  0.74513805,  0.11126217, -0.68565977,\n",
            "              1.2453926 , -1.0598183 ,  0.00515713,  0.371567  ,  0.04155875,\n",
            "              0.23447782], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 8.73017490e-01,  2.03249007e-01,  3.07695627e-01,\n",
            "               1.02801657e+00, -1.57717562e+00, -5.60853958e-01,\n",
            "               4.26085979e-01,  1.18141949e-01,  3.08799148e-01,\n",
            "               5.87722540e-01, -1.54190552e+00,  1.74992144e-01,\n",
            "               1.98454812e-01, -7.69934356e-02,  1.16750516e-01,\n",
            "              -5.02114296e-01],\n",
            "             [ 3.75290751e-01,  5.64014316e-01,  2.91108847e-01,\n",
            "              -2.87444115e-01,  7.40918159e-01, -7.73234442e-02,\n",
            "               1.57186103e+00,  6.98814273e-01, -5.13696492e-01,\n",
            "              -2.63751030e-01,  4.17735010e-01,  4.41513062e-01,\n",
            "              -2.21761405e-01,  6.03959978e-01,  9.15477276e-02,\n",
            "              -5.47436237e-01],\n",
            "             [-3.11161071e-01, -4.92259145e-01,  7.36995816e-01,\n",
            "               7.20228301e-04,  2.60605309e-02, -7.76444301e-02,\n",
            "               2.55990088e-01, -3.75720203e-01, -4.58882377e-02,\n",
            "               6.32713437e-01, -8.65858972e-01,  1.24210739e+00,\n",
            "              -9.51499224e-01,  1.07096028e+00, -3.85740191e-01,\n",
            "              -6.36742473e-01],\n",
            "             [ 7.77171731e-01,  3.61172795e-01,  1.14118874e-01,\n",
            "               3.39156002e-01,  1.02725722e-01, -1.34269094e+00,\n",
            "               2.16103852e-01, -8.94156337e-01,  1.10719609e+00,\n",
            "              -8.03466380e-01,  3.31660174e-02, -1.05139995e+00,\n",
            "              -1.15291333e+00,  4.87751007e-01, -3.50010216e-01,\n",
            "               6.73496187e-01],\n",
            "             [-9.36712742e-01,  3.72421533e-01,  1.42987394e+00,\n",
            "               4.03820544e-01, -1.98151827e-01, -1.45476758e-01,\n",
            "              -9.08328176e-01, -8.66291285e-01, -2.01471403e-01,\n",
            "              -1.22033405e+00,  6.81155846e-02, -4.78856117e-01,\n",
            "              -2.36062184e-01, -1.60191774e-01, -6.71858072e-01,\n",
            "               1.30573720e-01],\n",
            "             [-3.99423003e-01, -2.29343936e-01,  3.50912720e-01,\n",
            "               8.64931464e-01,  4.27976876e-01,  1.05050993e+00,\n",
            "               7.73554742e-02, -3.57446760e-01,  2.02134505e-01,\n",
            "               1.28192580e+00,  1.50382519e-01, -8.18499148e-01,\n",
            "               1.01949394e+00,  5.20275950e-01,  1.48235802e-02,\n",
            "               2.09523126e-01],\n",
            "             [ 1.11547060e-01,  1.76333451e+00, -3.64109993e-01,\n",
            "              -7.09323585e-01,  3.64192486e-01,  2.96951741e-01,\n",
            "              -1.71478152e-01, -2.65414059e-01, -2.55372941e-01,\n",
            "               1.25344098e-01,  1.36510417e-01,  1.48446158e-01,\n",
            "               1.82270166e-02,  9.89542827e-02, -1.19445717e+00,\n",
            "              -4.46422786e-01],\n",
            "             [ 7.30139554e-01, -9.36577395e-02,  1.87815547e-01,\n",
            "               1.67407081e-01, -2.88081318e-01, -7.62984276e-01,\n",
            "               4.40704167e-01,  8.45079839e-01,  8.23677599e-01,\n",
            "              -1.27384543e+00,  1.10482380e-01,  2.73626149e-01,\n",
            "               4.97063935e-01,  9.90763843e-01, -3.70052308e-01,\n",
            "              -3.19241613e-01],\n",
            "             [ 6.67387128e-01, -9.21496391e-01,  7.06623077e-01,\n",
            "              -2.94275194e-01, -7.63213113e-02, -2.64509141e-01,\n",
            "              -2.60761797e-01,  4.03029621e-02, -5.59258103e-01,\n",
            "              -4.11948740e-01, -7.86957443e-01,  2.62044519e-01,\n",
            "              -7.56868541e-01, -1.52997124e+00, -3.17542553e-01,\n",
            "              -6.80148482e-01],\n",
            "             [ 1.10520363e-01, -3.99096429e-01, -5.19756600e-02,\n",
            "               3.43204319e-01,  5.62475502e-01,  7.95629323e-01,\n",
            "              -4.67217535e-01, -4.82726321e-02,  2.83450931e-01,\n",
            "              -1.69111595e-01, -1.23717785e-01,  1.65642977e-01,\n",
            "               1.92819536e-01, -4.17382680e-02, -2.25746775e+00,\n",
            "              -2.39296481e-01],\n",
            "             [ 1.24275714e-01, -3.51575136e-01, -8.64959955e-01,\n",
            "               1.13006592e+00, -2.91553438e-01,  4.06442344e-01,\n",
            "              -1.86802134e-01,  2.80663401e-01, -3.90466869e-01,\n",
            "               2.68045127e-01,  3.41394514e-01,  1.53719366e-01,\n",
            "               1.57280910e+00, -2.84155160e-01, -7.67138481e-01,\n",
            "              -4.23740804e-01],\n",
            "             [ 5.54436862e-01,  2.07038119e-01, -5.76497138e-01,\n",
            "              -5.74952006e-01,  2.45553777e-01,  5.40049970e-01,\n",
            "               1.35477120e-02, -1.46796718e-01, -5.49512386e-01,\n",
            "               6.39976487e-02, -1.61856282e+00,  6.20073855e-01,\n",
            "              -2.44288579e-01, -6.46999538e-01,  7.24915266e-01,\n",
            "              -1.04844940e+00],\n",
            "             [ 1.58330481e-02, -3.96660566e-01, -9.81774151e-01,\n",
            "               1.07934743e-01, -5.64356625e-01, -4.54623103e-01,\n",
            "               9.87606645e-01, -6.66863799e-01, -2.58465767e-01,\n",
            "              -1.03655763e-01, -7.53296256e-01, -5.63267350e-01,\n",
            "              -4.26967859e-01, -4.05351520e-01, -3.89688492e-01,\n",
            "              -5.38046181e-01],\n",
            "             [-7.28014052e-01,  2.24553895e+00, -6.66337490e-01,\n",
            "              -3.95929486e-01,  3.74837011e-01,  2.88424883e-02,\n",
            "               3.79300803e-01, -4.67361510e-01,  3.21926713e-01,\n",
            "               7.51507282e-01, -3.30172569e-01,  4.81274635e-01,\n",
            "              -6.06388003e-02,  4.97432761e-02, -2.05304191e-01,\n",
            "               1.90482959e-02],\n",
            "             [-1.25535846e-01,  1.87809113e-02,  7.89764047e-01,\n",
            "               8.37577432e-02, -3.89850706e-01, -1.29899049e+00,\n",
            "               7.55754471e-01,  3.23251039e-01,  2.57944494e-01,\n",
            "              -7.71139622e-01, -2.39773437e-01,  4.23985213e-01,\n",
            "              -4.93077308e-01, -1.03446454e-01,  6.92047238e-01,\n",
            "              -6.24940217e-01],\n",
            "             [-1.59962642e+00,  1.71527088e-01, -2.78041273e-01,\n",
            "               5.09687424e-01,  2.40640249e-02,  6.41101718e-01,\n",
            "               4.84432399e-01, -1.21208608e+00,  2.92274535e-01,\n",
            "               3.27272147e-01,  1.74595326e-01,  6.28491715e-02,\n",
            "               1.06084108e+00, -3.38379949e-01, -1.52004644e-01,\n",
            "              -1.79543428e-03],\n",
            "             [-1.52832165e-01,  1.26168299e+00,  4.58512872e-01,\n",
            "              -4.32555974e-01,  3.58164817e-01, -7.86992162e-02,\n",
            "              -5.37546337e-01, -6.96275830e-01, -8.24202359e-01,\n",
            "               7.71835372e-02,  7.96666205e-01,  6.96795344e-01,\n",
            "               2.31331781e-01,  8.91040489e-02,  6.75923824e-01,\n",
            "              -5.17298691e-02],\n",
            "             [ 1.54555991e-01,  3.81259501e-01, -2.09583953e-01,\n",
            "              -3.94281626e-01,  1.84534442e+00,  1.24184370e+00,\n",
            "               1.74035028e-01, -5.80563009e-01,  1.36420101e-01,\n",
            "               7.50586808e-01, -1.12426460e-01,  1.35598814e+00,\n",
            "               1.58933461e-01, -1.43121287e-01, -1.73439980e-01,\n",
            "              -5.01123011e-01],\n",
            "             [ 3.94231647e-01,  6.21910095e-02, -4.97769117e-02,\n",
            "              -1.37248024e-01, -5.28078735e-01, -7.72648454e-01,\n",
            "               4.68617827e-01,  5.45303941e-01, -8.18740427e-02,\n",
            "              -1.03953981e+00,  5.08097932e-02, -3.96832591e-03,\n",
            "               1.89647257e-01,  1.59897101e+00, -8.20722654e-02,\n",
            "               2.93175220e-01],\n",
            "             [ 4.73892570e-01, -6.36005029e-02,  5.62932253e-01,\n",
            "              -6.87416270e-02,  2.20201567e-01,  2.20163539e-01,\n",
            "              -1.08802915e+00,  4.23999697e-01,  1.82445515e-02,\n",
            "              -1.59015507e-01,  1.35682121e-01,  1.15975738e+00,\n",
            "               1.25215501e-01, -1.04852700e+00, -3.24673474e-01,\n",
            "               2.68106232e-04],\n",
            "             [-2.68889040e-01, -1.10372186e+00,  8.21244270e-02,\n",
            "              -8.66692483e-01, -1.24223566e+00, -8.99158299e-01,\n",
            "              -2.76480526e-01,  6.08969092e-01, -4.74396080e-01,\n",
            "              -3.29795778e-01,  2.90391177e-01, -3.24631810e-01,\n",
            "              -5.59412479e-01,  8.33409429e-01,  5.85304081e-01,\n",
            "               9.32338774e-01],\n",
            "             [-9.14679289e-01,  3.55887473e-01,  9.11929190e-01,\n",
            "               6.46263361e-01, -3.53385925e-01, -3.07644576e-01,\n",
            "              -7.23865807e-01, -3.49520922e-01, -5.26873842e-02,\n",
            "              -9.06690478e-01,  4.35702831e-01,  4.83528554e-01,\n",
            "              -1.17219456e-01,  8.92235190e-02, -9.29329455e-01,\n",
            "               3.13833207e-01],\n",
            "             [-1.31142282e+00,  7.96158314e-02,  4.72874194e-01,\n",
            "              -3.09388727e-01,  5.49440265e-01,  1.17317389e-03,\n",
            "               1.30790591e+00, -1.20411503e+00, -2.09116235e-01,\n",
            "               1.31384164e-01,  3.68983477e-01,  1.00137189e-01,\n",
            "              -1.48095384e-01, -3.79183650e-01,  1.31933659e-01,\n",
            "              -1.31546766e-01],\n",
            "             [-2.95629263e-01, -5.47368944e-01, -3.77892047e-01,\n",
            "               8.37272763e-01, -2.24837810e-01,  1.44923851e-01,\n",
            "               4.59171347e-02, -2.40457252e-01, -3.60485762e-01,\n",
            "              -2.06831649e-01,  8.12363923e-01,  3.63532931e-01,\n",
            "               1.77366829e+00, -5.13634026e-01, -3.85643303e-01,\n",
            "              -7.58846402e-02],\n",
            "             [-6.03809059e-01, -6.27491772e-01,  1.19328749e+00,\n",
            "               6.93402350e-01,  2.21080050e-01, -3.57296079e-01,\n",
            "              -1.45184457e+00, -8.80510151e-01, -1.53301251e+00,\n",
            "              -1.20598304e+00,  4.90910202e-01,  8.90252832e-03,\n",
            "               2.48324096e-01,  1.11757621e-01, -4.84855443e-01,\n",
            "               3.38324517e-01],\n",
            "             [ 1.30394742e-01,  1.59173712e-01,  6.44551888e-02,\n",
            "               5.51550388e-02, -1.63775787e-01, -6.67674541e-01,\n",
            "               3.99020255e-01, -8.01805556e-01,  1.66875362e+00,\n",
            "              -1.60045251e-02, -1.13146961e-01, -3.67551833e-01,\n",
            "              -4.21079785e-01, -3.54594558e-01, -4.64736074e-02,\n",
            "               1.04084539e+00],\n",
            "             [-1.09898973e+00, -5.87164834e-02,  3.04410338e-01,\n",
            "              -7.76078820e-01,  7.34919965e-01,  3.25792760e-01,\n",
            "              -3.76327261e-02, -1.04199505e+00,  6.64838493e-01,\n",
            "               1.70121461e-01,  4.43833582e-02,  1.25858486e+00,\n",
            "              -1.47551864e-01, -1.36862993e-01, -1.55085579e-01,\n",
            "               9.75411713e-01],\n",
            "             [-8.67331207e-01, -6.90947548e-02,  9.05842006e-01,\n",
            "               5.60480297e-01,  1.46687075e-01,  3.00363779e-01,\n",
            "               5.44046536e-02, -1.10684681e+00, -3.06487799e-01,\n",
            "              -5.29979765e-01,  4.06935692e-01, -4.49273914e-01,\n",
            "               9.29452032e-02, -1.23890114e+00, -2.36153573e-01,\n",
            "               3.70290011e-01],\n",
            "             [ 3.60913187e-01,  8.93406048e-02, -9.66317594e-01,\n",
            "              -7.84776211e-01, -1.83554396e-01,  4.21377778e-01,\n",
            "               4.04727280e-01, -2.10770980e-01, -6.57887995e-01,\n",
            "              -1.38859659e-01, -1.12364578e+00, -2.03449398e-01,\n",
            "              -3.46916795e-01, -5.15813172e-01,  1.37852144e+00,\n",
            "              -4.84074324e-01],\n",
            "             [-1.83549389e-01,  1.86217919e-01,  1.04028262e-01,\n",
            "               3.29812884e-01, -1.25750393e-01, -1.11338902e+00,\n",
            "               4.79290634e-01, -7.48166323e-01,  9.68695462e-01,\n",
            "              -2.30511785e-01,  3.46249379e-02, -4.27714050e-01,\n",
            "              -1.00432992e+00, -2.79876590e-01,  1.35308847e-01,\n",
            "               1.47594595e+00],\n",
            "             [-9.56022918e-01, -7.38166094e-01,  1.41560042e+00,\n",
            "              -1.08483210e-01, -5.46905637e-01, -6.93991920e-03,\n",
            "              -6.88893318e-01, -5.13080418e-01, -8.95006716e-01,\n",
            "               5.72885394e-01, -3.61528946e-04,  1.17132771e+00,\n",
            "              -1.95951551e-01,  1.46698847e-01,  1.11400194e-01,\n",
            "               2.31110409e-01],\n",
            "             [-1.93184197e-01,  4.90727909e-02, -2.49427184e-02,\n",
            "              -1.73096895e-01, -2.85628885e-01, -6.26172364e-01,\n",
            "               3.62012208e-01, -6.93986773e-01,  7.16500521e-01,\n",
            "              -7.12101832e-02, -1.28936291e-01, -3.20418894e-01,\n",
            "              -9.76507187e-01, -6.01365529e-02,  7.76290074e-02,\n",
            "               1.67907250e+00]], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'linear4': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([0.09093971], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 0.45612103],\n",
            "             [-0.4762298 ],\n",
            "             [-0.9274099 ],\n",
            "             [-0.2925087 ],\n",
            "             [-0.32540497],\n",
            "             [-0.46727207],\n",
            "             [ 0.71824837],\n",
            "             [ 0.12836869],\n",
            "             [ 0.7460342 ],\n",
            "             [-0.24343564],\n",
            "             [-0.6750462 ],\n",
            "             [-0.5768682 ],\n",
            "             [-0.4948865 ],\n",
            "             [ 0.9393672 ],\n",
            "             [ 0.5211749 ],\n",
            "             [ 0.74389917]], dtype=float32)\n",
            "    )\n",
            "  }\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing a New Model and Load!"
      ],
      "metadata": {
        "id": "GVyXqnLJrQF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newModel = MLP(10, 16, 32, 16, 1, rngs=nnx.Rngs(0))  # Re-initialize the model\n",
        "abstract_model = nnx.eval_shape(lambda: newModel)  # Evaluate the abstract shape of the model\n",
        "graphdef, abstract_state = nnx.split(abstract_model)  # Split the model into graph and state\n",
        "print('The abstract NNX state (all leaves are abstract arrays):')\n",
        "#nnx.display(abstract_state)  # Uncomment this to display the abstract state (optional for debugging)\n",
        "\n",
        "state_restored = checkpointer.restore(ckpt_dir / 'state', abstract_state)  # Restore the state from checkpoint\n",
        "#nnx.display(state_restored['dp1'])  # Optional: Display restored state of dp1\n",
        "\n",
        "prng_key_value = state[\"dp1\"][\"rngs\"][\"default\"][\"key\"].value  # Access the PRNG key from the original state\n",
        "state_restored[\"dp1\"][\"rngs\"][\"default\"][\"key\"].type(nnx.RngKey)  # Ensure the restored key has the correct type\n",
        "state_restored[\"dp1\"][\"rngs\"][\"default\"][\"key\"].value = jax.random.wrap_key_data(prng_key_value)  # Modify the PRNG key value\n",
        "\n",
        "nnx.display(state_restored['dp1'])  # Display the restored dp1 state\n",
        "nnx.display(stateOrg['dp1'])  # Display the original dp1 state\n",
        "jax.tree.map(np.testing.assert_array_equal, stateOrg, state_restored)  # Ensure both states are equal\n",
        "print('NNX State restored: ')\n",
        "#nnx.display(state_restored)  # Uncomment this to display the full restored state\n",
        "\n",
        "newModel = nnx.merge(graphdef, state_restored)  # Merge the graph definition with the restored state\n",
        "optimizer = nnx.Optimizer(newModel, optax.adam(learning_rate))  # Reinitialize the optimizer\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),  # Initialize metrics for tracking the loss\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sIjj6adSeQ88",
        "outputId": "d308678d-e4fe-45ac-f7fa-ec492bc68f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The abstract NNX state (all leaves are abstract arrays):\n",
            "State({\n",
            "  'rngs': {\n",
            "    'default': {\n",
            "      'count': VariableState(\n",
            "        type=RngCount,\n",
            "        value=Array(49574, dtype=uint32),\n",
            "        tag='default'\n",
            "      ),\n",
            "      'key': VariableState(\n",
            "        type=RngKey,\n",
            "        value=Array((), dtype=key<fry>) overlaying:\n",
            "        [0 0],\n",
            "        tag='default'\n",
            "      )\n",
            "    }\n",
            "  }\n",
            "})\n",
            "State({\n",
            "  'rngs': {\n",
            "    'default': {\n",
            "      'count': VariableState(\n",
            "        type=RngCount,\n",
            "        value=Array(49574, dtype=uint32),\n",
            "        tag='default'\n",
            "      ),\n",
            "      'key': VariableState(\n",
            "        type=RngKey,\n",
            "        value=Array((), dtype=key<fry>) overlaying:\n",
            "        [0 0],\n",
            "        tag='default'\n",
            "      )\n",
            "    }\n",
            "  }\n",
            "})\n",
            "NNX State restored: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/orbax/checkpoint/type_handlers.py:1330: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train more if needed"
      ],
      "metadata": {
        "id": "t6s624pmsBwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "#model.train()\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'train_accuracy': [],\n",
        "  'train_precision': [],\n",
        "  'train_recall': [],\n",
        "  'train_f1': [],\n",
        "  'val_loss': [],\n",
        "  'val_accuracy': [],\n",
        "  'val_precision': [],\n",
        "  'val_recall': [],\n",
        "  'val_f1': [],\n",
        "  'test_loss': [],\n",
        "  'test_accuracy': [],\n",
        "  'test_precision': [],\n",
        "  'test_recall': [],\n",
        "  'test_f1': [],\n",
        "}\n",
        "\n",
        "num_epochs = 40  # Number of training epochs\n",
        "for epoch in range(num_epochs):\n",
        "    # Train on each batch in the training dataset\n",
        "    for batch in train_ds:\n",
        "        loss, logits = train_step(model, optimizer, metrics, batch)\n",
        "\n",
        "    # Compute and log training metrics for this epoch\n",
        "    for metric, value in metrics.compute().items():\n",
        "        metrics_history[f'train_{metric}'].append(value)\n",
        "\n",
        "    # Reset metrics for the next epoch\n",
        "    metrics.reset()\n",
        "\n",
        "    # Log training performance\n",
        "    print(\n",
        "        f\"[train] epoch: {epoch + 1}/{num_epochs}, \"\n",
        "        f\"loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
        "    )\n",
        "\n",
        "# later stuff\n",
        "\"\"\"\n",
        "for batch in val_ds:\n",
        "    loss = eval_step(model, metrics, batch)\n",
        "\n",
        "for metric, value in metrics.compute().items():\n",
        "    metrics_history[f'val_{metric}'].append(value)\n",
        "\n",
        "metrics.reset()\n",
        "print(\n",
        "    f\"[val] epoch: {epoch + 1}/{num_epochs}, \"\n",
        "    f\"loss: {metrics_history['val_loss'][-1]:.4f}, \"\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"[train] epoch: {epoch + 1}/{num_epochs}, \"\n",
        "    f\"loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
        "    f\"accuracy: {metrics_history['train_accuracy'][-1]:.2f}\"\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "O8Ukn5kWrwut",
        "outputId": "c8c0047c-27b5-402b-b2ca-6554dd7f5db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] epoch: 1/40, loss: 0.0841, \n",
            "[train] epoch: 2/40, loss: 0.0807, \n",
            "[train] epoch: 3/40, loss: 0.0819, \n",
            "[train] epoch: 4/40, loss: 0.0822, \n",
            "[train] epoch: 5/40, loss: 0.0796, \n",
            "[train] epoch: 6/40, loss: 0.0797, \n",
            "[train] epoch: 7/40, loss: 0.0819, \n",
            "[train] epoch: 8/40, loss: 0.0812, \n",
            "[train] epoch: 9/40, loss: 0.0852, \n",
            "[train] epoch: 10/40, loss: 0.0787, \n",
            "[train] epoch: 11/40, loss: 0.0867, \n",
            "[train] epoch: 12/40, loss: 0.0796, \n",
            "[train] epoch: 13/40, loss: 0.0790, \n",
            "[train] epoch: 14/40, loss: 0.0806, \n",
            "[train] epoch: 15/40, loss: 0.0839, \n",
            "[train] epoch: 16/40, loss: 0.0809, \n",
            "[train] epoch: 17/40, loss: 0.0812, \n",
            "[train] epoch: 18/40, loss: 0.0830, \n",
            "[train] epoch: 19/40, loss: 0.0808, \n",
            "[train] epoch: 20/40, loss: 0.0848, \n",
            "[train] epoch: 21/40, loss: 0.0811, \n",
            "[train] epoch: 22/40, loss: 0.0825, \n",
            "[train] epoch: 23/40, loss: 0.0804, \n",
            "[train] epoch: 24/40, loss: 0.0751, \n",
            "[train] epoch: 25/40, loss: 0.0824, \n",
            "[train] epoch: 26/40, loss: 0.0797, \n",
            "[train] epoch: 27/40, loss: 0.0782, \n",
            "[train] epoch: 28/40, loss: 0.0844, \n",
            "[train] epoch: 29/40, loss: 0.0778, \n",
            "[train] epoch: 30/40, loss: 0.0792, \n",
            "[train] epoch: 31/40, loss: 0.0802, \n",
            "[train] epoch: 32/40, loss: 0.0833, \n",
            "[train] epoch: 33/40, loss: 0.0856, \n",
            "[train] epoch: 34/40, loss: 0.0796, \n",
            "[train] epoch: 35/40, loss: 0.0829, \n",
            "[train] epoch: 36/40, loss: 0.0789, \n",
            "[train] epoch: 37/40, loss: 0.0813, \n",
            "[train] epoch: 38/40, loss: 0.0807, \n",
            "[train] epoch: 39/40, loss: 0.0817, \n",
            "[train] epoch: 40/40, loss: 0.0861, \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor batch in val_ds:\\n    loss = eval_step(model, metrics, batch)\\n\\nfor metric, value in metrics.compute().items():\\n    metrics_history[f\\'val_{metric}\\'].append(value)\\n\\nmetrics.reset()\\nprint(\\n    f\"[val] epoch: {epoch + 1}/{num_epochs}, \"\\n    f\"loss: {metrics_history[\\'val_loss\\'][-1]:.4f}, \"\\n)\\n\\nprint(\\n    f\"[train] epoch: {epoch + 1}/{num_epochs}, \"\\n    f\"loss: {metrics_history[\\'train_loss\\'][-1]:.4f}, \"\\n    f\"accuracy: {metrics_history[\\'train_accuracy\\'][-1]:.2f}\"\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate saved model"
      ],
      "metadata": {
        "id": "7qZ0YMvQsI5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def pred_step(model: MLP, batch):\n",
        "    logits = model(batch['features'])\n",
        "    return nnx.sigmoid(logits)\n",
        "\n",
        "test_ds = DataLoader(test_set, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "ypred = []  # List to store predicted probabilities\n",
        "label = []  # List to store true labels\n",
        "\n",
        "for test_batch in test_ds:\n",
        "    logits = pred_step(model, test_batch)\n",
        "    ypred.extend(np.ravel(logits))  # Flatten and collect predictions\n",
        "    label.extend(np.ravel(test_batch[\"labels\"]))  # Flatten and collect true labels\n",
        "\n",
        "binary_ypred = np.where(np.array(ypred) > 0.5, 1, 0)\n",
        "\n",
        "accuracy = sum([1 for pred, true in zip(binary_ypred, label) if pred == true]) / len(label)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqlRLOx02910",
        "outputId": "b6c00706-daa1-4036-daa6-80aa0112aa53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9671\n"
          ]
        }
      ]
    }
  ]
}