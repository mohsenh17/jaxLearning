{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOaz1UgScCmQbxlNBvcwdwl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenh17/jaxLearning/blob/main/flax/MNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and test set (TensorFlow)"
      ],
      "metadata": {
        "id": "AQ-Ybb3IgJaY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hVX_LVs1gDSs"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "from flax import nnx\n",
        "from functools import partial\n",
        "\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import optax\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "train_steps = 1400\n",
        "eval_every = 200\n",
        "batch_size = 32\n",
        "\n",
        "mnist_train: tf.data.Dataset = tfds.load('mnist', split='train')\n",
        "mnist_test: tf.data.Dataset = tfds.load('mnist', split='test')\n",
        "\n",
        "mnist_train = mnist_train.map(lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255., 'label': sample['label']})\n",
        "mnist_test = mnist_test.map(lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255., 'label': sample['label']})\n",
        "\n",
        "# the current setting won't cover all images -> (1875, 32)\n",
        "mnist_train = mnist_train.repeat().shuffle(1024)\n",
        "mnist_train = mnist_train.batch(batch_size, drop_remainder=True).take(train_steps).prefetch(1)\n",
        "\n",
        "mnist_test = mnist_test.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xtO6PIn2gWMP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nnx.Module):\n",
        "  def __init__(self, *, num_classes: int, rngs:nnx.Rngs):\n",
        "    self.num_classes = num_classes\n",
        "    self.conv1 = nnx.Conv(in_features=1, out_features=32, kernel_size=(3, 3), rngs=rngs)\n",
        "    self.conv2 = nnx.Conv(in_features=32, out_features=64, kernel_size=(3, 3), rngs=rngs)\n",
        "    self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2))\n",
        "    self.linear1 = nnx.Linear(3136, 256, rngs=rngs)\n",
        "    self.linear2 = nnx.Linear(256, num_classes, rngs=rngs)\n",
        "  def __call__(self, x):\n",
        "    x = self.avg_pool(nnx.gelu(self.conv1(x)))\n",
        "    x = self.avg_pool(nnx.gelu(self.conv2(x)))\n",
        "    x = x.reshape(x.shape[0], -1)  # flatten\n",
        "    x = nnx.gelu(self.linear1(x))\n",
        "    x = self.linear2(x)\n",
        "    return x\n",
        "\n",
        "# Instantiate the model.\n",
        "model = CNN(num_classes=10, rngs=nnx.Rngs(0))\n",
        "# Visualize it.\n",
        "nnx.display(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZLFYXLLovr0",
        "outputId": "4b769589-b8be-4eb6-e90d-4e332e758654"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  num_classes=10,\n",
            "  conv1=Conv(\n",
            "    kernel_shape=(3, 3, 1, 32),\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(3, 3, 1, 32), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    in_features=1,\n",
            "    out_features=32,\n",
            "    kernel_size=(3, 3),\n",
            "    strides=1,\n",
            "    padding='SAME',\n",
            "    input_dilation=1,\n",
            "    kernel_dilation=1,\n",
            "    feature_group_count=1,\n",
            "    use_bias=True,\n",
            "    mask=None,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "    bias_init=<function zeros at 0x797f3e206b00>,\n",
            "    conv_general_dilated=<function conv_general_dilated at 0x797f3ea2b760>\n",
            "  ),\n",
            "  conv2=Conv(\n",
            "    kernel_shape=(3, 3, 32, 64),\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(3, 3, 32, 64), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(64,), dtype=float32)\n",
            "    ),\n",
            "    in_features=32,\n",
            "    out_features=64,\n",
            "    kernel_size=(3, 3),\n",
            "    strides=1,\n",
            "    padding='SAME',\n",
            "    input_dilation=1,\n",
            "    kernel_dilation=1,\n",
            "    feature_group_count=1,\n",
            "    use_bias=True,\n",
            "    mask=None,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "    bias_init=<function zeros at 0x797f3e206b00>,\n",
            "    conv_general_dilated=<function conv_general_dilated at 0x797f3ea2b760>\n",
            "  ),\n",
            "  avg_pool=functools.partial(<function avg_pool at 0x797f234d95a0>, window_shape=(2, 2), strides=(2, 2)),\n",
            "  linear1=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(3136, 256), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(256,), dtype=float32)\n",
            "    ),\n",
            "    in_features=3136,\n",
            "    out_features=256,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "    bias_init=<function zeros at 0x797f3e206b00>,\n",
            "    dot_general=<function dot_general at 0x797f3e9ceef0>\n",
            "  ),\n",
            "  linear2=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(256, 10), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(10,), dtype=float32)\n",
            "    ),\n",
            "    in_features=256,\n",
            "    out_features=10,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "    bias_init=<function zeros at 0x797f3e206b00>,\n",
            "    dot_general=<function dot_general at 0x797f3e9ceef0>\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = model(jnp.ones((1, 28, 28, 1)))\n",
        "nnx.display(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa_aDbNku8GX",
        "outputId": "6b649e2c-966d-4457-e63f-2b4d513cd9ed"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.0380987  -0.1663384   0.04239376 -0.1821506   0.05388633  0.03874649\n",
            "  -0.03934722  0.05054386  0.17043065 -0.06008959]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "momentum = 0.9\n",
        "\n",
        "optimizer = nnx.Optimizer(model, optax.adamw(lr, momentum))\n",
        "\n",
        "metrics = nnx.MultiMetric(\n",
        "  accuracy=nnx.metrics.Accuracy(),\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "\n",
        "nnx.display(metrics)\n",
        "nnx.display(optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kakaoe1w5CQ",
        "outputId": "9c0140c1-f81d-41d9-a99a-3f0d97ab563f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiMetric(\n",
            "  accuracy=Accuracy(\n",
            "    argname='values',\n",
            "    total=MetricState(\n",
            "      value=Array(0., dtype=float32)\n",
            "    ),\n",
            "    count=MetricState(\n",
            "      value=Array(0, dtype=int32)\n",
            "    )\n",
            "  ),\n",
            "  loss=Average(\n",
            "    argname='loss',\n",
            "    total=MetricState(\n",
            "      value=Array(0., dtype=float32)\n",
            "    ),\n",
            "    count=MetricState(\n",
            "      value=Array(0, dtype=int32)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Optimizer(\n",
            "  step=OptState(\n",
            "    value=Array(0, dtype=uint32)\n",
            "  ),\n",
            "  model=CNN(\n",
            "    num_classes=10,\n",
            "    conv1=Conv(\n",
            "      kernel_shape=(3, 3, 1, 32),\n",
            "      kernel=Param(\n",
            "        value=Array(shape=(3, 3, 1, 32), dtype=float32)\n",
            "      ),\n",
            "      bias=Param(\n",
            "        value=Array(shape=(32,), dtype=float32)\n",
            "      ),\n",
            "      in_features=1,\n",
            "      out_features=32,\n",
            "      kernel_size=(3, 3),\n",
            "      strides=1,\n",
            "      padding='SAME',\n",
            "      input_dilation=1,\n",
            "      kernel_dilation=1,\n",
            "      feature_group_count=1,\n",
            "      use_bias=True,\n",
            "      mask=None,\n",
            "      dtype=None,\n",
            "      param_dtype=<class 'jax.numpy.float32'>,\n",
            "      precision=None,\n",
            "      kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "      bias_init=<function zeros at 0x797f3e206b00>,\n",
            "      conv_general_dilated=<function conv_general_dilated at 0x797f3ea2b760>\n",
            "    ),\n",
            "    conv2=Conv(\n",
            "      kernel_shape=(3, 3, 32, 64),\n",
            "      kernel=Param(\n",
            "        value=Array(shape=(3, 3, 32, 64), dtype=float32)\n",
            "      ),\n",
            "      bias=Param(\n",
            "        value=Array(shape=(64,), dtype=float32)\n",
            "      ),\n",
            "      in_features=32,\n",
            "      out_features=64,\n",
            "      kernel_size=(3, 3),\n",
            "      strides=1,\n",
            "      padding='SAME',\n",
            "      input_dilation=1,\n",
            "      kernel_dilation=1,\n",
            "      feature_group_count=1,\n",
            "      use_bias=True,\n",
            "      mask=None,\n",
            "      dtype=None,\n",
            "      param_dtype=<class 'jax.numpy.float32'>,\n",
            "      precision=None,\n",
            "      kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "      bias_init=<function zeros at 0x797f3e206b00>,\n",
            "      conv_general_dilated=<function conv_general_dilated at 0x797f3ea2b760>\n",
            "    ),\n",
            "    avg_pool=functools.partial(<function avg_pool at 0x797f234d95a0>, window_shape=(2, 2), strides=(2, 2)),\n",
            "    linear1=Linear(\n",
            "      kernel=Param(\n",
            "        value=Array(shape=(3136, 256), dtype=float32)\n",
            "      ),\n",
            "      bias=Param(\n",
            "        value=Array(shape=(256,), dtype=float32)\n",
            "      ),\n",
            "      in_features=3136,\n",
            "      out_features=256,\n",
            "      use_bias=True,\n",
            "      dtype=None,\n",
            "      param_dtype=<class 'jax.numpy.float32'>,\n",
            "      precision=None,\n",
            "      kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "      bias_init=<function zeros at 0x797f3e206b00>,\n",
            "      dot_general=<function dot_general at 0x797f3e9ceef0>\n",
            "    ),\n",
            "    linear2=Linear(\n",
            "      kernel=Param(\n",
            "        value=Array(shape=(256, 10), dtype=float32)\n",
            "      ),\n",
            "      bias=Param(\n",
            "        value=Array(shape=(10,), dtype=float32)\n",
            "      ),\n",
            "      in_features=256,\n",
            "      out_features=10,\n",
            "      use_bias=True,\n",
            "      dtype=None,\n",
            "      param_dtype=<class 'jax.numpy.float32'>,\n",
            "      precision=None,\n",
            "      kernel_init=<function variance_scaling.<locals>.init at 0x797f231a68c0>,\n",
            "      bias_init=<function zeros at 0x797f3e206b00>,\n",
            "      dot_general=<function dot_general at 0x797f3e9ceef0>\n",
            "    )\n",
            "  ),\n",
            "  tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x797f15b5e560>, update=<function chain.<locals>.update_fn at 0x797f15b5e9e0>),\n",
            "  opt_state=(ScaleByAdamState(count=Array(0, dtype=int32), mu=State({\n",
            "    'conv1': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(32,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(3, 3, 1, 32), dtype=float32)\n",
            "      )\n",
            "    },\n",
            "    'conv2': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(64,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(3, 3, 32, 64), dtype=float32)\n",
            "      )\n",
            "    },\n",
            "    'linear1': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(256,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(3136, 256), dtype=float32)\n",
            "      )\n",
            "    },\n",
            "    'linear2': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(10,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(256, 10), dtype=float32)\n",
            "      )\n",
            "    }\n",
            "  }), nu=State({\n",
            "    'conv1': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(32,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(3, 3, 1, 32), dtype=float32)\n",
            "      )\n",
            "    },\n",
            "    'conv2': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(64,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(3, 3, 32, 64), dtype=float32)\n",
            "      )\n",
            "    },\n",
            "    'linear1': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(256,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(3136, 256), dtype=float32)\n",
            "      )\n",
            "    },\n",
            "    'linear2': {\n",
            "      'bias': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(10,), dtype=float32)\n",
            "      ),\n",
            "      'kernel': VariableState(\n",
            "        type=Param,\n",
            "        value=Array(shape=(256, 10), dtype=float32)\n",
            "      )\n",
            "    }\n",
            "  })), EmptyState(), EmptyState()),\n",
            "  wrt=<class 'flax.nnx.nnx.variables.Param'>\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(model: CNN, batch):\n",
        "  logits = model(batch['image'])\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "    logits=logits, labels=batch['label']\n",
        "  ).mean()\n",
        "  return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: CNN, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(model, batch)\n",
        "  metrics.update(loss=loss, logits=logits, labels=batch['label'])  # In-place updates.\n",
        "  optimizer.update(grads)  # In-place updates.\n",
        "\n",
        "@nnx.jit\n",
        "def eval_step(model: CNN, metrics: nnx.MultiMetric, batch):\n",
        "  loss, logits = loss_fn(model, batch)\n",
        "  metrics.update(loss=loss, logits=logits, labels=batch['label'])  # In-place updates."
      ],
      "metadata": {
        "id": "fFIUf6-pxpRz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'train_accuracy': [],\n",
        "  'test_loss': [],\n",
        "  'test_accuracy': [],\n",
        "}\n",
        "\n",
        "for step, batch in enumerate(mnist_train.as_numpy_iterator()):\n",
        "  # Run the optimization for one step and make a stateful update to the following:\n",
        "  # - The train state's model parameters\n",
        "  # - The optimizer state\n",
        "  # - The training loss and accuracy batch metrics\n",
        "  train_step(model, optimizer, metrics, batch)\n",
        "\n",
        "  if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
        "    # Log the training metrics.\n",
        "    for metric, value in metrics.compute().items():  # Compute the metrics.\n",
        "      metrics_history[f'train_{metric}'].append(value)  # Record the metrics.\n",
        "    metrics.reset()  # Reset the metrics for the test set.\n",
        "\n",
        "    # Compute the metrics on the test set after each training epoch.\n",
        "    for test_batch in mnist_test.as_numpy_iterator():\n",
        "      eval_step(model, metrics, test_batch)\n",
        "\n",
        "    # Log the test metrics.\n",
        "    for metric, value in metrics.compute().items():\n",
        "      metrics_history[f'test_{metric}'].append(value)\n",
        "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
        "\n",
        "    print(\n",
        "      f\"[train] step: {step}, \"\n",
        "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
        "      f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\"\n",
        "    )\n",
        "    print(\n",
        "      f\"[test] step: {step}, \"\n",
        "      f\"loss: {metrics_history['test_loss'][-1]}, \"\n",
        "      f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_CAnH_ryVB4",
        "outputId": "e7ac2ded-c6d0-47e6-d3f8-eae6a17c65e6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] step: 200, loss: 0.4062541127204895, accuracy: 87.98196411132812\n",
            "[test] step: 200, loss: 0.14917251467704773, accuracy: 95.49000549316406\n",
            "[train] step: 400, loss: 0.13892975449562073, accuracy: 95.828125\n",
            "[test] step: 400, loss: 0.09353181719779968, accuracy: 96.98999786376953\n",
            "[train] step: 600, loss: 0.09272447228431702, accuracy: 97.203125\n",
            "[test] step: 600, loss: 0.06901627033948898, accuracy: 97.83999633789062\n",
            "[train] step: 800, loss: 0.0753866508603096, accuracy: 97.65625\n",
            "[test] step: 800, loss: 0.0621148981153965, accuracy: 97.98999786376953\n",
            "[train] step: 1000, loss: 0.06575710326433182, accuracy: 97.703125\n",
            "[test] step: 1000, loss: 0.050169169902801514, accuracy: 98.41999816894531\n",
            "[train] step: 1200, loss: 0.06956712901592255, accuracy: 97.890625\n",
            "[test] step: 1200, loss: 0.05240246653556824, accuracy: 98.2699966430664\n",
            "[train] step: 1399, loss: 0.058782048523426056, accuracy: 98.13127899169922\n",
            "[test] step: 1399, loss: 0.05572550743818283, accuracy: 98.18999481201172\n"
          ]
        }
      ]
    }
  ]
}