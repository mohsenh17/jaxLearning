{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9tR4A1+vG+Opcar26Fglg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenh17/jaxLearning/blob/main/flax/flaxCheckpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUAfhJeb-vgD",
        "outputId": "f811a2eb-316b-4b34-ca4f-ba1480a68b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (0.8.5)\n",
            "Collecting flax\n",
            "  Downloading flax-0.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting orbax\n",
            "  Downloading orbax-0.1.9.tar.gz (1.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (0.4.33)\n",
            "Collecting jax\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax) (0.2.3)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax) (0.1.67)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax) (6.0.2)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from jax) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax) (1.13.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.4.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.10.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: chex>=0.1.86 in /usr/local/lib/python3.10/dist-packages (from optax->flax) (0.1.87)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.86->optax->flax) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.20.2)\n",
            "Downloading flax-0.10.1-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.3/419.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: orbax\n",
            "  Building wheel for orbax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for orbax: filename=orbax-0.1.9-py3-none-any.whl size=1493 sha256=26ae858ce1819ec45fb4de7fc6670df253a79c08cfcec075aa00979995098d1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/7a/98/b955a4db98b54317c311ee32367994ca530721c62a87ec56a7\n",
            "Successfully built orbax\n",
            "Installing collected packages: jaxlib, jax, orbax, flax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "Successfully installed flax-0.10.1 jax-0.4.35 jaxlib-0.4.35 orbax-0.1.9\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade flax orbax jax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "from flax import nnx\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import orbax.checkpoint as ocp\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import io\n",
        "\n"
      ],
      "metadata": {
        "id": "JFO7NNqQ_Dtc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate a dataset with 100 samples, 10 features, 5 informative, 5 redundant, and 2 classes\n",
        "X, y = make_classification(n_samples=10000, n_features=10, n_informative=8,\n",
        "                          n_classes=2, random_state=42)\n",
        "df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
        "df['Class'] = y\n",
        "csv_data = df.to_csv(index=True)\n"
      ],
      "metadata": {
        "id": "W4I2hnqUodtF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataset, transform=None, target_transform=None):\n",
        "        completeDF = pd.read_csv(dataset, index_col=0)\n",
        "        self.labels = pd.DataFrame(completeDF['Class'])\n",
        "        features_raw = pd.DataFrame(completeDF.drop('Class', axis=1))\n",
        "        #self.features = pd.DataFrame(completeDF.drop('Class', axis=1))\n",
        "        scaler = MinMaxScaler()\n",
        "        self.features = pd.DataFrame(scaler.fit_transform(features_raw))\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = np.array(self.features.iloc[idx, :])\n",
        "        label = self.labels.iloc[idx, 0]\n",
        "        if self.transform:\n",
        "            features = self.transform(features)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return features, label"
      ],
      "metadata": {
        "id": "ZBPFELboo4e1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"mssmartypants/rice-type-classification\")\n",
        "print(path)\n",
        "/content/dataset/riceClassification.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "tWv5r05xBvNk",
        "outputId": "8ad2459a-0c60-4a0c-8298-cc7f10bb6e09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.4)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mssmartypants/rice-type-classification?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 888k/888k [00:00<00:00, 94.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "/root/.cache/kagglehub/datasets/mssmartypants/rice-type-classification/versions/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'content' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d9ed3797b33b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mssmartypants/rice-type-classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcontent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mriceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls /root/.cache/kagglehub/datasets/mssmartypants/rice-type-classification/versions/2/\n",
        "! mv /root/.cache/kagglehub/datasets/mssmartypants/rice-type-classification/versions/2/riceClassification.csv dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXec3_GJByPc",
        "outputId": "6ae3d148-4647-4ff0-d373-36850428cefc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "riceClassification.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = CustomImageDataset(dataset=io.StringIO(csv_data))\n",
        "dataset = CustomImageDataset(dataset=\"riceClassification.csv\")\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [.9, 0.05,0.05])\n",
        "data_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "for features, labels in data_loader:\n",
        "    print(\"Batch of features has shape: \",features.shape)\n",
        "    print(\"Batch of labels has shape: \", labels.shape)\n",
        "    print(features)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s3fFrv-otNR",
        "outputId": "759151f9-d873-4181-bbd4-c444254f5cf0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of features has shape:  torch.Size([4, 10])\n",
            "Batch of labels has shape:  torch.Size([4])\n",
            "tensor([[0.7568, 0.7503, 0.7214, 0.7574, 0.7112, 0.8087, 0.3426, 0.5589, 0.8031,\n",
            "         0.3516],\n",
            "        [0.8192, 0.7784, 0.7657, 0.7490, 0.7707, 0.8597, 0.6165, 0.5892, 0.8091,\n",
            "         0.3420],\n",
            "        [0.3573, 0.6650, 0.2552, 0.9352, 0.3369, 0.4401, 0.1183, 0.4155, 0.6118,\n",
            "         0.6982],\n",
            "        [0.4589, 0.6977, 0.3798, 0.8956, 0.4299, 0.5423, 0.3457, 0.4572, 0.6647,\n",
            "         0.5847]], dtype=torch.float64)\n",
            "tensor([0, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nnx.Module):\n",
        "    def __init__(self, din, dm1, dm2, dm3, num_classes: int, rngs: nnx.Rngs):\n",
        "      self.dp1 = nnx.Dropout(rate=0.4, rngs=rngs)\n",
        "      self.linear1 = nnx.Linear(10, 16, rngs=rngs)\n",
        "      self.bn1 = nnx.BatchNorm(16, rngs=rngs)\n",
        "      self.dp2 = nnx.Dropout(rate=0.2, rngs=rngs)\n",
        "      self.linear2 = nnx.Linear(16, 32, rngs=rngs)\n",
        "      self.bn2 = nnx.BatchNorm(32, rngs=rngs)\n",
        "      self.dp3 = nnx.Dropout(rate=0.1, rngs=rngs)\n",
        "      self.linear3 = nnx.Linear(32, 16, rngs=rngs)\n",
        "      self.bn3 = nnx.BatchNorm(16, rngs=rngs)\n",
        "      self.linear4 = nnx.Linear(16, 1, rngs=rngs)\n",
        "\n",
        "\n",
        "    def __call__(self, x):\n",
        "      x = self.dp1(x)\n",
        "      x = self.linear1(x)\n",
        "      x = nnx.gelu(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.dp2(x)\n",
        "      x = self.linear2(x)\n",
        "      x = nnx.gelu(x)\n",
        "      x = self.bn2(x)\n",
        "      x = self.dp3(x)\n",
        "      x = self.linear3(x)\n",
        "      x = nnx.gelu(x)\n",
        "      x = self.bn3(x)\n",
        "      x= self.linear4(x)\n",
        "      return x #nnx.sigmoid(x)\n",
        "\n",
        "# Instantiate and test the model\n",
        "model = MLP(10, 16, 32, 16, 1, rngs=nnx.Rngs(0))\n",
        "y = model(x=jnp.ones((3, 10)))\n",
        "\n",
        "nnx.display(model)\n",
        "\n",
        "#model = MLP([2, 4], 1, rngs=nnx.Rngs(0))\n",
        "y = model(x=jnp.ones((3, 10)))\n",
        "\n",
        "nnx.display(y)\n",
        "y\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C2PTOR3_Abz",
        "outputId": "e75bf550-26d1-4557-9b29-73330c9f8acb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  dp1=Dropout(rate=0.4, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(\n",
            "    default=RngStream(\n",
            "      key=RngKey(\n",
            "        value=Array((), dtype=key<fry>) overlaying:\n",
            "        [0 0],\n",
            "        tag='default'\n",
            "      ),\n",
            "      count=RngCount(\n",
            "        value=Array(17, dtype=uint32),\n",
            "        tag='default'\n",
            "      )\n",
            "    )\n",
            "  )),\n",
            "  linear1=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(10, 16), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    in_features=10,\n",
            "    out_features=16,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7d17a70ff400>,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    dot_general=<function dot_general at 0x7d17a836ae60>\n",
            "  ),\n",
            "  bn1=BatchNorm(\n",
            "    mean=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    var=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    scale=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    num_features=16,\n",
            "    use_running_average=False,\n",
            "    axis=-1,\n",
            "    momentum=0.99,\n",
            "    epsilon=1e-05,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    use_bias=True,\n",
            "    use_scale=True,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    scale_init=<function ones at 0x7d17a7a52200>,\n",
            "    axis_name=None,\n",
            "    axis_index_groups=None,\n",
            "    use_fast_variance=True\n",
            "  ),\n",
            "  dp2=Dropout(rate=0.2, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(...)),\n",
            "  linear2=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(16, 32), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    in_features=16,\n",
            "    out_features=32,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7d17a70ff400>,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    dot_general=<function dot_general at 0x7d17a836ae60>\n",
            "  ),\n",
            "  bn2=BatchNorm(\n",
            "    mean=BatchStat(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    var=BatchStat(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    scale=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(32,), dtype=float32)\n",
            "    ),\n",
            "    num_features=32,\n",
            "    use_running_average=False,\n",
            "    axis=-1,\n",
            "    momentum=0.99,\n",
            "    epsilon=1e-05,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    use_bias=True,\n",
            "    use_scale=True,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    scale_init=<function ones at 0x7d17a7a52200>,\n",
            "    axis_name=None,\n",
            "    axis_index_groups=None,\n",
            "    use_fast_variance=True\n",
            "  ),\n",
            "  dp3=Dropout(rate=0.1, broadcast_dims=(), deterministic=False, rng_collection='dropout', rngs=Rngs(...)),\n",
            "  linear3=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(32, 16), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    in_features=32,\n",
            "    out_features=16,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7d17a70ff400>,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    dot_general=<function dot_general at 0x7d17a836ae60>\n",
            "  ),\n",
            "  bn3=BatchNorm(\n",
            "    mean=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    var=BatchStat(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    scale=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array(shape=(16,), dtype=float32)\n",
            "    ),\n",
            "    num_features=16,\n",
            "    use_running_average=False,\n",
            "    axis=-1,\n",
            "    momentum=0.99,\n",
            "    epsilon=1e-05,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    use_bias=True,\n",
            "    use_scale=True,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    scale_init=<function ones at 0x7d17a7a52200>,\n",
            "    axis_name=None,\n",
            "    axis_index_groups=None,\n",
            "    use_fast_variance=True\n",
            "  ),\n",
            "  linear4=Linear(\n",
            "    kernel=Param(\n",
            "      value=Array(shape=(16, 1), dtype=float32)\n",
            "    ),\n",
            "    bias=Param(\n",
            "      value=Array([0.], dtype=float32)\n",
            "    ),\n",
            "    in_features=16,\n",
            "    out_features=1,\n",
            "    use_bias=True,\n",
            "    dtype=None,\n",
            "    param_dtype=<class 'jax.numpy.float32'>,\n",
            "    precision=None,\n",
            "    kernel_init=<function variance_scaling.<locals>.init at 0x7d17a70ff400>,\n",
            "    bias_init=<function zeros at 0x7d17a7a52050>,\n",
            "    dot_general=<function dot_general at 0x7d17a836ae60>\n",
            "  )\n",
            ")\n",
            "[[-0.19967312]\n",
            " [ 0.56698495]\n",
            " [-0.36731178]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[-0.19967312],\n",
              "       [ 0.56698495],\n",
              "       [-0.36731178]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "learning_rate = 0.005\n",
        "momentum = 0.9\n",
        "\n",
        "model = MLP(10, 16, 32, 16, 1, rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(learning_rate))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")"
      ],
      "metadata": {
        "id": "ASj5AcVZqpXN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def loss_fn(model: MLP, batch):\n",
        "  logits = model(batch['features'])\n",
        "  loss = optax.sigmoid_binary_cross_entropy(\n",
        "    logits=logits, labels=batch['labels'].reshape(-1, 1)\n",
        "  ).mean()\n",
        "  #loss = (logits - batch['labels'])**2\n",
        "  #print(logits.pval)\n",
        "  return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: MLP, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(model, batch)\n",
        "\n",
        "  metrics.update(loss=loss, logits=logits, labels=batch['labels'])  # In-place updates.\n",
        "  optimizer.update(grads)  # In-place updates.\n",
        "  return loss, nnx.sigmoid(logits)\n",
        "\n",
        "@nnx.jit\n",
        "def eval_step(model: MLP, metrics: nnx.MultiMetric, batch):\n",
        "  loss, logits = loss_fn(model, batch)\n",
        "  metrics.update(loss=loss, logits=logits, labels=batch['labels'])  # In-place updates.\n",
        "  return loss"
      ],
      "metadata": {
        "id": "HsPjwULJq8GS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels =  np.array(transposed_data[1])\n",
        "    features = np.array(transposed_data[0])\n",
        "\n",
        "    return {\"features\":features, \"labels\":labels}\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [.7, 0.1,0.2])\n",
        "\n",
        "train_ds = DataLoader(train_set, batch_size=64, shuffle=True, drop_last=True, collate_fn=custom_collate_fn)\n",
        "val_set = DataLoader(val_set, batch_size=4, shuffle=True, drop_last=True, collate_fn=custom_collate_fn)\n",
        "test_ds = DataLoader(test_set, batch_size=4, shuffle=True, drop_last=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "# test\n",
        "batch_data = next(iter(train_ds))\n",
        "imgs = batch_data['features']\n",
        "lbls = batch_data['labels']\n",
        "print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)\n",
        "print(lbls)\n",
        "\n",
        "#loss = train_step(model, optimizer, metrics, batch_data)\n",
        "loss, logits = loss_fn(model, batch_data)\n",
        "print(loss.shape, logits.shape)\n",
        "print(f'{loss = }')\n",
        "#print(f'{logits = }')\n",
        "print(f'{optimizer.step.value = }')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fioF1mHarC8_",
        "outputId": "e8421ecf-c31c-4733-d86c-404fae4bf694"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 10) float64 (64,) int64\n",
            "[0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 1\n",
            " 1 1 1 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0]\n",
            "() (64, 1)\n",
            "loss = Array(0.71133876, dtype=float32)\n",
            "optimizer.step.value = Array(0, dtype=uint32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "#model.train()\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'train_accuracy': [],\n",
        "  'train_precision': [],\n",
        "  'train_recall': [],\n",
        "  'train_f1': [],\n",
        "  'val_loss': [],\n",
        "  'val_accuracy': [],\n",
        "  'val_precision': [],\n",
        "  'val_recall': [],\n",
        "  'val_f1': [],\n",
        "  'test_loss': [],\n",
        "  'test_accuracy': [],\n",
        "  'test_precision': [],\n",
        "  'test_recall': [],\n",
        "  'test_f1': [],\n",
        "}\n",
        "\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_ds:\n",
        "      loss, logits = train_step(model, optimizer, metrics, batch)\n",
        "\n",
        "    for metric, value in metrics.compute().items():\n",
        "      metrics_history[f'train_{metric}'].append(value)\n",
        "\n",
        "    metrics.reset()\n",
        "    print(\n",
        "      f\"[train] epoch: {epoch}, \"\n",
        "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_cxxkNtrH0O",
        "outputId": "9f5b94ae-5e1b-449f-8bc2-d84f2e7db650"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] epoch: 0, loss: 0.3609307110309601, \n",
            "[train] epoch: 1, loss: 0.18825207650661469, \n",
            "[train] epoch: 2, loss: 0.16301099956035614, \n",
            "[train] epoch: 3, loss: 0.1410873830318451, \n",
            "[train] epoch: 4, loss: 0.13903899490833282, \n",
            "[train] epoch: 5, loss: 0.13084694743156433, \n",
            "[train] epoch: 6, loss: 0.11954084783792496, \n",
            "[train] epoch: 7, loss: 0.12038098275661469, \n",
            "[train] epoch: 8, loss: 0.11598961800336838, \n",
            "[train] epoch: 9, loss: 0.11178449541330338, \n",
            "[train] epoch: 10, loss: 0.10916774719953537, \n",
            "[train] epoch: 11, loss: 0.10636737197637558, \n",
            "[train] epoch: 12, loss: 0.10746248811483383, \n",
            "[train] epoch: 13, loss: 0.10622826218605042, \n",
            "[train] epoch: 14, loss: 0.10226988792419434, \n",
            "[train] epoch: 15, loss: 0.10102538019418716, \n",
            "[train] epoch: 16, loss: 0.10771719366312027, \n",
            "[train] epoch: 17, loss: 0.09823504090309143, \n",
            "[train] epoch: 18, loss: 0.10105704516172409, \n",
            "[train] epoch: 19, loss: 0.09536466002464294, \n",
            "[train] epoch: 20, loss: 0.09680689871311188, \n",
            "[train] epoch: 21, loss: 0.10003606975078583, \n",
            "[train] epoch: 22, loss: 0.09590370208024979, \n",
            "[train] epoch: 23, loss: 0.09650293737649918, \n",
            "[train] epoch: 24, loss: 0.09486982226371765, \n",
            "[train] epoch: 25, loss: 0.09289126843214035, \n",
            "[train] epoch: 26, loss: 0.09846393764019012, \n",
            "[train] epoch: 27, loss: 0.09034810215234756, \n",
            "[train] epoch: 28, loss: 0.09239278733730316, \n",
            "[train] epoch: 29, loss: 0.08767802268266678, \n",
            "[train] epoch: 30, loss: 0.0927547961473465, \n",
            "[train] epoch: 31, loss: 0.09303397685289383, \n",
            "[train] epoch: 32, loss: 0.0846639946103096, \n",
            "[train] epoch: 33, loss: 0.09225499629974365, \n",
            "[train] epoch: 34, loss: 0.08876484632492065, \n",
            "[train] epoch: 35, loss: 0.09177852421998978, \n",
            "[train] epoch: 36, loss: 0.08703823387622833, \n",
            "[train] epoch: 37, loss: 0.08632825314998627, \n",
            "[train] epoch: 38, loss: 0.08357124030590057, \n",
            "[train] epoch: 39, loss: 0.09041403979063034, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.eval() # Switch to evaluation mode.\n",
        "\n",
        "@nnx.jit\n",
        "def pred_step(model: MLP, batch):\n",
        "  logits = model(batch['features'])\n",
        "  return nnx.sigmoid(logits)\n",
        "\n",
        "test_ds = DataLoader(test_set, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "ypred = []\n",
        "label = []\n",
        "for test_batch in test_ds:\n",
        "  logits = pred_step(model, test_batch)\n",
        "  #print(np.ravel(logits))\n",
        "  #print(np.ravel(logits))\n",
        "  #break\n",
        "  ypred.extend(np.ravel(logits))\n",
        "  label.extend(np.ravel(test_batch[\"labels\"]))\n",
        "  #print(logits, test_batch[\"labels\"])\n",
        "  #break\n",
        "print(ypred)\n",
        "binary_ypred = np.where(np.array(ypred) > 0.5, 1, 0)\n",
        "print(len(label), sum(label))\n",
        "accuracy = sum([1 for pred, true in zip(binary_ypred, label) if pred == true]) / len(label)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JdDFipvrj4L",
        "outputId": "7068ded7-f4a4-414b-f233-6a8e107564c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8388507, 0.6814183, 0.0045904433, 0.9608203, 0.9998684, 0.9699249, 0.997973, 0.9962095, 0.9953205, 0.0035713236, 0.9991178, 0.99937856, 0.9868825, 0.9971988, 8.94274e-06, 0.9140317, 6.76457e-05, 0.9298944, 0.8736703, 0.1722382, 0.0005041713, 1.1624091e-07, 0.99262416, 0.006653903, 0.9632338, 0.99684167, 0.99898046, 0.00094376085, 0.0021273254, 4.3147926e-07, 0.9979582, 0.98690295, 0.99979514, 9.292264e-05, 0.9984646, 0.99429953, 0.8424435, 0.99933004, 1.8321753e-06, 0.9999634, 0.99941254, 0.9885642, 0.9998124, 5.220526e-07, 0.0061407452, 0.0003635076, 0.9996294, 0.9998596, 0.99740833, 0.6951318, 0.9631061, 0.00071271765, 0.737289, 2.4510264e-05, 0.058060925, 0.9942801, 0.9987179, 0.99140996, 0.91261804, 0.0001780182, 1.2471223e-05, 0.0009101696, 0.97418857, 0.007924607, 0.99887735, 0.0007361447, 0.00025779151, 0.95175827, 0.00013428627, 8.144445e-06, 0.99879444, 0.99958926, 0.9427793, 0.9823737, 0.9158278, 0.017996624, 6.90913e-07, 0.9996954, 0.9966582, 0.99627054, 0.6429792, 0.99459887, 0.82064694, 0.9980781, 0.0075179534, 0.97820675, 0.9924515, 0.0011619959, 0.9770139, 0.73258466, 0.98345023, 3.0905416e-05, 0.97184235, 0.010497537, 0.99959224, 0.0013043166, 0.023848785, 0.02907038, 9.153728e-05, 0.98482555, 0.99862003, 0.99996614, 0.024346864, 0.0009398718, 0.0050356165, 0.9925343, 0.9999709, 0.0038418889, 0.0037865294, 0.9996505, 0.00081911025, 0.8654178, 0.0027828326, 0.9991049, 0.99842465, 0.002740599, 0.99999523, 0.9920393, 0.019162774, 0.9938398, 0.0005112523, 0.00059282745, 0.9999379, 0.0005453572, 6.454697e-06, 0.99699676, 0.96065265, 0.9997243, 0.01238998, 0.7445627, 0.9998115, 0.9910919, 6.854957e-05, 0.99933213, 0.9423522, 0.98567736, 0.9949993, 0.0017108504, 0.0003674809, 0.0004911651, 0.0013581229, 0.9499734, 0.99784076, 0.027836893, 3.4593257e-05, 0.999848, 0.00015928176, 0.0046664, 0.98337746, 0.10841425, 0.99928766, 0.9862192, 0.92098445, 0.9969451, 0.0013240108, 3.0631334e-05, 0.99995816, 0.99987674, 0.0031973775, 0.9997528, 0.7860625, 0.9452471, 0.9998896, 0.9846219, 0.9999732, 0.96401864, 0.99980694, 0.0059126294, 0.0009781594, 0.978288, 0.9969054, 0.049651448, 0.004081579, 0.009127005, 0.027715458, 0.0025239007, 0.04028901, 0.9939611, 0.039620455, 0.00017710919, 0.0010547759, 0.99714726, 1.3636131e-07, 0.00021542767, 0.99996483, 0.99985003, 0.99852765, 0.9974765, 0.9938306, 4.015727e-06, 0.9995541, 0.026293216, 1.4429477e-06, 0.99104464, 0.12113183, 0.004622808, 9.1740454e-05, 0.9665945, 0.9780659, 0.04423931, 0.1578115, 0.9938413, 0.00032145614, 0.99335194, 0.76235294, 0.9899114, 0.9999049, 0.93243825, 0.00026368268, 0.6145313, 0.99876165, 1.4159593e-06, 0.9959777, 0.94654787, 0.9973647, 3.449682e-07, 0.7423681, 0.99231017, 0.8731659, 0.9999676, 0.9592247, 0.7940038, 0.9724557, 0.49199608, 0.00035140713, 0.9999193, 0.042696066, 0.9995913, 0.001167797, 0.0003610661, 0.99993646, 0.99893385, 1.8925526e-05, 0.9992162, 0.9636031, 0.0046068383, 0.9997372, 0.75639457, 0.98894125, 0.015249624, 0.9991767, 0.99973434, 0.9997985, 0.9975803, 0.0014815242, 0.00047024156, 4.457595e-05, 0.0009244406, 0.00025062342, 1.158738e-05, 0.9996865, 9.759398e-05, 0.99385804, 0.9969216, 0.9999217, 0.09704411, 0.004750152, 5.2652125e-05, 0.9941293, 0.9935127, 0.032921, 0.99961627, 0.00015032741, 0.0007385795, 0.9398692, 0.9980685, 0.9999739, 0.006293927, 0.9754073, 0.8600587, 0.99378455, 0.8705294, 3.7039157e-05, 0.99462503, 0.99950504, 0.0073036426, 0.00028221778, 0.40611, 0.98970896, 0.00027182684, 0.9963825, 0.9973863, 0.99683386, 0.99849796, 0.0012876745, 0.1384322, 0.9886835, 0.00037051318, 0.0020837886, 0.9999194, 0.29221126, 9.968639e-05, 4.294607e-07, 0.9986191, 0.97553957, 0.999731, 0.99960786, 0.013546389, 0.99841666, 0.014163861, 0.01391949, 0.0003456484, 0.974309, 0.15440883, 0.9756797, 0.99823797, 4.922838e-05, 0.21327803, 0.9999838, 0.0007508715, 0.004367708, 0.10833559, 0.9999478, 0.999821, 0.9971641, 0.99963963, 0.0021589953, 0.9989235, 0.01795806, 0.00016457244, 0.9997359, 0.0007574239, 0.99762493, 0.9852629, 0.36498526, 0.027437283, 0.9982735, 0.03674085, 0.99998903, 0.00011863798, 0.011044804, 0.9993979, 0.24509203, 0.093171336, 4.120705e-05, 0.99220604, 0.9988267, 0.00040528335, 0.006205896, 0.42759106, 0.9999541, 0.9998361, 0.9999758, 0.0071027577, 0.9420215, 0.9786808, 0.95684975, 0.013299674, 0.021542676, 0.0030467282, 0.008817841, 0.00029260298, 0.042575274, 0.001871042, 2.0509548e-05, 0.0024245926, 0.9999224, 0.89436316, 0.00054019125, 0.99981886, 0.99981827, 0.008241193, 0.0030800195, 0.0071388674, 5.6433787e-06, 0.99997365, 0.99993813, 0.008481462, 8.0121616e-05, 0.9999535, 0.9410073, 0.011751538, 0.0002411898, 0.7781161, 0.99888843, 0.99982125, 0.9962269, 0.9612805, 0.024086602, 0.98685104, 0.07016315, 0.058929358, 0.9956761, 0.9997013, 1.5757536e-05, 0.051083464, 0.02442634, 0.80240995, 0.99372584, 0.0003684665, 0.99854636, 0.99763453, 0.99998784, 0.63054067, 0.99956423, 1.8162102e-06, 0.9911731, 0.9151893, 0.96540046, 0.9965395, 0.90186954, 0.9880106, 0.99924755, 0.996992, 0.0002034751, 0.9832819, 0.94736284, 0.9369316, 0.9441018, 0.0018618522, 0.00024255522, 0.9644506, 0.99018186, 0.00013457378, 0.00014660033, 0.0006815377, 3.060117e-05, 0.0001743075, 0.9935267, 0.99981695, 0.9556998, 0.6969089, 0.0007742897, 0.9968659, 0.9990096, 0.9991603, 0.9951212, 0.99908626, 0.033843935, 2.2153251e-06, 0.99557674, 0.91641146, 0.0003940826, 0.99917334, 0.97826946, 9.724375e-05, 0.99913186, 0.9999188, 0.99687433, 0.82404065, 0.007998676, 0.001957477, 0.98507786, 8.833837e-06, 0.013071165, 0.20703356, 0.0043510017, 0.9931369, 0.020227976, 0.012845214, 0.41954422, 0.99892765, 0.99992657, 0.97963655, 0.99953425, 0.99988997, 0.00024784272, 0.0014113319, 0.0016980926, 0.99954635, 0.03917646, 0.9204397, 0.69593954, 0.00033494053, 0.99210125, 0.0004944128, 0.9511273, 0.96737814, 0.00088425574, 0.9894944, 0.9985384, 0.00015410528, 0.99103457, 1.0604892e-05, 0.9115798, 0.9995782, 0.9978309, 2.8217655e-05, 0.022350539, 0.9988275, 0.0004113047, 0.00092054036, 0.998643, 0.80336285, 0.17972575, 0.039767563, 0.00011838744, 0.25215685, 0.6984307, 0.36370674, 0.0007084311, 0.99800867, 0.9970577, 0.0035610807, 0.0021566933, 0.99964476, 0.99996436, 0.9994247, 0.014918503, 0.00011124521, 0.99950886, 0.999526, 1.4707768e-05, 0.41258746, 0.9921286, 0.9997377, 0.99863476, 0.0012856195, 0.004571186, 0.0697359, 0.9913067, 0.9999629, 0.6281146, 0.999371, 0.86248404, 0.014398757, 0.85294455, 0.93763536, 0.99874496, 0.99831724, 0.99113804, 0.9997203, 0.9989857, 0.93814033, 0.011523554, 0.00020598914, 0.9960692, 0.99453765, 0.99324703, 0.99228597, 1.2996961e-05, 0.90984154, 1.0691564e-05, 0.99916875, 0.002319059, 4.808297e-05, 0.9998554, 0.12093236, 0.19762039, 4.5213255e-05, 0.9878055, 6.808578e-05, 0.88297045, 0.00023746949, 0.07150669, 0.99256825, 0.0001086374, 0.9522839, 0.990833, 0.037133493, 5.243126e-05, 0.0013552668, 0.99360585, 0.00020064323, 0.004105735, 2.8494183e-05, 0.99583286, 0.98964685, 0.99921024, 0.004897194, 0.0018447733, 0.99762017, 0.004297276, 0.03945267, 0.9995228, 0.9999521, 0.9993381, 0.875082, 0.9998826, 0.9999039, 9.308617e-05, 0.9727787, 1.9482584e-05, 0.008733933, 0.9997867, 0.99997926, 0.009318566, 0.998548, 0.85617596, 2.1376058e-05, 0.9987006, 0.047289893, 2.7474653e-05, 8.660513e-05, 0.23944202, 0.99548894, 0.999734, 0.99819034, 0.9998634, 0.99514186, 0.7306419, 0.9971329, 0.00010691131, 0.26737067, 0.99268395, 0.9990049, 4.5606845e-05, 0.9492404, 0.9831686, 5.8549816e-05, 0.9595983, 0.9839924, 0.00017557824, 0.0035151858, 0.9931716, 0.11697713, 0.9957675, 0.7208808, 0.9940234, 0.0054029035, 0.99998, 0.9270391, 9.566689e-06, 0.91678596, 0.99994016, 0.99260426, 0.9251865, 0.96887666, 0.996872, 0.7898732, 0.98161685, 0.94332963, 0.99626786, 0.9561645, 1.0803113e-06, 0.7568998, 0.0049443226, 0.9994017, 0.77616113, 1.6821552e-06, 0.99728143, 2.0101284e-05, 3.567414e-05, 0.29403332, 0.0005726874, 0.70268613, 0.98936874, 0.99812454, 0.9224256, 0.0005419997, 0.04308339, 0.8648124, 0.9999634, 0.99779534, 0.99579096, 0.969577, 0.33955833, 0.9999937, 0.993327, 0.0003456787, 0.021693947, 0.014729776, 0.9645068, 0.010766433, 0.99968624, 0.9989882, 0.1890767, 0.9999683, 9.0445574e-05, 0.09818335, 3.99741e-07, 0.09048345, 0.748829, 0.9950937, 0.9999418, 0.00029391708, 0.98203963, 4.2894888e-05, 0.00025224133, 0.0032093616, 0.99978894, 0.0009064634, 0.0009898961, 0.021426477, 0.99980694, 0.99998903, 0.3282427, 0.9997135, 0.9984517, 0.00010944709, 0.010884859, 0.99637455, 0.99999857, 0.053764626, 0.08596622, 0.025655128, 0.9999149, 0.00033921478, 0.022859108, 0.017158631, 0.95210266, 0.0051483545, 0.9537473, 0.0045002294, 0.014772591, 0.0018781037, 0.999961, 0.999987, 0.00019505514, 0.009716474, 0.9914733, 0.99709845, 9.090982e-05, 0.5317119, 0.00018864236, 0.9998405, 0.0051354286, 0.0013022659, 0.99870837, 0.99947256, 0.99996924, 0.0005083333, 0.0013987308, 0.0016131871, 0.0015777533, 0.00049798284, 0.25547254, 0.5856402, 0.009355656, 3.0053503e-05, 0.98560554, 0.021633103, 0.9920391, 0.040276032, 0.99997437, 0.9884948, 0.99517953, 0.7971257, 0.8954145, 0.99998856, 0.00029525958, 0.0059954543, 0.00815366, 0.99166745, 0.99992096, 0.9999933, 0.0011267976, 0.99977165, 3.7170028e-06, 0.02636912, 0.001364421, 0.07071307, 0.99397933, 0.9999684, 0.00011763685, 0.0035325796, 0.03794174, 0.009089202, 0.9999839, 0.99997926, 0.0131713925, 0.21040732, 0.9999863, 0.86611044, 0.9914779, 0.00449819, 0.99943763, 0.014306604, 0.5278344, 0.5229809, 0.9806192, 0.9989687, 0.047313195, 0.9999665, 0.21017855, 0.00026068676, 0.73786294, 0.04930268, 0.0064553954, 0.9998971, 0.99141824, 0.9936051, 0.9997315, 0.0006948611, 0.0037673747, 0.054710172, 0.0011094428, 0.9996012, 2.225431e-05, 0.00064428925, 0.0008345696, 5.3533735e-05, 0.05965351, 0.99892503, 0.00013900465, 0.013008436, 0.2316833, 0.0020151455, 0.99987936, 0.98074156, 0.9997924, 0.99539787, 0.99775535, 0.9988217, 0.9831286, 0.9973924, 0.040080525, 0.99085885, 0.995129, 0.009603599, 0.99973875, 0.0012692793, 0.99983, 0.0037943388, 0.9999062, 0.99971205, 5.7132267e-05, 0.0020084106, 0.007397694, 0.99911755, 0.99914086, 0.9997621, 0.10687804, 0.0022335835, 0.0056551336, 0.0055865436, 0.00016968543, 0.99969983, 0.7558642, 1.5668362e-05, 0.99995947, 0.99923, 0.016663203, 0.9589669, 0.2619246, 0.9975733, 0.009516686, 0.0041507133, 0.9999912, 0.92377067, 0.0017620651, 0.0030893588, 0.00025355638, 5.3536677e-08, 8.864275e-07, 0.9844786, 0.5012675, 0.03397717, 0.97497594, 0.9790881, 0.99904054, 0.9988392, 0.004666788, 0.0018915506, 0.00064595824, 0.0002796788, 0.6332377, 0.00012361257, 0.99827254, 0.9978885, 0.9999405, 0.012967201, 0.48578852, 0.99954957, 0.9595722, 0.8738926, 0.8727145, 0.9995945, 0.98380035, 0.748089, 0.97028536, 0.93973804, 0.9817089, 0.9896142, 0.9996675, 0.9964019, 0.001567885, 0.9777355, 0.6800806, 0.0010899875, 0.9986582, 0.00222648, 0.00032573508, 7.514125e-06, 0.0005134534, 0.99993885, 0.99865204, 0.9999101, 0.0008901873, 0.999124, 0.0012035726, 0.0037040596, 0.37738943, 0.999962, 0.965761, 0.00034201748, 0.999318, 1.389993e-05, 0.9984049, 0.0014671099, 0.9999858, 0.3554851, 0.9988771, 0.00066959386, 0.089497216, 0.9995515, 0.9988607, 0.0011296581, 0.024717942, 0.033324294, 0.8612915, 0.018479409, 0.9978108, 0.001506988, 0.8635585, 0.00076307054, 0.00032350805, 0.0011288244, 0.32268447, 0.99952734, 0.993986, 5.532408e-05, 0.0043283813, 0.9998958, 0.9934269, 0.0023877716, 0.9713306, 0.9999796, 0.9996153, 0.9898693, 0.99398756, 0.13940899, 0.98719287, 0.9996666, 0.9971239, 8.5815545e-06, 0.00019499786, 0.9897466, 0.97665536, 0.9996605, 0.9851071, 0.99989307, 3.151426e-05, 0.00023466526, 0.791448, 0.99356776, 0.00046581586, 0.001635026, 0.9332401, 0.8126195, 1.6053464e-05, 0.9992083, 0.9948559, 0.9994173, 0.99665976, 0.38076356, 0.97436553, 0.0067005428, 0.99973303, 0.028699376, 0.00013898317, 5.074558e-06, 0.0008346352, 0.00011469721, 0.999584, 0.9981725, 0.9540837, 0.001971297, 0.9980902, 0.99959284, 0.99057543, 0.00020964348, 0.99291915, 0.9942311, 0.89879274, 0.9482579, 0.45270053, 0.99985945, 0.99979836, 5.7936973e-05, 2.1111557e-05, 0.0014937451, 0.9957749, 1.47263645e-05, 0.0030742532, 0.99956995, 0.9920353, 0.99884343, 0.9996691, 0.00072153733, 0.9327359, 0.9964408, 4.8179863e-05, 0.98595506, 0.9952573, 0.9045731, 0.052693, 0.9996152, 4.245298e-05, 0.9036388, 2.2188688e-06, 0.9652138, 0.9997652, 0.00023367864, 0.033522453, 0.99542254, 0.9871623, 0.74284923, 0.8812041, 7.008658e-05, 0.9973345, 0.96490043, 0.99820673, 0.94868946, 0.0005903671, 0.00031654703, 0.00014241855, 0.9997634, 0.9988883, 0.9994771, 0.018746106, 0.9999591, 0.9998882, 0.99826777, 0.00035176106, 3.521021e-06, 0.8701381, 0.07951191, 0.9896975, 0.99993837, 7.34961e-05, 0.006469724, 0.34246904, 0.00020981506, 0.9226322, 0.9998902, 0.9978296, 0.99764496, 1.0146669e-05, 0.6591087, 0.9933664, 0.00010430568, 0.00017518827, 0.9986858, 0.9999039, 0.9820362, 0.0003737655, 0.99848914, 9.855916e-07, 0.00010731251, 0.98253673, 0.031782757, 0.9951426, 0.9999536, 0.00086785527, 0.00050360814, 0.011836154, 0.981281, 0.0001200333, 0.998497, 0.33966205, 0.9999155, 0.00799371, 0.9983033, 0.9987159, 0.046234045, 0.00015853686, 0.99971944, 0.9833636, 0.00252259, 0.99928266, 0.70623225, 0.00021807669, 1.4364082e-05, 0.9997296, 0.99620074, 0.9923677, 0.0007730099, 0.98563486, 0.99023867, 1.7538816e-05, 0.9703873, 0.99046296, 0.9971808, 0.9887263, 0.99992657, 0.00031319875, 0.15840371, 0.0028649182, 0.9967026, 0.744289, 0.8503511, 9.094064e-07, 0.00013650673, 0.8662982, 0.99825436, 0.7022351, 0.9944529, 0.0024213076, 0.99506277, 3.8723315e-06, 0.00044246658, 0.99941874, 0.06517073, 0.95857066, 0.99982893, 0.00017582066, 0.0005115445, 0.010184817, 0.9999739, 0.00051325763, 0.0018421743, 0.07771197, 0.99831057, 0.0009199191, 0.029604096, 0.99983907, 0.99837995, 0.91159505, 0.0007136637, 0.99984145, 0.9995547, 0.97256136, 0.9935289, 0.99738497, 0.9998996, 0.9998167, 0.06774935, 2.7069225e-05, 0.042273037, 0.0053725755, 0.017121028, 0.9987852, 0.9999889, 0.0020500128, 0.00062672765, 0.9953774, 3.0113548e-05, 0.9974113, 0.9954, 0.9677426, 0.00042888615, 0.0038292953, 0.98722106, 0.03667275, 0.00066449546, 0.9999229, 0.002262088, 0.99565244, 0.9944965, 0.99996257, 0.99208856, 0.90019023, 0.99222636, 0.00052120665, 0.06901582, 0.9995241, 0.00048842013, 0.99948967, 0.9997384, 2.7229333e-05, 0.0001398194, 0.0008471806, 0.99931765, 0.96656823, 0.99662244, 0.9991171, 0.00048479316, 0.99856585, 0.9998586, 0.97500855, 0.038156115, 0.0007961619, 0.95189553, 0.97457176, 0.9963063, 0.0030735985, 0.9703087, 0.024673667, 0.99388736, 4.011233e-06, 5.0984894e-05, 0.9943415, 0.99521095, 4.9438953e-05, 0.0004044435, 0.78311586, 0.9528622, 0.29648155, 2.8655368e-05, 0.9998035, 0.9998442, 0.93701804, 0.012887815, 9.3387935e-05, 0.03867067, 0.07365988, 0.93983865, 0.99523586, 0.00013975303, 0.00613861, 0.0074981973, 0.99996257, 0.9990582, 0.014737287, 0.95063484, 0.0020864161, 0.99951816, 0.9996425, 0.9987852, 0.9999354, 8.9013787e-07, 0.99410844, 0.99607074, 0.9917681, 0.001413962, 0.99923253, 0.007364264, 0.99957603, 0.00035822552, 0.9420917, 0.0002545143, 0.9987488, 0.018798752, 0.9989611, 9.670741e-05, 0.9729691, 0.9990496, 0.974934, 0.99943095, 0.00037830055, 0.019433636, 0.995572, 0.0017061838, 0.00032178764, 0.97911376, 0.0070316806, 0.00035724757, 0.9917081, 0.99998677, 0.96989655, 0.99990165, 0.0033970368, 0.26773477, 0.007843185, 0.9998983, 0.9938984, 0.9998648, 0.9980393, 0.0010234256, 0.0030625244, 0.99880266, 0.99982977, 0.9877687, 6.0875824e-05, 0.000114743714, 1.6498618e-05, 0.0003481665, 0.98567766, 0.8397305, 0.9994716, 0.001218314, 3.5628276e-05, 0.355958, 0.00028395455, 0.0031760673, 0.0006915132, 0.00011717381, 0.9983456, 0.67869455, 0.9828924, 4.317572e-05, 0.9973801, 0.9999777, 0.99997497, 0.9999633, 0.9984785, 0.8321692, 0.0017721216, 0.9938182, 0.00017414818, 0.999589, 0.00034242205, 1.6301632e-05, 0.0045863385, 0.980093, 0.9543934, 0.83403397, 0.99906904, 0.998412, 0.9999958, 0.9984895, 0.9922167, 0.004543534, 0.00036141896, 0.9999901, 0.008880189, 5.3551197e-05, 0.00061074656, 0.00028411244, 0.0047299988, 0.99990606, 0.0005434536, 0.0048354827, 0.99991274, 0.010441844, 0.013184302, 0.0013052417, 0.9930299, 0.99738973, 0.999974, 0.9999255, 0.054162502, 0.0009882207, 0.028309837, 0.998722, 0.93543535, 0.999376, 0.89609283, 0.077647, 0.00035666907, 0.9930629, 0.0005984868, 0.9985623, 0.04545229, 5.6966495e-05, 0.9711039, 0.0033027297, 0.9996958, 0.99326867, 0.99970335, 0.9999223, 0.012517348, 0.00015537122, 0.0015291243, 0.9494672, 0.6784413, 0.9905308, 0.0002281468, 0.99416476, 0.9999473, 0.9954419, 1.9146257e-05, 0.99330586, 9.770109e-07, 0.9988514, 0.99986744, 0.0011711519, 0.9981589, 0.009675972, 0.00028694214, 0.97972107, 0.9999503, 0.014669574, 0.9779764, 0.0052199923, 0.8779106, 0.97226226, 0.9990534, 0.023811404, 0.00673244, 0.012714092, 0.0005865574, 0.99884737, 0.9999645, 0.9981001, 0.00027908984, 5.4672375e-05, 0.999998, 0.103724085, 0.97221214, 0.07146463, 0.9999603, 0.00089884456, 0.9998191, 0.9991549, 0.99989593, 0.00059021043, 0.0002751731, 3.1322776e-05, 0.8294694, 0.15366097, 0.99521106, 0.9941294, 0.07462249, 5.898954e-05, 0.96230245, 0.9578362, 9.0174995e-05, 0.8083475, 1.0954011e-05, 0.9218751, 0.68530256, 0.9369429, 0.9116223, 3.98643e-06, 3.91816e-06, 0.014041673, 0.9980432, 0.4216954, 0.97506917, 0.82336444, 0.999949, 0.77816, 0.98957634, 0.9719616, 0.9462266, 1.2895118e-05, 0.96187234, 0.7935334, 0.9919968, 2.1999099e-06, 0.99838865, 0.99988854, 0.97944194, 0.68239534, 0.8897731, 0.73636836, 1.0932501e-05, 0.9975381, 0.98387897, 0.0032424526, 0.99441105, 0.9637885, 0.00057748816, 0.95765775, 0.00022482629, 0.9975108, 0.99242604, 0.07562883, 0.99998987, 7.4226823e-06, 0.010928063, 0.9901839, 0.00029717517, 0.999908, 0.99291706, 0.99932694, 0.9968773, 0.9959045, 0.99759525, 0.9949116, 0.95796746, 0.00032176124, 0.9830867, 5.5229244e-05, 0.9994547, 1.5767126e-05, 0.4396653, 2.571251e-05, 0.00029417273, 0.9956455, 0.99984455, 0.99887437, 0.06377563, 0.99877495, 0.996906, 0.0072463416, 5.0271945e-05, 0.9998184, 0.9985134, 0.00014236043, 0.98920447, 0.0006347667, 2.6850682e-06, 0.002448548, 0.8843211, 0.9887129, 0.4502272, 0.86814064, 0.028146302, 0.000109327455, 0.74657, 0.73295695, 0.98824894, 0.99777126, 0.62387127, 0.792542, 0.9947948, 0.9997894, 7.593612e-06, 0.96998197, 0.90585834, 0.9247077, 0.99085814, 0.28733093, 0.0073624863, 0.00040556781, 0.9996836, 0.9985796, 0.00038207206, 0.9998486, 0.99905485, 6.2510296e-08, 0.0006585779, 0.9893267, 0.99406284, 0.99964964, 0.9971227, 0.00016045125, 0.99999464, 0.9965496, 0.00014918076, 0.08947292, 0.0004252641, 0.9974408, 0.76018983, 0.013485405, 0.0012923958, 0.41423023, 0.997097, 0.01648383, 0.99983585, 0.0014753127, 0.9996082, 9.84192e-05, 0.47047052, 0.9999535, 0.0036288577, 0.0010746953, 0.9999788, 0.6948003, 0.0009385697, 0.99974793, 5.0344195e-05, 0.9994678, 0.9983126, 0.00023204545, 0.99984336, 0.7790669, 0.00015688495, 0.9998524, 0.9999933, 0.4161024, 0.0006254209, 0.9985569, 0.0006567301, 0.0006096666, 0.14997607, 0.97533923, 0.99419665, 0.47263044, 0.00065227883, 0.011337599, 0.00011942867, 0.9999833, 0.9980907, 3.422313e-05, 0.9990477, 0.9997795, 0.99398416, 9.7411554e-05, 0.0017648343, 0.9999411, 0.0012705432, 1.9240038e-06, 0.9972287, 0.99794894, 0.00056412735, 0.008219165, 0.9998258, 0.007994901, 0.9999411, 0.0013291282, 0.98239523, 0.009293684, 0.9992693, 0.98994714, 0.00023919536, 0.9986022, 0.9917847, 0.0004941758, 0.99852896, 0.9050637, 1.8296649e-05, 0.9133617, 0.020886444, 0.9993401, 0.9981633, 0.69739413, 9.420562e-08, 0.99982905, 0.6951041, 0.9766708, 3.0346453e-05, 0.99763393, 0.9980946, 0.9821066, 0.996075, 0.00022669575, 0.9936371, 0.21167001, 0.26221913, 0.9153444, 0.9533832, 0.5957647, 0.9999962, 0.00026099192, 0.00039839573, 0.9016411, 0.0005588293, 0.99984586, 0.9691281, 0.93568355, 0.99971646, 0.001344069, 0.99961334, 1.0564283e-05, 0.06660563, 9.7253396e-05, 0.9963568, 0.99419737, 0.0005863296, 0.99991894, 0.9986644, 0.00021801848, 0.9997813, 0.97948724, 0.994323, 0.8576575, 0.005445594, 5.441015e-08, 0.8418859, 0.00072846684, 0.999653, 0.008471254, 0.99919564, 0.99212474, 9.100186e-05, 0.04562407, 0.9993549, 0.9997009, 0.9999534, 0.0001932233, 0.9993393, 9.28501e-06, 0.09545086, 1.714882e-05, 0.081469305, 0.96356577, 0.062422074, 0.99165314, 0.9419531, 0.00129905, 0.0082580475, 0.99920386, 0.38133162, 0.9870705, 0.9999485, 0.0048372354, 0.0017131184, 0.99446, 0.99998033, 0.9992986, 0.0043084016, 0.9921566, 0.12506478, 0.022788726, 0.013477287, 0.99987423, 0.0004837354, 0.0011034774, 0.002818037, 5.677749e-05, 0.00073896773, 0.9992735, 0.98174864, 0.00049922185, 0.9997466, 0.9992956, 0.99999917, 4.659075e-05, 0.49668968, 0.44749886, 0.9972269, 0.7017598, 0.0014146104, 0.79080915, 0.41115606, 0.9996346, 9.70655e-05, 0.52691835, 0.0014484173, 0.0038624867, 0.9858204, 0.00012407158, 0.19372554, 0.0006224826, 0.69775933, 0.17224166, 0.9989778, 0.0010200809, 0.9999751, 0.9995357, 7.680214e-05, 0.9999461, 0.9993445, 4.5521334e-05, 0.99737334, 0.9995764, 0.98132354, 0.994936, 0.9978109, 4.178391e-06, 0.9989151, 0.0055116834, 0.00027939334, 0.012778038, 0.06938051, 0.358421, 0.11681728, 0.00057053263, 0.92604244, 0.99978644, 0.9998485, 3.6049783e-05, 0.00016160296, 0.99802774, 0.99108094, 0.9999889, 0.99999785, 0.057768747, 6.6984e-06, 0.9999083, 0.9025272, 0.99998605, 0.008121336, 0.00026193447, 0.99939203, 0.017280925, 0.0051838052, 0.97516507, 0.009935119, 0.99998, 0.9999461, 0.0002615362, 0.007838678, 0.05495346, 0.9996593, 0.8402317, 0.9942958, 0.009709778, 0.99985373, 0.0046292106, 0.00013964846, 0.04869307, 4.756475e-05, 0.99451643, 0.00020765686, 0.9998447, 2.8537312e-05, 0.40693915, 7.4681106e-05, 0.98681706, 0.0004496689, 0.98795867, 0.999987, 0.99935824, 0.8622153, 0.017286602, 0.99894184, 0.00012646108, 0.94126785, 0.99934393, 0.978336, 0.997036, 0.97735935, 8.884983e-05, 0.9992506, 0.002992981, 0.019224852, 0.9999304, 0.9999937, 0.87849075, 0.0068921573, 0.000496292, 0.91004026, 0.0010730992, 0.010485108, 3.2605578e-05, 0.37707895, 0.0057601263, 0.9999229, 0.99977285, 0.99203837, 0.9916449, 0.8277993, 0.0011242041, 0.0066426764, 0.050066147, 0.01240721, 0.001081401, 8.8198736e-05, 0.99997497, 7.9341764e-05, 0.9997578, 0.23417844, 0.99977666, 0.95592034, 0.760287, 0.99968874, 0.9570846, 0.99861276, 0.043404806, 0.0004063364, 0.027640145, 0.07290378, 0.0006882259, 0.0043071867, 0.0054005696, 0.0015344131, 0.99705017, 0.99979764, 0.49005738, 0.0006503174, 0.99999034, 0.9993955, 0.00053307385, 0.9997564, 0.0039159358, 0.0044870363, 8.232703e-06, 0.31077006, 0.022461817, 0.99987495, 0.0053637796, 0.9994772, 0.9998338, 0.99999535, 0.99975246, 2.9502686e-05, 0.0077633043, 0.99992776, 0.9997584, 0.007525664, 0.0013722111, 0.07810704, 0.99994266, 0.07048097, 0.99999833, 0.0001042693, 0.5843708, 0.99823207, 0.9986512, 0.00067464844, 0.90135306, 0.0018039002, 0.0020185825, 0.972583, 0.999328, 0.16351144, 0.9949897, 0.0014994086, 0.003246877, 0.984894, 0.00030298467, 0.99942446, 0.99957114, 0.9983078, 0.0019136035, 0.00023717555, 0.99987316, 0.9979273, 0.00066878134, 0.00029515178, 0.99993956, 0.0027787823, 0.99998367, 0.00090183294, 0.014278659, 0.9994092, 0.00017303918, 0.9999746, 0.99944216, 0.06963609, 0.99983263, 0.0022459135, 0.000399404, 0.9824133, 0.99995875, 0.96783686, 0.0017344846, 0.00036619484, 0.0014463837, 0.9998332, 0.99846137, 0.06908574, 0.0015333906, 0.996675, 0.0028503323, 0.0007017807, 0.9797166, 0.006426842, 0.99886703, 0.007905194, 0.992046, 0.26210874, 0.99279046, 0.990854, 0.99998987, 2.3366572e-05, 0.9963341, 3.2634944e-05, 0.0026178092, 0.000844172, 0.13826601, 5.1985517e-05, 0.9994435, 0.0018939142, 0.0003587667, 0.97704166, 0.0018373714, 0.9944179, 0.7418646, 0.99939084, 0.99996924, 0.7913339, 0.9954326, 0.065720685, 0.9999379, 4.5731624e-05, 0.0003522097, 0.002067005, 0.5122321, 0.00019244355, 0.9993525, 0.99949145, 0.9994423, 0.99838793, 0.9990834, 0.9982589, 0.99998903, 0.00014622074, 0.0005948305, 0.98887324, 0.0017878406, 0.98542154, 0.99693066, 0.99437344, 0.0068551665, 0.0010287067, 0.03801919, 0.0014894677, 0.99869955, 0.94481605, 0.99761045, 0.00028023878, 0.86269945, 0.99176896, 0.052170306, 0.041973617, 0.59868824, 0.99579704, 1.6250038e-05, 0.0066268397, 0.9972837, 1.5847301e-06, 0.99918157, 0.99726415, 0.0031234126, 0.99984837, 0.9997278, 0.0020624914, 0.9975555, 2.2420209e-05, 0.9994103, 0.9998424, 0.39695412, 0.9959915, 0.018460536, 0.9962973, 0.0061361347, 0.99998724, 1.6042062e-05, 0.80173135, 0.99996984, 0.00016586187, 0.9989747, 0.91671956, 0.0007460339, 3.5609935e-06, 0.04352021, 0.98351836, 0.00082797656, 0.9999337, 0.0006976759, 0.9987601, 0.9996904, 0.008623628, 0.014376342, 0.5608985, 0.96189296, 0.98641807, 0.14107771, 0.9995041, 0.0007135661, 0.9997378, 0.9997756, 5.062719e-05, 0.9962375, 0.9996402, 0.0014649496, 0.9999068, 8.733651e-06, 0.9999378, 0.9890627, 0.25867444, 0.76139647, 0.005827933, 0.7772907, 0.99899524, 0.0007212256, 0.933619, 0.044601314, 0.9979673, 2.174442e-05, 0.0002246039, 0.004519977, 0.9947014, 0.0004509552, 7.023491e-05, 0.9944824, 0.13084358, 0.90664375, 0.9972465, 0.99519855, 0.03436244, 0.99870205, 0.99950945, 5.18972e-05, 0.03677611, 0.9999664, 0.9993728, 0.0027449187, 0.99989533, 0.0010419571, 0.99993193, 0.9999896, 0.999132, 0.0070758625, 0.7739968, 0.022461265, 3.3246644e-05, 0.03405237, 0.9952803, 0.028169759, 0.9999616, 0.0037926212, 0.00313309, 0.0113604665, 0.96777904, 0.0002335332, 0.983928, 0.00050302816, 0.9999163, 0.0017362962, 0.99902034, 0.00016184151, 0.0002544927, 7.656668e-05, 0.98349345, 0.9996722, 0.00044851072, 0.99944514, 0.00078523485, 0.9993407, 0.00029950522, 0.309611, 0.061722927, 6.2084946e-05, 0.9773177, 0.99994624, 0.00018133265, 0.0006603033, 0.993404, 0.994167, 0.99987495, 0.9994031, 0.00047806615, 0.0010045948, 0.14888139, 0.99557024, 8.806611e-05, 0.0018533125, 0.99905604, 0.99822444, 0.8955093, 0.9993081, 0.99900216, 0.9994374, 0.9917318, 0.0011760993, 0.99874604, 0.98525256, 2.494655e-05, 0.9987056, 0.0060526924, 6.22972e-05, 0.99958783, 0.99822646, 0.99376667, 0.9996234, 1.711113e-05, 4.7241127e-05, 0.0007482725, 0.999461, 0.028849997, 0.9992798, 0.0065434137, 0.008235891, 0.9896853, 0.0013705128, 0.00023275494, 0.98463446, 0.99877816, 0.9996872, 0.9918298, 0.026501432, 0.9967629, 0.98120314, 0.002375106, 0.9795966, 0.9986564, 0.99839956, 0.0023554873, 0.0012127733, 0.99946004, 0.004686093, 0.00058264425, 5.045344e-05, 0.039230928, 0.99999297, 0.00059821876, 0.00025500252, 0.004727227, 0.20877816, 0.9999454, 0.9998129, 0.0362246, 0.9999881, 0.017752256, 0.99983835, 0.9992907, 0.8366257, 0.9999652, 0.0009018363, 0.057208646, 0.23487476, 0.0014373959, 0.9999647, 0.0023944906, 0.00096133514, 0.9999962, 0.002254594, 0.96134555, 0.00013640926, 0.99093854, 0.00013566793, 0.99967086, 9.893239e-05, 0.995663, 0.99898726, 0.99925524, 1.6758784e-06, 0.999811, 0.9954277, 0.005411723, 0.0020597754, 6.574961e-05, 1.9825746e-05, 7.5906e-06, 0.9127981, 0.9937947, 0.027414571, 0.0020303938, 0.00019970762, 0.9967271, 0.9971552, 0.980292, 0.89150405, 0.99994004, 0.9514516, 0.7887077, 0.99936646, 0.9999219, 0.991928, 0.00014991342, 0.00085146213, 0.97673076, 0.5085884, 0.000180564, 0.0018632228, 0.0022745696, 0.99958557, 0.0057888, 0.009387835, 6.8459936e-05, 0.81643814, 0.9936172, 0.0010043484, 0.99996436, 0.9999455, 0.9950529, 0.015193128, 0.9989255, 0.9998722, 0.99963903, 0.9211926, 0.99981767, 0.9964001, 3.045735e-05, 0.9961409, 0.00040642373, 0.9996573, 0.99250776, 0.985677, 6.4681524e-07, 0.7461513, 0.99988985, 0.004483751, 0.6736429, 0.024326317, 0.0011360905, 0.00080706825, 0.00032864817, 0.0798265, 0.9494549, 0.0033866018, 0.0012095781, 0.9994332, 0.00045030055, 0.000749481, 0.9998642, 1.1935751e-05, 0.0500754, 0.9993843, 0.99915564, 0.00027623138, 0.99282044, 0.9556128, 0.9999795, 0.76543987, 0.99861217, 0.9999722, 0.99999523, 0.00065155566, 0.9974269, 0.02355761, 0.028777543, 0.9988464, 0.9999491, 0.002237968, 0.99985635, 0.10522063, 0.022291189, 0.9959246, 0.00040438992, 0.99997246, 0.9977259, 0.0002918605, 0.0012276927, 0.02233695, 0.97612494, 0.021700501, 0.068416, 0.9996635, 0.0016456484, 0.0030619232, 0.01577951, 0.040248375, 0.00015750398, 0.9999945, 0.99996734, 0.0021676754, 0.9999151, 0.0018435594, 0.99961984, 0.9965126, 0.00030759964, 0.9846443, 0.00020438255, 0.99963665, 1.0348539e-05, 0.0013903731, 0.98024553, 7.896323e-06, 0.9978929, 0.99119544, 0.9996679, 0.9977823, 0.0019112764, 0.07029434, 0.9999682, 0.0003868603, 0.99564797, 0.99970394, 0.005116821, 0.9950494, 0.09658577, 0.9999839, 0.0003019906, 0.99790454, 0.0065718004, 0.00052777515, 0.007208607, 0.0007965296, 0.99993145, 0.99873394, 0.96844125, 0.0010222606, 0.99931526, 0.008501116, 0.0770854, 0.9983321, 0.000113890936, 0.9278252, 0.99533623, 0.9995566, 0.03990291, 0.99967694, 0.89745355, 0.9999974, 0.06227296, 0.0044589317, 0.0001123295, 0.9888678, 0.00060369645, 2.436029e-05, 0.9638626, 0.16706146, 0.9966677, 0.99198955, 0.000984354, 0.99529904, 0.00018656433, 0.98950094, 0.9993843, 0.9995983, 0.006312468, 8.406581e-06, 0.99953043, 0.010593539, 0.07962331, 0.94885725, 0.009215528, 0.9997125, 0.9982278, 0.9998234, 0.9997062, 0.93095195, 0.00023414899, 0.99915266, 0.571661, 0.99671644, 0.82513404, 0.00018172106, 0.00027496094, 0.869128, 0.99537414, 0.9990578, 2.0921196e-05, 0.0008527523, 0.001976441, 0.0021275845, 0.21859638, 0.9945468, 0.9999647, 0.0013523728, 0.99995995, 0.00029970062, 0.00046307742, 0.9992205, 0.98809344, 0.00033539426, 9.7106335e-05, 0.017911565, 0.99999106, 0.9916015, 0.00030141044, 0.0012915849, 0.99760866, 0.9986053, 0.28400835, 0.0076806806, 0.041043542, 0.00803209, 0.99985814, 0.01047733, 0.0013552319, 0.9999683, 0.99998605, 0.9998332, 0.10248672, 0.9953433, 0.0018438112, 0.9998815, 0.12163129, 0.9999945, 0.9999628, 0.06984152, 0.9997532, 0.9940376, 0.8444694, 0.0014315138, 0.99093425, 0.00012813826, 0.012497383, 0.0032355927, 0.00015680717, 1.6044429e-06, 0.00029425297, 0.4493463, 0.00010419883, 0.9979976, 0.9876835, 0.00011517924, 1.6111147e-05, 0.97210896, 0.99931335, 0.9861471, 0.9986091, 3.7307014e-05, 0.0038226857, 0.9999819, 0.9521283, 0.9980605, 0.89191407, 0.0005186615, 0.993328, 0.9950178, 0.1582143, 0.99500823, 0.00036731822, 0.998973, 0.9915627, 4.4184264e-05, 0.9973354, 0.92436093, 0.41404042, 0.9996301, 7.3107976e-06, 0.91918087, 0.9929537, 0.01086153, 0.9981792, 0.00012967076, 0.9999583, 0.9993112, 0.98208976, 0.00047457963, 0.97666216, 2.3328664e-06, 0.8993133, 0.999263, 0.99993813, 0.9981152, 0.020887375, 0.9985177, 0.16363429, 0.9442982, 0.00806094, 0.9993013, 0.98922205, 0.0012330802, 7.611917e-05, 3.47468e-05, 0.9991153, 0.002484105, 0.9967811, 0.012126012, 0.897667, 1.4655502e-05, 0.9914648, 0.08557284, 0.00041510837, 0.02480429, 0.9999958, 0.020752802, 0.00163982, 0.9973041, 0.79109365, 0.052067786, 0.071355775, 0.99994075, 0.0012719568, 7.723834e-05, 0.0008013476, 0.999995, 0.9986821, 0.0002392414, 0.00071235775, 0.0028144617, 0.0007610911, 0.0054785237, 0.0027780726, 0.9999809, 0.042508278, 0.9996012, 5.0406838e-05, 0.9977387, 0.9962692, 0.99997413, 0.99989736, 0.99996936, 0.21807902, 0.97296685, 0.98638445, 0.9925057, 7.084543e-05, 0.0001589939, 0.990021, 5.464037e-05, 0.9729189, 0.9982169, 0.013301238, 0.7685305, 0.99990654, 0.9998969, 0.00042323768, 0.9952022, 0.0023623519, 0.9928201, 2.7358175e-05, 0.5776726, 0.00021132406, 0.011085206, 0.29119852, 0.9999211, 0.015361504, 0.999845, 0.9929597, 0.9982139, 0.0021999865, 0.021858774, 0.9549637, 0.99873585, 0.99971884, 0.00014768208, 0.9530736, 0.00030499688, 0.9998735, 0.0034059095, 0.8129546, 0.00016205847, 0.9999757, 0.9939709, 0.0002602415, 3.4400834e-06, 0.11998663, 0.99868315, 0.06663979, 0.0009437429, 0.015583625, 0.021144666, 0.9948442, 0.9999751, 0.99995863, 0.99904376, 2.938966e-05, 0.9989574, 0.999686, 0.21519652, 0.9870443, 0.96344346, 0.0007538307, 0.01167507, 0.002978388, 0.004998, 0.99848074, 0.9994199, 0.9982412, 0.99884725, 0.9997491, 0.90562415, 9.360902e-06, 0.99756026, 0.010321618, 0.8754669, 0.99867105, 0.004427012, 0.9928697, 0.9196866, 2.59225e-05, 0.96368754, 0.9961318, 4.4650087e-06, 0.9743333, 0.9998908, 0.0052158334, 0.9359547, 0.000549443, 0.34341615, 0.0009481935, 0.8246328, 0.9045109, 0.19418791, 0.0022177198, 0.9999088, 0.99370193, 0.00040878815, 0.99947387, 0.00048216007, 0.9588406, 0.99754333, 0.99818003, 0.6553369, 0.97567534, 0.00028372317, 0.00017154963, 0.003947671, 8.7846114e-05, 0.9970741, 0.0018311841, 0.9848642, 0.9581582, 0.00022469171, 7.923547e-06, 0.99973613, 0.99901867, 0.99921334, 2.7393655e-05, 0.7829827, 3.26688e-06, 0.0016044865, 0.10979257, 0.98800963, 0.971106, 0.9939078, 0.9920449, 0.9988257, 0.9913539, 0.97062063, 0.99018973, 0.9750463, 0.18288885, 0.0007459561, 0.77272016, 0.9982406, 0.99622285, 0.99911994, 0.99279743, 0.0012153671, 0.0026027544, 0.9985001, 0.0022043944, 0.0038681985, 9.598437e-06, 0.9999908, 0.010891062, 0.03046109, 0.9839166, 0.0053416113, 1.5840073e-05, 0.9928964, 0.9598257, 2.1055179e-05, 0.9999709, 0.0018293348, 0.9081431, 0.9999739, 0.0014425314, 0.9968753, 0.0007805088, 0.99986875, 0.99932325, 0.9987606, 0.13191816, 0.9768339, 0.998792, 0.99954885, 0.9999243, 0.051699746, 0.63095087, 0.9995407, 0.0004488516, 0.9999083, 0.999887, 0.9965389, 0.9989549, 0.0020509192, 1.6131021e-06, 0.9985806, 0.9824707, 0.040118776, 2.8871682e-05, 0.08470566, 0.03345279, 0.544396, 0.00062164135, 0.00020747121, 0.99739444, 0.0010297706, 0.99996424, 0.0010038806, 0.9999459, 0.0017112902, 0.25331092, 0.00071686506, 6.296775e-05, 0.0031072237, 0.9986212, 0.9962729, 0.032645453, 1.4708245e-05, 0.99150777, 0.99999416, 0.26388058, 0.0017102144, 0.99835926, 0.98963296, 0.9984132, 0.9999572, 0.0018993999, 0.39129683, 1.7457884e-05, 0.010819985, 0.9997297, 0.007226802, 0.98281235, 0.0013173787, 0.00012960116, 0.00021485782, 0.9986071, 0.99969375, 0.00057475467, 0.3725171, 0.99995863, 0.9993476, 0.02715551, 0.9991341, 0.018091055, 0.9999703, 0.03240646, 0.99590945, 0.0486301, 0.5521735, 0.0840774, 0.98077583, 0.060808998, 0.00026431947, 0.00174134, 0.062121138, 0.08947332, 0.059639964, 0.0008699901, 0.9994862, 0.99148995, 0.9999646, 0.99904484, 0.0009813121, 0.022874556, 0.0011637505, 0.0050926013, 0.00039945543, 0.035488974, 0.0027346718, 0.9999769, 0.994266, 0.9993969, 0.99999964, 0.9997409, 0.029744001, 5.586105e-05, 0.0051458417, 0.9983398, 0.9999937, 5.5046476e-05, 0.0005944276, 0.0006120944, 0.018441278, 0.9995353, 0.9972499, 0.9983903, 0.017097821, 0.10891235, 0.7851385, 0.99044925, 0.01246352, 0.99663407, 0.99996674, 0.9810987, 9.234546e-05, 0.95589954, 0.97696656, 0.9089671, 0.99813205, 0.00021226815, 0.99712795, 0.0013194958, 0.0060731345, 0.015050339, 0.99773645, 0.122473545, 0.9976285, 0.9922537, 1.9803316e-05, 0.6048944, 0.99374986, 0.9959694, 0.9870547, 0.0005628121, 2.0993932e-06, 0.99996424, 0.8060533, 0.99957067, 0.99993134, 0.9988716, 0.69027, 2.4382067e-05, 0.98487353, 7.962243e-05, 0.9964574, 0.0079578, 0.69623446, 0.88900775, 0.9998703, 6.791015e-06, 0.0001588916, 0.98461837, 0.00011325856, 0.99350315, 0.005417285, 0.9714986, 0.5152646, 0.99450797, 0.93123347, 0.00010600489, 8.560874e-06, 0.99098235, 0.9991536, 0.004691614, 0.9997851, 0.00069329876, 0.0030812642, 0.99535674, 0.7596709, 0.9999281, 0.9984426, 0.99994195, 0.003860623, 0.003271595, 7.734307e-05, 0.99991155, 0.99992394, 0.0923855, 0.62488157, 0.006514662, 0.9990268, 0.0060978713, 0.65378314, 0.99950135, 0.0040831957, 1.4847073e-05, 0.99967825, 0.00096370064, 0.6793935, 0.99111253, 0.96289396, 0.9992718, 0.9886, 4.4378103e-06, 0.98486024, 0.00012217925, 0.00022096992, 0.9619982, 0.9992588, 0.98982936, 0.91523284, 0.9985071, 0.9895113, 0.9985121, 0.00040246165, 0.9724267, 0.99834263, 0.99344206, 0.7260406, 1.1479272e-05, 0.9717943, 0.00015070818, 0.0011658836, 0.99950254, 6.731937e-05, 3.116917e-05, 0.94294, 0.9185864, 0.0073703695, 0.9971117, 0.3261585, 0.99927825, 0.8461003, 0.9974023, 0.9996282, 0.99867404, 0.9995371, 0.9844263, 0.9996208, 0.0006357408, 0.00015937442, 0.999047, 0.000318659, 0.99814856, 0.9747243, 0.007023199, 0.52002275, 0.008242094, 5.6941837e-05, 0.9999428, 0.0005462494, 0.99981827, 4.607108e-05, 0.0018685742, 0.998254, 0.99838686, 0.038115025, 0.48843616, 0.00690105, 0.11167227, 0.99843293, 0.49522236, 0.9976973, 9.57015e-05, 0.002539823, 0.35516146, 0.000227644, 0.18759328, 8.018346e-06, 0.00024852777, 0.998987, 0.99684167, 0.00019239089, 0.9765029, 0.99995315, 0.9989667, 0.99817276, 0.0007305277, 0.9938824, 0.9898949, 0.38671783, 3.928159e-05, 0.9977671, 2.0732472e-05, 0.99246395, 0.00034521535, 0.933672, 0.9990011, 0.13563341, 0.98287547, 0.009106599, 0.99921393, 0.9261721, 0.99811375, 0.99604625, 0.99847955, 7.808401e-06, 0.0004396662, 0.99862766, 0.9997837, 0.9993205, 0.00020145199, 0.9999721, 0.009726299, 0.9901924, 0.9724988, 0.99908674, 0.9209098, 0.009736697, 0.9989742, 0.99288017, 0.0008874586, 0.9910822, 0.00027802886, 0.03400017, 0.0026122201, 0.71502525, 0.99104416, 0.00026534006, 3.9257493e-06, 0.999665, 0.00010407439, 0.00014771489, 0.99796826, 0.99932766, 0.0850546, 0.0007502428, 0.9993994, 0.9894482, 0.012166049, 0.99991345, 0.9933252, 0.00041289965, 0.97943455, 0.99999595, 0.996221, 1.5626163e-05, 0.9315582, 0.99990857, 0.9999013, 4.649909e-05, 0.82850283, 0.0039007862, 0.9827542, 0.9705766, 0.35370627, 0.00021729918, 3.1858513e-06, 0.0131197395, 0.0001287822, 3.2391763e-05, 0.99810755, 0.93188965, 0.99926704, 0.0028801619, 0.9998155, 0.99998534, 0.25789472, 0.011609016, 0.9997538, 0.0006422128, 0.012995591, 0.0095127765, 0.0021123406, 0.00037913417, 0.57902396, 0.9997918, 0.977478, 0.9986754, 0.99917823, 0.9890667, 0.99965334, 0.010468237, 0.0020542333, 0.88057137, 0.9997249, 0.9971558, 0.028189404, 0.008893238, 0.0010085857, 0.0060201427, 0.0064598164, 3.7244294e-07, 0.97889966, 0.99994993, 0.99971503, 0.9999336, 0.9987287, 0.6055922, 0.00024810186, 0.99984753, 0.05383031, 8.337558e-05, 0.43332317, 0.99411607, 0.99527234, 0.012714708, 0.99999785, 0.0014565192, 0.004963559, 0.99993956, 0.04666091, 0.9744065, 0.00040933857, 0.9997521, 0.00020373698, 0.998679, 0.058881044, 0.9995913, 0.99794525, 0.008760204, 0.9999863, 0.00035884607, 0.012351546, 0.012016518, 8.879342e-05, 0.044783626, 0.085582525, 0.07114879, 0.0007206399, 0.068211794, 0.0015717774, 0.99994075, 0.9999105, 0.9955799, 0.00022833723, 0.9974401, 0.0125521645, 0.9999951, 0.99993145, 0.520185, 0.00034617406, 0.00027849362, 0.0024220266, 0.99624497, 0.00012173257, 0.9998708, 0.999361, 0.9833302, 0.02322446, 0.99921584, 0.99734443, 0.94854546, 0.00190876, 0.00029505556, 0.023771634, 0.9845388, 0.16986237, 0.0006148946, 0.00593929, 0.9997714, 0.99867237, 0.005356564, 0.9999356, 0.00016062558, 0.99977785, 0.0045296894, 0.0004761874, 0.116511926, 0.01837792, 7.098501e-05, 0.9997527, 6.8630674e-05, 0.99997044, 0.9970999, 0.9999075, 0.9955511, 0.97840846, 0.0037049484, 0.99933845, 0.07729582, 0.00012756574, 0.9992009, 0.00065425155, 0.9987191, 0.99051815, 0.99985325, 0.76008487, 7.753322e-05, 0.00059522066, 0.0040436946, 0.93709725, 0.00060820754, 0.028701017, 0.08016209, 0.0026984455, 0.9999938, 0.99889356, 0.99822575, 0.0020738856, 0.99996066, 5.13875e-05, 0.99188787, 0.00045065721, 0.002254035, 0.0012664034, 0.9999913, 0.99999404, 0.00044423548, 0.66951954, 0.0030380487, 0.9996861, 0.9997148, 0.5229994, 0.9999075, 0.040866226, 0.45325658, 0.9942363, 0.005779238, 0.99903345, 0.0022292868, 0.99939287, 7.44224e-05, 0.9991547, 0.96890277, 0.0028189954, 0.00027802063, 0.0033818574, 0.59421384, 0.99869823, 0.4339332, 1.8360636e-06, 0.8074183, 0.9992694, 0.9964401, 5.9749646e-06, 5.2735842e-05, 0.0010222509, 0.9975848, 0.99771535, 0.0165842, 0.9928213, 0.99301153, 0.7794724, 0.00024962134, 0.976858, 0.93339014, 0.9993199, 0.997271, 0.9999815, 0.97909755, 0.00012680957, 0.00017223177, 0.99862933, 0.00072558597, 0.94483125, 0.003912013, 0.42780006, 0.99956137, 0.99963915, 0.9985429, 5.6161355e-07, 0.97775567, 0.00043893655, 0.9783053, 0.9934394, 0.99994767, 0.00886729, 9.42102e-05, 0.9999211, 0.0032363003, 0.07017721, 0.99964416, 0.9993814, 0.021514585, 0.0028566942, 0.1052379, 0.0007931817, 0.9998543, 0.13481064, 0.99950385, 0.9614748, 0.04402075, 0.0138873905, 0.032476123, 0.00012804774, 0.50595456, 0.004644865, 0.07302317, 0.30573872, 0.99997246, 0.9295082, 3.3202343e-06, 0.9999964, 0.99998236, 0.00033756244, 0.9813141, 0.98409015, 0.000108576, 0.9997749, 0.00010361809, 0.99550337, 0.99944216, 0.020205082, 0.08158199, 0.99959666, 0.99788266, 0.99672157, 0.9982141, 0.7801922, 0.14976844, 0.99891627, 0.0015645721, 0.9480377, 0.000710604, 0.00016777156, 0.9996593, 0.9932319, 0.886503, 5.728966e-05, 0.9953927, 1.8265355e-05, 0.49770105, 0.00094305933, 0.9969319, 0.99821645, 0.0012417944, 0.0001294758, 0.99924004, 0.9962174, 0.99992454, 0.00015323388, 0.9995993, 0.99787104, 0.9959055, 2.5658076e-06, 3.871796e-05, 0.99148315, 0.000103939215, 0.00084999367, 0.84190005, 0.7570882, 0.95404583, 0.0014926112, 0.9997831, 0.00018409325, 0.9959111, 9.96592e-05, 0.00095638045, 0.99992657, 0.000732243, 0.00046517406, 0.888711, 0.99623895, 0.9891889, 0.995605, 0.9921164, 0.97681856, 0.9998838, 0.9951375, 0.96886194, 0.99935395, 0.00048300144, 0.00067939336, 0.94959193, 0.9988777, 0.99891734, 0.47977495, 0.99781007, 0.9867563, 0.97137016, 0.00027428308, 0.00033249584, 1.9396227e-06, 0.9968798, 0.005590609, 0.99914193, 0.0003679035, 0.91230696, 0.9997751, 0.007728716, 1.11904e-05, 0.9855585, 0.9816549, 0.0030060376, 0.9690131, 0.23729919, 0.99891794, 0.045671444, 0.0009353475, 0.99994814, 0.99871325, 0.00038114472, 0.002527537, 0.00036020574, 0.7431683, 0.9976876, 2.2355689e-05, 0.99865556, 0.99979943, 0.9986627, 0.99823296, 0.00015810858, 0.99961746, 0.09754959, 0.008232851, 0.999918, 0.5018083, 0.1880432, 0.5422301, 0.00011320318, 0.9992785, 0.9874964, 0.99972683, 1.903538e-05, 0.044957772, 0.00038751142, 0.9998547, 0.15892652, 0.026465226, 0.033802032, 0.9964552, 0.96878517, 0.00086033845, 0.99998856, 0.007869209, 0.93372023, 0.0018237777, 0.49303988, 0.031826958, 0.002923447, 0.99997807, 0.0034181187, 0.99929225, 0.99999666, 0.0009929385, 0.14324814, 0.104971476, 0.9999939, 0.06396798, 0.36200947, 0.99865746, 0.00021356985, 0.9999815, 0.99379575, 0.008657301, 0.00015450917, 0.010922879, 0.000106330466, 0.008488164, 0.0047118594, 0.99889815, 0.9997104, 0.4147252, 0.99909186, 0.99872094, 0.0004926867, 0.0048066247, 0.000502291, 0.9999846, 0.00013871692, 0.0063060587, 0.9993893, 0.808071, 0.9999317, 0.0030634007, 0.38667578, 0.47104985, 0.07078331, 0.043045577, 0.9748511, 0.99999964, 0.99803466, 0.00013548926, 0.9998068, 0.9616475, 0.005736389, 0.0013214464, 0.08981179, 0.0003556086, 0.019138483, 0.99886274, 0.0056192265, 0.9998765, 0.99867696, 0.9977464, 0.0011973903, 0.00011705276, 1.9897116e-05, 0.17171706, 0.99987745, 0.99952173, 0.99351066, 0.9918995, 0.86629575, 0.0005124576, 0.9563981, 0.99864596, 0.0023510656, 0.018962108, 0.0004926179, 0.0015205389, 0.00033108634, 0.00066724967, 0.9971059, 0.9998474, 0.7723161, 0.9997346, 0.010127395, 0.3915818, 0.99931073, 0.9013526, 0.03791004, 0.9997739, 0.99860805, 0.9977851, 0.010021151, 0.9999665, 9.317433e-07, 0.9996414, 0.99999297, 0.0014822657, 0.00014988155, 0.9997484, 0.00027859802, 0.99989235, 0.009355409, 0.99912375, 0.98381424, 0.9998853, 0.95340943, 0.0022742418, 0.99469626, 0.0014013214, 0.9999082, 0.0035257796, 0.9975005, 0.9705475, 0.0001076183, 0.0006154414, 0.9968748, 0.00014223167, 0.0003276352, 0.950089, 7.8350946e-05, 2.7597365e-05, 0.9998259, 0.9951551, 9.352312e-05, 0.99662614, 0.9833161, 0.0014092327, 0.00045746416, 0.0010221438, 0.92166686, 0.00024125694, 0.99889576, 0.00020224665, 0.5520479, 0.99727684, 0.66076285, 0.2565293, 0.0004586796, 0.000107886786, 0.9977584, 0.97819376, 0.99911445, 0.0052131573, 0.00863061, 0.9981084, 0.9964921, 0.9996836, 0.96802783, 5.756117e-05, 0.9987973, 0.9654618, 0.9887306, 0.9953151, 0.9994912, 0.0016749806, 0.0022058622, 0.99605983, 0.99860674, 0.8787738, 0.0011465368, 0.98249626, 0.46182543, 0.000109450644, 0.0037781117, 0.9850503, 0.9927248, 0.9996822, 0.999902, 0.9887406, 0.00090766343, 0.99985313, 0.9528617, 7.485984e-07, 0.00013030464, 0.9993268, 0.9913089, 0.14352842, 0.42735013, 0.99676716, 0.99752, 3.3289856e-05, 0.98649234, 0.99880624, 0.010915607, 0.007334649, 8.83767e-06, 0.99976486, 0.96895546, 0.00023845691, 0.99749976, 0.03366099, 0.9883157, 0.006912556, 7.238586e-07, 0.0027494768, 0.0007065419, 0.94605285, 0.998288, 0.008252601, 0.9877275, 0.99919087, 0.993486, 0.00045597088, 3.3065458e-06, 0.73031086, 0.9929259, 5.3657626e-05, 0.003458616, 0.0053672297, 0.9980861, 0.94658446, 0.99953306, 0.9999502, 0.9973816, 0.9994906, 0.9991549, 0.9904738, 0.94837755, 0.99994576, 0.00031552833, 0.9626447, 0.0011551027, 0.001591998, 0.99998116, 0.000115433235, 0.0002686895, 0.9963362, 0.00024526726, 0.99906045, 0.04494523, 0.17978314, 0.011116403, 0.65489125, 0.84681803, 0.99908495, 0.99903274, 0.00020277532, 0.9431516, 0.99966025, 2.0137048e-05, 0.06029505, 0.014148709, 0.9195446, 0.99891543, 0.99998474, 0.89287424, 0.999603, 1.0991314e-06, 0.99767536, 0.9952421, 0.99288017, 0.9899753, 3.0607272e-05, 0.08132949]\n",
            "3616 1917\n",
            "Accuracy: 0.9624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from orbax.checkpoint.type_handlers import TypeHandler\n",
        "from orbax.checkpoint.type_handlers import register_type_handler\n",
        "from copy import deepcopy\n",
        "\n",
        "state = nnx.state(model)\n",
        "_, state = nnx.split(model)\n",
        "stateOrg = deepcopy(state)\n",
        "nnx.display(state)\n",
        "prng_key_value = state[\"dp1\"][\"rngs\"][\"default\"][\"key\"].value\n",
        "#prng_key_value2 = state[\"dp2\"][\"rngs\"][\"default\"][\"key\"].value\n",
        "#prng_key_value3 = state[\"dp3\"][\"rngs\"][\"default\"][\"key\"].value\n",
        "#prng_key_value = state[\"dp4\"][\"rngs\"][\"default\"][\"key\"].value\n",
        "#print(prng_key_value.dtype)\n",
        "state[\"dp1\"][\"rngs\"][\"default\"][\"key\"] = nnx.VariableState(\n",
        "    type=nnx.Param, value=jax.random.key_data(prng_key_value), tag='default'\n",
        ")\n",
        "\"\"\"\n",
        "state[\"dp2\"][\"rngs\"][\"default\"][\"key\"] = nnx.VariableState(\n",
        "    type=nnx.Param, value=jax.random.key_data(prng_key_value2), tag='default'\n",
        ")\n",
        "state[\"dp3\"][\"rngs\"][\"default\"][\"key\"] = nnx.VariableState(\n",
        "    type=nnx.Param, value=jax.random.key_data(prng_key_value3), tag='default'\n",
        ")\"\"\"\n",
        "\n",
        "#nnx.display(state)\n",
        "# Save the parameters\n",
        "ckpt_dir = ocp.test_utils.erase_and_create_empty('/content/my-checkpoints/')\n",
        "checkpointer = ocp.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/my-checkpoints/state', state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jeAzhDPJYSIp",
        "outputId": "ba4b3b27-0b99-422c-bcf3-1420bd212d39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State({\n",
            "  'bn1': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.03455117,  0.26241738,  0.33922154,  0.10407713,  0.08230435,\n",
            "             -0.32102835, -0.32010397,  0.15780467,  0.29116517,  0.4769199 ,\n",
            "              0.14203657, -0.55824643, -0.02837778, -0.05293359,  0.49786356,\n",
            "             -0.06772533], dtype=float32)\n",
            "    ),\n",
            "    'mean': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([-0.07490372, -0.06379531,  0.04793598, -0.07928919, -0.07841757,\n",
            "              0.07537311,  0.59677255,  1.7198774 , -0.01150165,  0.08214779,\n",
            "             -0.00557855,  0.20106544, -0.06757829,  0.37422734, -0.0277496 ,\n",
            "              0.2827872 ], dtype=float32)\n",
            "    ),\n",
            "    'scale': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([0.8666285 , 0.83494526, 1.3698127 , 0.98531693, 1.1697022 ,\n",
            "             1.0743585 , 1.4047605 , 1.1715559 , 1.1336113 , 1.543907  ,\n",
            "             1.2256165 , 0.9802988 , 1.0446272 , 1.2718362 , 1.2823906 ,\n",
            "             0.95863837], dtype=float32)\n",
            "    ),\n",
            "    'var': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([0.00291741, 0.0031151 , 0.07543166, 0.00211692, 0.00204645,\n",
            "             0.02709916, 0.8817686 , 1.0859945 , 0.03786616, 0.1080988 ,\n",
            "             0.02966389, 0.07393619, 0.00159334, 0.3550671 , 0.02824394,\n",
            "             0.16612124], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'bn2': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([ 0.05878618, -0.11392809,  0.4456851 ,  0.4636922 ,  0.46050212,\n",
            "              0.12284669, -0.0162288 ,  0.09645074,  0.03878656,  0.325701  ,\n",
            "              0.32864955, -0.47257626, -0.3166958 ,  0.1293413 , -0.09870107,\n",
            "             -0.18806048,  0.26203445,  0.07255119,  0.19052115,  0.23563774,\n",
            "              0.1006816 ,  0.29872966, -0.14692608,  0.41545784,  0.45674676,\n",
            "              0.20516509,  0.4478688 ,  0.38881174, -0.129647  ,  0.3732569 ,\n",
            "              0.29808375,  0.6544025 ], dtype=float32)\n",
            "    ),\n",
            "    'mean': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([1.7582502 , 1.6966234 , 0.85422236, 0.79282683, 1.0005958 ,\n",
            "             1.876774  , 1.3194228 , 1.6282679 , 0.65874314, 1.0107261 ,\n",
            "             2.8313918 , 0.8942696 , 0.77017045, 1.6463703 , 0.6587784 ,\n",
            "             0.6788278 , 2.2122364 , 1.2448205 , 1.4692974 , 1.1375773 ,\n",
            "             1.5225232 , 1.0605634 , 0.954641  , 2.1156921 , 1.1161277 ,\n",
            "             1.4097812 , 1.634834  , 1.5427475 , 0.8305698 , 1.2423147 ,\n",
            "             1.0121977 , 1.1967651 ], dtype=float32)\n",
            "    ),\n",
            "    'scale': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([0.9892832 , 1.0272722 , 1.4557784 , 1.3308823 , 0.8148601 ,\n",
            "             1.1865973 , 0.99140257, 1.3214953 , 1.048535  , 1.1989578 ,\n",
            "             0.95009327, 1.1162659 , 1.1363548 , 0.85762775, 0.95374   ,\n",
            "             0.99522364, 0.77637666, 1.0438102 , 1.3618002 , 1.1935546 ,\n",
            "             1.1425685 , 1.1147695 , 0.89872724, 1.1635357 , 1.310377  ,\n",
            "             1.136099  , 0.77933353, 1.3401066 , 1.1762254 , 1.234993  ,\n",
            "             1.130814  , 1.3252205 ], dtype=float32)\n",
            "    ),\n",
            "    'var': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([ 3.7120342,  3.1764886,  2.163216 ,  3.0893657,  3.2723148,\n",
            "              8.924599 ,  5.6302342,  5.7905025,  1.3593363,  2.9983664,\n",
            "             12.727532 ,  2.2597048,  1.4776319,  6.61648  ,  1.8445144,\n",
            "              1.4638693,  7.003222 ,  6.435542 ,  5.5295815,  4.3086553,\n",
            "              6.968475 ,  4.075554 ,  3.643736 , 13.770131 ,  3.5235066,\n",
            "              2.8614686,  5.067847 ,  8.422252 ,  1.8682883,  4.4286833,\n",
            "              3.7829034,  4.0817714], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'bn3': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([ 0.13741702,  0.10430374,  0.03738025,  0.08325984, -0.01331604,\n",
            "              0.08880693,  0.10045969,  0.09504471,  0.16075726,  0.06035723,\n",
            "              0.03432121, -0.02765701,  0.10806862,  0.16850153,  0.04866601,\n",
            "              0.1020349 ], dtype=float32)\n",
            "    ),\n",
            "    'mean': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([2.2576134, 2.82282  , 4.046783 , 1.9246709, 1.0163424, 2.2697551,\n",
            "             2.130457 , 1.0723807, 2.2450144, 0.6588666, 3.3475778, 2.129528 ,\n",
            "             2.4523485, 2.3721259, 1.5096161, 2.3622463], dtype=float32)\n",
            "    ),\n",
            "    'scale': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([1.942329 , 1.3515016, 1.6582822, 1.2409835, 1.2556701, 1.3014222,\n",
            "             1.5101744, 1.5047926, 1.6746172, 0.8537005, 1.4140581, 1.2309651,\n",
            "             1.5700245, 1.3325529, 1.1293257, 1.5271274], dtype=float32)\n",
            "    ),\n",
            "    'var': VariableState(\n",
            "      type=BatchStat,\n",
            "      value=Array([11.915819 , 21.676966 , 29.22253  , 11.066945 ,  2.9188704,\n",
            "             14.610164 ,  9.786393 ,  6.5251956, 14.152573 ,  1.4728594,\n",
            "             22.062506 , 16.929121 , 18.79682  , 10.373651 ,  5.4755807,\n",
            "             13.007946 ], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'dp1': {\n",
            "    'rngs': {\n",
            "      'default': {\n",
            "        'count': VariableState(\n",
            "          type=RngCount,\n",
            "          value=Array(24116, dtype=uint32),\n",
            "          tag='default'\n",
            "        ),\n",
            "        'key': VariableState(\n",
            "          type=RngKey,\n",
            "          value=Array((), dtype=key<fry>) overlaying:\n",
            "          [0 0],\n",
            "          tag='default'\n",
            "        )\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  'linear1': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.14964521, -0.3440017 , -1.5807265 , -0.1651829 , -0.16338302,\n",
            "             -2.204617  , -1.0824177 ,  1.718513  , -1.5015045 , -1.7822933 ,\n",
            "             -1.711733  , -2.075836  , -0.09264389,  1.3382683 , -1.4534051 ,\n",
            "              1.1573975 ], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[-1.2861244e-01,  1.4544072e-02,  2.0446930e+00, -8.4658198e-02,\n",
            "               5.3717382e-02, -1.7377496e-02,  8.6710742e-03,  1.7996250e-02,\n",
            "              -1.6519192e-01, -6.8417735e-02, -8.6963154e-02,  1.6487492e-02,\n",
            "               2.4021350e-02, -6.8404205e-02,  1.5652122e+00, -5.2885926e-01],\n",
            "             [ 8.6625397e-02, -9.0924397e-02, -9.1777958e-02, -2.0060011e-03,\n",
            "               1.7802065e-02, -4.3593902e-02, -2.7397817e-02, -7.6492847e-04,\n",
            "              -5.2324262e-02, -6.3014314e-02, -1.1155581e-01, -3.9027158e-02,\n",
            "               5.8421413e-03, -7.3049339e-03, -1.3413273e-01, -8.6086139e-02],\n",
            "             [-3.0489162e-01, -6.5258823e-02, -3.3954310e-01, -1.8128786e+00,\n",
            "              -3.5847042e-02, -7.3875472e-02,  2.5613959e+00,  1.1470855e+00,\n",
            "              -3.5587144e-01,  2.1040342e+00, -1.2889339e-01, -4.9758796e-04,\n",
            "              -3.6264241e-02, -1.3092076e+00, -2.1286547e-01,  1.7497633e-01],\n",
            "             [-2.9551149e-03,  7.8490011e-02, -1.6990041e-02, -6.7461203e-03,\n",
            "              -1.2271639e-01,  1.8504218e+00, -9.8737283e-03,  9.1828004e-02,\n",
            "               6.9104679e-02, -1.0456445e-02,  8.3140202e-02,  1.9458022e+00,\n",
            "              -1.2705353e+00,  7.0977919e-02, -9.2722774e-03, -6.6827661e-01],\n",
            "             [-1.9238298e+00, -6.7996919e-02, -4.3927029e-02, -8.9696459e-03,\n",
            "              -4.0149181e-03, -4.9249455e-02, -5.6898005e-02, -4.9782310e-02,\n",
            "               1.8322707e+00, -4.4556178e-02, -2.0025413e-01, -7.4732013e-02,\n",
            "               8.3352001e-03,  1.3634704e-01, -7.7170864e-02, -1.2007728e-01],\n",
            "             [-1.3189091e-01, -1.0783378e+00, -9.5631376e-02,  3.1656366e-02,\n",
            "               3.8505998e-02,  3.2306470e-02,  8.8287383e-02,  3.6100394e-01,\n",
            "              -5.8173224e-02,  4.9176212e-02,  1.7016531e+00, -1.6476743e-02,\n",
            "               1.6638381e-02,  7.3459715e-02, -3.3168558e-02,  9.8872744e-04],\n",
            "             [ 3.5825003e-02, -3.4020558e-02,  1.0725083e-01, -2.7215290e-03,\n",
            "              -6.2185593e-02,  3.9084613e-02,  8.8583767e-02, -7.6351754e-02,\n",
            "               5.4628782e-02, -9.2596281e-03, -7.8004137e-02, -5.8118296e-03,\n",
            "              -4.5683723e-02,  3.1953316e-02,  8.6054556e-02, -1.8588377e-02],\n",
            "             [ 1.2456442e-01, -2.3976481e-02, -1.4739120e-01,  3.5266399e-02,\n",
            "              -1.1489070e-02, -4.3521877e-02, -5.0701831e-02,  4.9664281e-02,\n",
            "              -6.6412628e-02, -4.1334279e-02, -1.0163737e-01, -6.2684350e-02,\n",
            "               1.9856635e-03,  1.4831756e-01, -8.6942874e-02, -9.1342673e-02],\n",
            "             [ 2.7303796e-02,  3.7389774e-02, -2.6246328e-02,  4.0069725e-02,\n",
            "              -6.9488777e-04, -1.5655054e-02, -8.9410990e-03, -7.2537787e-02,\n",
            "               2.8446408e-02,  1.3613905e-02,  7.2275952e-02,  3.5781909e-02,\n",
            "              -4.8243821e-02,  4.6300709e-02,  7.1239844e-02,  5.3570732e-03],\n",
            "             [-1.6452895e-02, -1.7300071e+00, -7.3980860e-02, -1.1641509e-01,\n",
            "              -2.0782013e+00, -1.2863994e-01, -9.5501289e-02, -1.5766078e+00,\n",
            "              -5.1252354e-02, -4.7748531e-03, -9.2474453e-02, -6.3485198e-02,\n",
            "              -3.0389637e-01, -1.3201519e+00,  9.0383627e-03,  1.8131706e-01]],      dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'linear2': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([ 0.79973334,  1.1934578 , -0.18998572, -0.32191297,  0.08594196,\n",
            "              0.24928357, -0.16609487,  0.03593244, -0.4570136 , -0.37316453,\n",
            "              0.79988736, -0.07925168, -0.39944988,  0.47070998, -0.0575754 ,\n",
            "             -0.32707843,  0.773781  , -0.32483116, -0.32372716, -0.18130776,\n",
            "             -0.44581324,  0.08696139, -0.6042025 ,  0.52464515,  0.25595632,\n",
            "              0.6856357 ,  0.25225842,  0.15473153, -0.3254921 ,  0.53345984,\n",
            "             -0.08022146,  0.6677654 ], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 3.71105298e-02, -1.81124985e-01, -2.17852313e-02,\n",
            "              -2.04385221e-02,  1.34866047e+00,  6.68254256e-01,\n",
            "               2.71175742e-01, -9.55325589e-02,  8.36512670e-02,\n",
            "               3.21301699e-01,  1.52916968e-01,  3.26933503e-01,\n",
            "              -5.67276657e-01,  5.58007300e-01,  1.31169701e+00,\n",
            "              -9.84522164e-01, -3.21993381e-01, -5.09475231e-01,\n",
            "              -7.26353610e-03, -9.87627357e-02,  4.66725349e-01,\n",
            "               3.54408205e-01, -6.02531552e-01,  2.49999743e-02,\n",
            "               4.78941686e-02,  1.03629321e-01,  6.40615262e-03,\n",
            "              -1.28214611e-02, -5.42690277e-01,  6.87613636e-02,\n",
            "              -2.02495828e-01, -1.18364424e-01],\n",
            "             [-6.16910636e-01,  8.97351801e-02, -5.87723926e-02,\n",
            "              -2.58543670e-01, -2.94428319e-02,  2.07503766e-01,\n",
            "              -3.71390671e-01,  6.64323688e-01,  2.39470880e-03,\n",
            "               1.17985618e+00,  1.21117257e-01, -1.03575933e+00,\n",
            "              -3.25139940e-01, -5.41650355e-01, -3.03467751e-01,\n",
            "              -1.26835227e-01,  7.91002735e-02, -2.18810067e-01,\n",
            "               6.18678391e-01, -1.84124798e-01,  2.05722779e-01,\n",
            "               1.12055933e+00,  1.04292102e-01, -1.37317196e-01,\n",
            "               3.64194401e-02, -2.56052941e-01,  5.17601907e-01,\n",
            "              -2.27352917e-01, -5.00185788e-01,  7.08020627e-02,\n",
            "               5.25816500e-01, -5.61535209e-02],\n",
            "             [-4.16697115e-01, -6.70315623e-01, -2.26785108e-01,\n",
            "               1.23302387e-02, -2.07005441e-01,  6.50782228e-01,\n",
            "               7.75794864e-01, -6.04461133e-02, -5.03383219e-01,\n",
            "              -5.74349649e-02,  1.01125622e+00,  9.35933739e-03,\n",
            "              -8.15597892e-01,  4.04635251e-01, -3.62913400e-01,\n",
            "              -2.17309996e-01,  6.64282680e-01,  1.05779004e+00,\n",
            "              -2.17399329e-01, -2.12645158e-01,  2.90909737e-01,\n",
            "              -6.19629920e-01, -9.40365911e-01, -1.61457002e-01,\n",
            "              -7.15940416e-01,  3.38664144e-01,  6.67507887e-01,\n",
            "              -5.46999574e-01, -1.67451560e-01, -8.47625658e-02,\n",
            "              -3.39323916e-02, -2.58694917e-01],\n",
            "             [-2.44310543e-01, -4.28159744e-01, -1.56940413e+00,\n",
            "               2.55023986e-01,  5.94210744e-01,  1.40785918e-01,\n",
            "              -1.83289289e-01, -6.18427247e-03,  9.74435955e-02,\n",
            "              -8.28210115e-02,  8.01401436e-01,  3.34301561e-01,\n",
            "              -2.20473960e-01,  3.45045403e-02,  3.64299893e-01,\n",
            "              -1.66149154e-01,  5.31608880e-01, -3.36365014e-01,\n",
            "               6.82837218e-02,  8.19549441e-01, -7.13447630e-02,\n",
            "               4.99996781e-01,  8.79559591e-02,  1.01437879e+00,\n",
            "               3.76446187e-01, -4.91683662e-01,  9.73097235e-02,\n",
            "               6.67587280e-01,  4.82171893e-01,  2.14517087e-01,\n",
            "               4.56902891e-01,  1.96443230e-01],\n",
            "             [ 3.76349241e-01, -1.87030405e-01,  1.28324226e-01,\n",
            "              -3.86747569e-01, -2.53798366e-01, -2.70421982e-01,\n",
            "              -6.46416619e-02,  1.19344306e+00, -1.16759002e+00,\n",
            "              -1.32724464e-01,  4.55906726e-02, -4.97476637e-01,\n",
            "               6.45075440e-02, -2.32934952e-01, -9.97308493e-01,\n",
            "              -9.57593620e-01,  1.91632211e-02, -2.18223795e-01,\n",
            "               1.25183976e+00,  1.71011481e-02, -1.09360717e-01,\n",
            "              -5.86194158e-01,  6.71444535e-01, -9.67516899e-02,\n",
            "              -8.94883573e-02,  5.04840553e-01,  3.13145593e-02,\n",
            "              -3.30749124e-01,  1.33589610e-01, -1.85993657e-01,\n",
            "              -7.25243706e-04, -1.59237728e-01],\n",
            "             [-3.52542043e-01, -1.89072996e-01, -3.02349031e-01,\n",
            "               1.30601859e+00,  1.18045941e-01, -3.85253668e-01,\n",
            "              -1.78863078e-01,  8.57947990e-02, -5.27388215e-01,\n",
            "              -2.44654283e-01, -3.75805408e-01, -1.08462147e-01,\n",
            "              -3.39796633e-01, -1.17852896e-01,  2.38102302e-01,\n",
            "               3.01438630e-01, -4.51337039e-01, -2.61151105e-01,\n",
            "              -1.48413137e-01,  6.68698847e-02, -2.09705710e-01,\n",
            "               3.47831436e-02, -1.15556203e-01, -6.17619812e-01,\n",
            "               1.78965271e-01,  2.48570964e-01, -1.79212466e-01,\n",
            "              -6.48650751e-02, -3.29246402e-01,  1.38574886e+00,\n",
            "              -1.08478196e-01,  1.01044250e+00],\n",
            "             [-4.22963202e-01, -4.72676367e-01,  3.11770231e-01,\n",
            "               3.78651097e-02,  2.04085663e-01, -4.18730915e-01,\n",
            "              -9.73669142e-02, -3.13620791e-02, -3.23706090e-01,\n",
            "              -2.80868858e-01, -8.65851939e-02, -5.87071776e-01,\n",
            "              -1.68486252e-01,  6.87920749e-02,  1.69110194e-01,\n",
            "              -4.03389841e-01, -7.13146552e-02, -4.07037474e-02,\n",
            "              -1.56331882e-02,  9.10504758e-01, -1.21602356e+00,\n",
            "               2.17833221e-01,  5.41867912e-02,  1.86469448e+00,\n",
            "               1.07940865e+00, -1.25350013e-01,  3.56518567e-01,\n",
            "               1.36975121e+00, -6.60956139e-03,  5.71484677e-02,\n",
            "              -7.85620958e-02,  7.59814074e-03],\n",
            "             [-2.40781665e-01,  3.62230599e-01,  5.14488399e-01,\n",
            "               2.53509700e-01, -4.05148000e-01, -1.03581798e+00,\n",
            "               5.33553818e-03, -7.58806288e-01,  1.81537308e-02,\n",
            "              -4.98347968e-01, -6.28763735e-02, -1.64575279e-01,\n",
            "               3.01759839e-01, -3.50207657e-01, -7.12045550e-01,\n",
            "              -3.26262027e-01, -3.30899213e-03,  2.23727465e-01,\n",
            "              -7.11323559e-01,  5.31865954e-01, -2.97910571e-01,\n",
            "               1.61905318e-01, -6.88511670e-01,  1.99661776e-01,\n",
            "               5.33244491e-01,  6.39396787e-01,  6.59545898e-01,\n",
            "               4.09299612e-01,  8.47912431e-01,  8.74627661e-03,\n",
            "               2.44202673e-01,  1.94916442e-01],\n",
            "             [-6.72875047e-01,  5.96660897e-02,  1.24226488e-01,\n",
            "              -1.39591182e-02,  1.49438083e-01,  6.89753771e-01,\n",
            "               8.83988217e-02, -2.23033577e-01, -4.42968279e-01,\n",
            "               3.66090864e-01,  8.25541496e-01, -4.56317931e-01,\n",
            "              -6.02269769e-01,  3.33691597e-01,  5.13194680e-01,\n",
            "              -1.00245953e+00, -4.28271830e-01,  1.88973233e-01,\n",
            "              -3.22648555e-01, -3.13625216e-01,  7.31293738e-01,\n",
            "               4.84806508e-01, -7.08209455e-01, -1.36378124e-01,\n",
            "              -5.64614058e-01, -1.83877617e-01,  2.20208820e-02,\n",
            "              -6.21154428e-01, -9.86074448e-01,  3.20286937e-02,\n",
            "               8.00409317e-01, -3.58131528e-02],\n",
            "             [-1.65135634e+00, -1.55719042e+00, -1.75029731e+00,\n",
            "              -1.94858551e-01,  7.39751279e-01, -3.92203182e-02,\n",
            "               2.42092460e-01, -7.62910843e-02,  1.12926111e-01,\n",
            "              -4.55800176e-01,  3.53917122e-01, -6.26911640e-01,\n",
            "              -1.00998104e+00,  1.20894574e-01, -1.89815447e-01,\n",
            "              -1.07756078e+00,  4.12584722e-01,  1.21119916e-02,\n",
            "              -2.55854994e-01,  2.10870326e-01,  3.47252995e-01,\n",
            "               5.70210209e-03,  1.96930170e-01,  5.79855084e-01,\n",
            "               3.17278832e-01, -9.68803406e-01,  2.11704254e-01,\n",
            "               7.93784380e-01,  3.87942821e-01, -8.15721080e-02,\n",
            "               2.36535072e-01,  6.10863790e-02],\n",
            "             [-8.95570815e-01,  7.45356560e-01, -1.28227532e-01,\n",
            "              -6.33339286e-02,  2.77914643e-01,  3.82194459e-01,\n",
            "              -3.84476423e-01, -1.80703476e-01,  2.15220302e-01,\n",
            "               8.60844791e-01, -1.17629297e-01, -3.06581646e-01,\n",
            "              -8.44407737e-01,  2.98988223e-01,  3.51874828e-01,\n",
            "              -5.67705572e-01,  2.18489379e-01,  1.48377568e-01,\n",
            "              -4.86007303e-01, -6.42731339e-02,  4.84707326e-01,\n",
            "               1.03208327e+00, -8.11361134e-01, -2.61677831e-01,\n",
            "              -2.73435652e-01,  1.37773708e-01, -2.62270737e-02,\n",
            "              -2.76583761e-01, -2.61634648e-01,  1.85853049e-01,\n",
            "               9.71235633e-01,  3.49414647e-02],\n",
            "             [-3.90335500e-01, -6.63657188e-02, -2.20326424e-01,\n",
            "               1.64479756e+00,  1.22894630e-01, -3.60475272e-01,\n",
            "              -8.34101811e-02, -1.28842313e-02, -4.60166633e-02,\n",
            "              -2.68591195e-01, -5.10543942e-01, -2.51044154e-01,\n",
            "              -1.16220556e-01, -1.73478201e-01,  1.21367253e-01,\n",
            "               2.30134100e-01, -5.52799225e-01, -7.35643134e-02,\n",
            "               6.98386878e-02, -3.20130698e-02, -1.63442150e-01,\n",
            "              -1.27246203e-02, -2.01568082e-01, -1.76933855e-01,\n",
            "               1.95645854e-01,  1.58494730e-02,  1.52977677e-02,\n",
            "              -7.57793412e-02, -3.90705973e-01,  1.43260574e+00,\n",
            "              -2.35294983e-01,  1.47864842e+00],\n",
            "             [ 2.50764817e-01,  2.39921510e-01, -2.02402756e-01,\n",
            "               3.84336978e-01,  1.71511825e-02, -8.30993950e-02,\n",
            "              -2.68466860e-01, -2.35617340e-01, -1.31929696e+00,\n",
            "              -1.99154671e-02,  2.53109932e-01,  2.02572346e-01,\n",
            "               1.15131550e-01, -2.66683668e-01, -2.39630058e-01,\n",
            "              -2.54762977e-01,  1.66350558e-01, -3.49479556e-01,\n",
            "              -2.95135170e-01, -4.42888811e-02,  5.82453050e-02,\n",
            "               2.83626258e-01, -1.12016499e+00, -4.07879837e-02,\n",
            "               1.79822475e-01,  1.56883824e+00, -6.31742954e-01,\n",
            "               4.31797802e-02,  1.10831439e+00,  4.90365922e-01,\n",
            "               8.05821121e-02,  7.62243688e-01],\n",
            "             [ 1.46635994e-02,  6.38126612e-01, -5.94847620e-01,\n",
            "               6.88937604e-02,  1.82873935e-01,  4.39286500e-01,\n",
            "              -9.91643965e-02, -1.42039692e+00,  1.21812634e-01,\n",
            "               5.52573204e-02, -6.68641210e-01,  8.30745280e-01,\n",
            "               1.45235792e-01,  3.28628242e-01,  3.48790348e-01,\n",
            "               1.70866922e-02, -7.03721166e-01, -6.19768426e-02,\n",
            "              -1.32207465e+00, -1.06183887e+00,  2.33104393e-01,\n",
            "               2.87898093e-01, -1.84707880e-01, -6.12547472e-02,\n",
            "              -9.40007389e-01,  3.32576007e-01,  8.62944871e-02,\n",
            "              -4.29405540e-01,  3.21232855e-01, -2.86581945e-02,\n",
            "              -1.73472419e-01,  1.75494999e-02],\n",
            "             [-6.77180946e-01, -3.09630841e-01, -2.30013832e-01,\n",
            "              -1.68976650e-01,  4.39011306e-02,  6.69696927e-01,\n",
            "               7.21975029e-01,  2.64443547e-01, -3.21092010e-01,\n",
            "               3.32934856e-01,  7.10362017e-01,  2.71458272e-02,\n",
            "              -1.01801348e+00,  8.08219075e-01,  3.09555456e-02,\n",
            "              -1.35634961e-02,  5.11858702e-01,  5.51601470e-01,\n",
            "              -1.26936123e-01, -4.00793940e-01,  8.06359291e-01,\n",
            "              -1.35144815e-01, -7.89977908e-01, -1.12353213e-01,\n",
            "              -3.68500173e-01,  1.16949007e-01,  7.56120533e-02,\n",
            "              -4.84427989e-01, -3.98458838e-01,  6.19583949e-02,\n",
            "              -7.16709316e-01, -3.14000577e-01],\n",
            "             [ 3.64297122e-01, -1.32783741e-01,  3.01918268e-01,\n",
            "              -3.93295169e-01,  6.63314402e-01,  1.41938686e-01,\n",
            "              -1.57856333e+00,  1.65222198e-01, -7.53325701e-01,\n",
            "               1.70564458e-01, -1.12695970e-01,  1.72505423e-01,\n",
            "               2.09245279e-01, -1.01092756e+00,  2.98235118e-01,\n",
            "               5.87296113e-02, -4.37155366e-01, -1.19982040e+00,\n",
            "               2.02304453e-01,  2.40554750e-01,  2.11657628e-01,\n",
            "               5.77274114e-02,  3.22351128e-01,  2.20633522e-01,\n",
            "              -7.97428598e-04,  2.23687321e-01, -1.94999099e+00,\n",
            "               1.05578616e-01,  1.64620593e-01, -8.33527923e-01,\n",
            "               1.53498068e-01, -5.35873771e-01]], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'linear3': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([-0.05060526,  0.6910335 ,  0.3543835 , -0.20975584, -0.24578345,\n",
            "              0.2721161 ,  0.28942192, -0.78330696,  0.20372611, -0.43987548,\n",
            "              0.7480665 ,  0.14616242, -0.5075694 ,  0.40943316,  0.29287735,\n",
            "              0.31391102], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 3.77037711e-02,  1.97828278e-01, -4.92254376e-01,\n",
            "              -6.40792727e-01, -3.46882135e-01, -7.42417634e-01,\n",
            "               2.38110811e-01, -5.34886479e-01, -3.12023640e-01,\n",
            "               3.14161360e-01, -8.53432477e-01,  3.43531549e-01,\n",
            "              -3.49551082e-01, -5.11256456e-02,  4.85384196e-01,\n",
            "              -3.60378981e-01],\n",
            "             [-6.19983613e-01,  4.74745214e-01, -8.52111161e-01,\n",
            "              -4.94828135e-01,  1.58174276e-01,  4.05840904e-01,\n",
            "               1.68311372e-01,  2.87064344e-01, -1.72713056e-01,\n",
            "              -8.57102692e-01,  1.43084869e-01,  3.56439441e-01,\n",
            "              -7.60587513e-01, -4.49508995e-01,  1.91965014e-01,\n",
            "               4.61694837e-01],\n",
            "             [-1.71944082e-01, -8.07273984e-01,  5.55174589e-01,\n",
            "               1.89105943e-01, -1.35344326e-01, -4.50836211e-01,\n",
            "               1.18413806e+00, -3.56913537e-01,  2.64575124e-01,\n",
            "               5.44435978e-01,  2.93034732e-01, -8.14348519e-01,\n",
            "              -5.28922677e-01,  7.95398474e-01, -5.52187413e-02,\n",
            "               5.17937392e-02],\n",
            "             [-2.93334842e-01, -8.14021170e-01,  2.44738266e-01,\n",
            "              -3.31359267e-01, -8.34974349e-01,  3.38837117e-01,\n",
            "              -8.73561800e-02, -7.46537030e-01,  1.03467655e+00,\n",
            "              -4.10196781e-02,  1.86007172e-01, -7.04146445e-01,\n",
            "              -7.08852530e-01,  1.64790168e-01,  2.20806658e-01,\n",
            "               7.61565089e-01],\n",
            "             [-8.06430429e-02,  4.72674280e-01,  2.62647271e-01,\n",
            "               4.27182972e-01,  3.81681733e-02,  1.35030195e-01,\n",
            "              -2.95347542e-01, -7.71440625e-01, -1.40433749e-02,\n",
            "               1.92661956e-02,  2.75654465e-01,  3.19080174e-01,\n",
            "              -2.90898770e-01, -1.95683405e-01, -7.12193787e-01,\n",
            "              -4.14589792e-01],\n",
            "             [ 3.95179451e-01,  9.97435093e-01, -5.71185827e-01,\n",
            "               1.39351368e-01, -7.70607293e-02,  1.48529619e-01,\n",
            "              -9.26757932e-01,  1.94485664e-01, -2.02147231e-01,\n",
            "               1.71459213e-01, -1.16126068e-01,  6.01292729e-01,\n",
            "               3.30801457e-01,  5.55082150e-02, -1.23460412e-01,\n",
            "              -2.63834178e-01],\n",
            "             [ 1.33689806e-01,  5.54750264e-01,  1.40486732e-01,\n",
            "              -3.79715204e-01,  3.54174785e-02,  4.76431638e-01,\n",
            "              -2.49243319e-01,  3.05970937e-01, -6.65807072e-03,\n",
            "              -1.07667461e-01,  9.37985256e-02,  5.64193010e-01,\n",
            "               1.00006402e+00,  9.11136740e-05, -8.06020975e-01,\n",
            "               3.01397353e-01],\n",
            "             [ 1.04648042e+00, -3.10674906e-01,  2.31599525e-01,\n",
            "               1.87457249e-01, -9.53785181e-01,  5.90822138e-02,\n",
            "              -2.10270569e-01,  1.75379794e-02,  2.15874895e-01,\n",
            "              -3.27226788e-01,  1.04892761e-01, -5.76823175e-01,\n",
            "               9.01091769e-02,  7.33099341e-01,  9.17367358e-03,\n",
            "              -4.15452808e-01],\n",
            "             [-8.69108588e-02,  1.81707665e-01,  6.25307336e-02,\n",
            "               1.72883406e-01,  1.95584357e-01,  2.74430752e-01,\n",
            "              -1.92429751e-01,  1.47458285e-01,  7.26163149e-01,\n",
            "              -5.26730657e-01, -5.28793074e-02,  1.19812369e-01,\n",
            "              -4.14590463e-02, -5.22847846e-02, -2.10629054e-03,\n",
            "              -1.27047610e+00],\n",
            "             [ 6.48712039e-01,  2.45575845e-01, -2.17034757e-01,\n",
            "              -2.03469634e-01,  2.16548502e-01,  9.63221550e-01,\n",
            "              -2.88396418e-01,  5.61877847e-01, -6.29177466e-02,\n",
            "              -6.43660963e-01,  2.86378741e-01,  5.17968953e-01,\n",
            "              -3.54731351e-01,  4.41342235e-01, -1.55407801e-01,\n",
            "              -4.51745450e-01],\n",
            "             [ 3.16271074e-02,  5.85271604e-02,  4.62797284e-01,\n",
            "               4.10276324e-01, -3.19631755e-01,  5.38047612e-01,\n",
            "              -1.76559091e-01, -1.63280815e-01, -5.81681848e-01,\n",
            "               2.40244821e-01,  2.98069984e-01, -2.84662515e-01,\n",
            "               1.30376434e+00,  1.43645048e-01,  1.88088223e-01,\n",
            "              -3.76098365e-01],\n",
            "             [-6.32471085e-01,  4.15185720e-01, -7.23528743e-01,\n",
            "              -2.85524666e-01, -6.80228099e-02, -1.46104768e-01,\n",
            "              -4.81307656e-01, -2.10697293e-01, -2.47338310e-01,\n",
            "               2.05897912e-01, -5.70100963e-01,  5.19066036e-01,\n",
            "               1.90130726e-01, -5.77863157e-01,  2.71024436e-01,\n",
            "               2.92704105e-01],\n",
            "             [-5.10605395e-01, -5.12931228e-01, -2.60140933e-03,\n",
            "               1.33909937e-02, -5.70373423e-02, -5.93730360e-02,\n",
            "               6.40689850e-01, -2.03753129e-01,  3.07247370e-01,\n",
            "              -3.74214679e-01,  7.81105310e-02, -2.76993752e-01,\n",
            "              -3.90724182e-01, -4.64962482e-01,  1.03598046e+00,\n",
            "              -3.53580087e-01],\n",
            "             [-1.37970775e-01,  6.60031974e-01, -7.79909417e-02,\n",
            "               3.42094600e-01,  3.22617084e-01,  3.96113545e-01,\n",
            "              -3.36657137e-01,  6.74607381e-02, -1.60002083e-01,\n",
            "               2.64783472e-01,  2.82829977e-03,  8.89536515e-02,\n",
            "               6.59260690e-01, -4.37168598e-01, -5.86834967e-01,\n",
            "               3.59916747e-01],\n",
            "             [-8.62595998e-03,  4.47189063e-01, -5.45367599e-01,\n",
            "               1.11671545e-01,  4.57616478e-01, -3.47084224e-01,\n",
            "              -5.22286832e-01,  3.72609831e-02,  5.64116538e-01,\n",
            "               3.06643397e-01,  1.44670710e-01,  7.15358436e-01,\n",
            "               3.34410101e-01, -5.82510373e-03, -4.04770255e-01,\n",
            "              -2.70487159e-01],\n",
            "             [-2.77247518e-01, -1.87253222e-01, -1.07977502e-01,\n",
            "              -1.49679139e-01,  1.44336119e-01,  5.58932185e-01,\n",
            "               3.87399405e-01,  3.35475296e-01,  1.03543234e+00,\n",
            "              -3.59027386e-01, -2.62997925e-01, -1.62410334e-01,\n",
            "               1.08491570e-01, -3.04481655e-01,  1.65124148e-01,\n",
            "              -5.90754807e-01],\n",
            "             [-1.69225886e-01,  2.52289146e-01,  8.32955658e-01,\n",
            "              -2.79728979e-01, -5.06555513e-02,  1.85140759e-01,\n",
            "              -2.56109357e-01, -1.60899922e-01, -4.24049973e-01,\n",
            "               2.39269715e-02,  2.77708203e-01, -6.01407140e-02,\n",
            "               7.98687339e-01,  1.26210257e-01,  2.43259937e-01,\n",
            "              -1.90964043e-01],\n",
            "             [-2.05623861e-02,  2.43607342e-01,  1.29236236e-01,\n",
            "              -3.68874401e-01, -2.29174092e-01,  8.35391700e-01,\n",
            "              -2.50036091e-01,  2.64271140e-01,  2.35899925e-01,\n",
            "              -1.66612327e-01, -7.44540751e-01,  5.28120510e-02,\n",
            "               7.23410666e-01, -2.28175312e-01, -5.62698424e-01,\n",
            "               7.52865076e-02],\n",
            "             [ 1.06638718e+00, -4.19535667e-01,  1.99873611e-01,\n",
            "               4.76499707e-01, -1.00883090e+00, -6.89772367e-02,\n",
            "              -2.11705521e-01, -1.77894428e-01, -3.42342630e-02,\n",
            "              -3.81788388e-02,  2.57606179e-01, -5.63507736e-01,\n",
            "              -2.25027993e-01,  7.46569932e-01,  1.05132554e-02,\n",
            "              -3.81821901e-01],\n",
            "             [-1.41169742e-01,  2.79376786e-02,  7.30106711e-01,\n",
            "               9.51555312e-01, -1.25479266e-01, -2.42630184e-01,\n",
            "              -3.62837225e-01, -2.98940748e-01, -3.15640748e-01,\n",
            "              -6.55553909e-03,  5.36281884e-01, -2.66853690e-01,\n",
            "               3.32961857e-01, -6.87865257e-01, -4.82526511e-01,\n",
            "              -2.50430495e-01],\n",
            "             [-2.85135657e-02,  9.09271002e-01, -2.21610919e-01,\n",
            "               2.26597801e-01, -2.41860762e-01,  1.85027227e-01,\n",
            "              -1.12956285e+00,  4.68028367e-01, -1.43706784e-01,\n",
            "              -1.93489656e-01, -1.76816240e-01,  8.43573570e-01,\n",
            "              -2.81770639e-02,  1.28069118e-01, -2.14454338e-01,\n",
            "               5.16900681e-02],\n",
            "             [-2.71621138e-01,  5.43321252e-01, -1.53244048e-01,\n",
            "               2.41227627e-01,  6.77593350e-02,  7.93428659e-01,\n",
            "              -1.72782481e-01,  5.01956105e-01,  2.15237647e-01,\n",
            "              -7.99673200e-02,  9.63325977e-01,  6.00050807e-01,\n",
            "              -5.35637103e-02, -1.81685269e-01, -4.60228205e-01,\n",
            "               3.96576345e-01],\n",
            "             [ 5.37985146e-01, -2.04841331e-01, -1.85626015e-01,\n",
            "              -7.43646920e-02, -2.09383339e-01, -7.67014101e-02,\n",
            "               3.07006359e-01, -2.58825272e-01,  5.95700026e-01,\n",
            "              -4.84155416e-02, -1.97881341e-01, -1.95218757e-01,\n",
            "              -7.08734095e-01, -1.05363755e-02,  4.12606537e-01,\n",
            "              -8.61586154e-01],\n",
            "             [-6.67753339e-01,  3.33909214e-01,  7.09878743e-01,\n",
            "               2.19867423e-01, -4.45967495e-01, -2.44495437e-01,\n",
            "              -2.43202448e-01, -4.06617582e-01, -5.28896928e-01,\n",
            "              -3.19873333e-01,  5.90375781e-01,  1.30861074e-01,\n",
            "               1.88390747e-01, -5.32881200e-01, -2.21366629e-01,\n",
            "              -7.38479435e-01],\n",
            "             [ 2.19885051e-01, -3.85246366e-01,  8.40736747e-01,\n",
            "               6.16839468e-01,  2.66846567e-01, -5.72659791e-01,\n",
            "              -1.39991120e-01, -1.25843537e+00, -1.93348125e-01,\n",
            "              -2.39838541e-01,  4.39101309e-01, -4.51162249e-01,\n",
            "               4.89234664e-02, -7.39798188e-01, -8.60979497e-01,\n",
            "               2.39006393e-02],\n",
            "             [-1.65764987e-01,  4.19658929e-01,  2.87113190e-01,\n",
            "              -2.86069542e-01, -1.46076798e+00,  5.77407718e-01,\n",
            "               2.48532854e-02, -1.64221123e-01,  3.26452732e-01,\n",
            "              -1.46494925e+00, -3.34127136e-02,  2.13204801e-01,\n",
            "              -1.74643323e-01, -5.25897369e-02,  9.77311730e-02,\n",
            "               1.12081952e-01],\n",
            "             [-9.13190693e-02,  4.02491875e-02,  7.49633670e-01,\n",
            "              -4.54116374e-01,  1.90520734e-01,  6.02098525e-01,\n",
            "              -1.16323226e-03,  2.74051994e-01,  1.00207135e-01,\n",
            "              -3.39211136e-01,  1.59772664e-01,  5.52143514e-01,\n",
            "               4.39118415e-01,  1.77848525e-02, -6.03340864e-01,\n",
            "               3.76746923e-01],\n",
            "             [-4.96599376e-01,  3.00191436e-02,  7.06458271e-01,\n",
            "               4.36117679e-01, -1.03019282e-01, -8.52881465e-03,\n",
            "              -4.30873334e-01, -1.11984968e+00, -1.31415529e-02,\n",
            "              -1.23360880e-01,  3.58782381e-01, -7.45270848e-01,\n",
            "              -4.09212649e-01, -8.75313818e-01, -2.92771071e-01,\n",
            "              -2.39592969e-01],\n",
            "             [-5.81818283e-01, -4.01699692e-01,  3.87203187e-01,\n",
            "               4.39917505e-01, -7.70970345e-01,  2.73817271e-01,\n",
            "              -1.31642133e-01, -4.46091354e-01, -7.30928704e-02,\n",
            "              -1.16410947e+00,  2.41103381e-01, -6.40155196e-01,\n",
            "              -1.02140248e-01,  4.39644326e-04,  6.74077511e-01,\n",
            "               1.83640987e-01],\n",
            "             [ 2.30333712e-02, -7.95663297e-01,  3.67286778e-03,\n",
            "              -9.54742551e-01, -8.63797069e-01,  2.85256281e-02,\n",
            "               1.77917629e-01, -7.22715557e-01,  5.02839029e-01,\n",
            "              -7.72595644e-01,  1.03127152e-01, -7.30099201e-01,\n",
            "              -7.46669233e-01,  3.42030704e-01,  1.80077896e-01,\n",
            "               8.50967944e-01],\n",
            "             [-2.78804690e-01,  2.73208730e-02,  3.47027570e-01,\n",
            "              -1.17702141e-01, -1.90375730e-01,  6.27944767e-01,\n",
            "               8.56249556e-02,  6.34296060e-01, -3.48908931e-01,\n",
            "              -3.34306389e-01,  1.15794897e+00,  4.24323767e-01,\n",
            "              -2.03213701e-03,  8.41289610e-02, -4.46752399e-01,\n",
            "              -1.82625994e-01],\n",
            "             [ 2.65155017e-01, -6.40884399e-01,  2.29293108e-01,\n",
            "              -8.80239785e-01, -1.09496844e+00,  1.28047660e-01,\n",
            "              -6.38962984e-02, -9.54461336e-01,  7.68269956e-01,\n",
            "              -5.41483104e-01,  5.88242933e-02, -5.35535872e-01,\n",
            "              -8.32447410e-01,  2.76437663e-02, -9.81457978e-02,\n",
            "               8.91468406e-01]], dtype=float32)\n",
            "    )\n",
            "  },\n",
            "  'linear4': {\n",
            "    'bias': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([0.06066415], dtype=float32)\n",
            "    ),\n",
            "    'kernel': VariableState(\n",
            "      type=Param,\n",
            "      value=Array([[ 0.724013  ],\n",
            "             [-0.3480021 ],\n",
            "             [-0.72921216],\n",
            "             [-0.33587837],\n",
            "             [-0.4087932 ],\n",
            "             [-0.5497875 ],\n",
            "             [ 0.67321557],\n",
            "             [-0.40321776],\n",
            "             [ 0.6775908 ],\n",
            "             [-0.22549036],\n",
            "             [-0.6233978 ],\n",
            "             [-0.32746822],\n",
            "             [-0.5029055 ],\n",
            "             [ 0.5095721 ],\n",
            "             [ 0.5049215 ],\n",
            "             [ 0.49850965]], dtype=float32)\n",
            "    )\n",
            "  }\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newModel = MLP(10, 16, 32, 16, 1, rngs=nnx.Rngs(0))\n",
        "abstract_model = nnx.eval_shape(lambda: newModel)\n",
        "graphdef, abstract_state = nnx.split(abstract_model)\n",
        "print('The abstract NNX state (all leaves are abstract arrays):')\n",
        "#nnx.display(abstract_state)\n",
        "\n",
        "state_restored = checkpointer.restore(ckpt_dir / 'state', abstract_state)\n",
        "#nnx.display(state_restored['dp1'])\n",
        "prng_key_value = state[\"dp1\"][\"rngs\"][\"default\"][\"key\"].value\n",
        "state_restored[\"dp1\"][\"rngs\"][\"default\"][\"key\"].type(nnx.RngKey)\n",
        "state_restored[\"dp1\"][\"rngs\"][\"default\"][\"key\"].value = jax.random.wrap_key_data(prng_key_value)\n",
        "\"\"\"= nnx.VariableState(\n",
        "    type=nnx.RngKey, value=jax.random.wrap_key_data(prng_key_value), tag='default'\n",
        ")\"\"\"\n",
        "nnx.display(state_restored['dp1'])\n",
        "nnx.display(stateOrg['dp1'])\n",
        "jax.tree.map(np.testing.assert_array_equal, stateOrg, state_restored)\n",
        "print('NNX State restored: ')\n",
        "#nnx.display(state_restored)\n",
        "\n",
        "# The model is now good to use!\n",
        "newModel = nnx.merge(graphdef, state_restored)\n",
        "optimizer = nnx.Optimizer(newModel, optax.adam(learning_rate))\n",
        "metrics = nnx.MultiMetric(\n",
        "  loss=nnx.metrics.Average('loss'),\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sIjj6adSeQ88",
        "outputId": "7000ae03-470d-4038-eb6e-648af62cfc16"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The abstract NNX state (all leaves are abstract arrays):\n",
            "State({\n",
            "  'rngs': {\n",
            "    'default': {\n",
            "      'count': VariableState(\n",
            "        type=RngCount,\n",
            "        value=Array(24116, dtype=uint32),\n",
            "        tag='default'\n",
            "      ),\n",
            "      'key': VariableState(\n",
            "        type=RngKey,\n",
            "        value=Array((), dtype=key<fry>) overlaying:\n",
            "        [0 0],\n",
            "        tag='default'\n",
            "      )\n",
            "    }\n",
            "  }\n",
            "})\n",
            "State({\n",
            "  'rngs': {\n",
            "    'default': {\n",
            "      'count': VariableState(\n",
            "        type=RngCount,\n",
            "        value=Array(24116, dtype=uint32),\n",
            "        tag='default'\n",
            "      ),\n",
            "      'key': VariableState(\n",
            "        type=RngKey,\n",
            "        value=Array((), dtype=key<fry>) overlaying:\n",
            "        [0 0],\n",
            "        tag='default'\n",
            "      )\n",
            "    }\n",
            "  }\n",
            "})\n",
            "NNX State restored: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/orbax/checkpoint/type_handlers.py:1330: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'train_accuracy': [],\n",
        "  'train_precision': [],\n",
        "  'train_recall': [],\n",
        "  'train_f1': [],\n",
        "  'val_loss': [],\n",
        "  'val_accuracy': [],\n",
        "  'val_precision': [],\n",
        "  'val_recall': [],\n",
        "  'val_f1': [],\n",
        "  'test_loss': [],\n",
        "  'test_accuracy': [],\n",
        "  'test_precision': [],\n",
        "  'test_recall': [],\n",
        "  'test_f1': [],\n",
        "}\n",
        "\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_ds:\n",
        "      loss, logits = train_step(newModel, optimizer, metrics, batch)\n",
        "\n",
        "    for metric, value in metrics.compute().items():\n",
        "      metrics_history[f'train_{metric}'].append(value)\n",
        "\n",
        "    metrics.reset()\n",
        "    print(\n",
        "      f\"[train] epoch: {epoch}, \"\n",
        "      f\"loss: {metrics_history['train_loss'][-1]}, \"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Ukn5kWrwut",
        "outputId": "dc636748-b4a7-49a9-f5f8-6b6ac1af97b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] epoch: 0, loss: 0.7126048803329468, \n",
            "[train] epoch: 1, loss: 0.6890543699264526, \n",
            "[train] epoch: 2, loss: 0.6823462843894958, \n",
            "[train] epoch: 3, loss: 0.6804338097572327, \n",
            "[train] epoch: 4, loss: 0.6683182120323181, \n",
            "[train] epoch: 5, loss: 0.6706789135932922, \n",
            "[train] epoch: 6, loss: 0.6680969595909119, \n",
            "[train] epoch: 7, loss: 0.6665178537368774, \n",
            "[train] epoch: 8, loss: 0.6604345440864563, \n",
            "[train] epoch: 9, loss: 0.6601880192756653, \n",
            "[train] epoch: 10, loss: 0.6607267260551453, \n",
            "[train] epoch: 11, loss: 0.6510515213012695, \n",
            "[train] epoch: 12, loss: 0.652760922908783, \n",
            "[train] epoch: 13, loss: 0.6461787819862366, \n",
            "[train] epoch: 14, loss: 0.6518374085426331, \n",
            "[train] epoch: 15, loss: 0.6428118348121643, \n",
            "[train] epoch: 16, loss: 0.6472353339195251, \n",
            "[train] epoch: 17, loss: 0.6407294273376465, \n",
            "[train] epoch: 18, loss: 0.6446413993835449, \n",
            "[train] epoch: 19, loss: 0.6374732851982117, \n",
            "[train] epoch: 20, loss: 0.640681266784668, \n",
            "[train] epoch: 21, loss: 0.6344079375267029, \n",
            "[train] epoch: 22, loss: 0.640116810798645, \n",
            "[train] epoch: 23, loss: 0.6415828466415405, \n",
            "[train] epoch: 24, loss: 0.6338638067245483, \n",
            "[train] epoch: 25, loss: 0.6351819634437561, \n",
            "[train] epoch: 26, loss: 0.6320218443870544, \n",
            "[train] epoch: 27, loss: 0.6283389329910278, \n",
            "[train] epoch: 28, loss: 0.6297717094421387, \n",
            "[train] epoch: 29, loss: 0.627047598361969, \n",
            "[train] epoch: 30, loss: 0.6283566355705261, \n",
            "[train] epoch: 31, loss: 0.6271138191223145, \n",
            "[train] epoch: 32, loss: 0.6277850270271301, \n",
            "[train] epoch: 33, loss: 0.624703586101532, \n",
            "[train] epoch: 34, loss: 0.6311822533607483, \n",
            "[train] epoch: 35, loss: 0.6265837550163269, \n",
            "[train] epoch: 36, loss: 0.6257098317146301, \n",
            "[train] epoch: 37, loss: 0.6211504936218262, \n",
            "[train] epoch: 38, loss: 0.6297398209571838, \n",
            "[train] epoch: 39, loss: 0.6207391619682312, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#newModel.eval() # Switch to evaluation mode.\n",
        "\n",
        "@nnx.jit\n",
        "def pred_step(model: MLP, batch):\n",
        "  logits = model(batch['features'])\n",
        "  return logits\n",
        "\n",
        "test_ds = DataLoader(test_set, batch_size=32, shuffle=False, drop_last=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "ypred = []\n",
        "label = []\n",
        "for test_batch in test_ds:\n",
        "  logits = pred_step(newModel, test_batch)\n",
        "  #print(np.ravel(logits))\n",
        "  #print(np.ravel(logits))\n",
        "  #break\n",
        "  ypred.extend(np.ravel(logits))\n",
        "  label.extend(np.ravel(test_batch[\"labels\"]))\n",
        "  #print(logits, test_batch[\"labels\"])\n",
        "  #break\n",
        "binary_ypred = np.where(np.array(ypred) > 0.5, 1, 0)\n",
        "print(len(label), sum(label))\n",
        "accuracy = sum([1 for pred, true in zip(binary_ypred, label) if pred == true]) / len(label)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqlRLOx02910",
        "outputId": "8441f6fb-065d-492c-e248-b80f6a334a05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3616 1917\n",
            "Accuracy: 0.9635\n"
          ]
        }
      ]
    }
  ]
}