{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJYO4z0GXMm9vMYfcqvbnf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenh17/jaxLearning/blob/main/mlpMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OBZVTh4I_ZuW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "from jax.scipy.special import logsumexp\n",
        "import jax\n",
        "from jax import jit, vmap, pmap, grad, value_and_grad\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_img_size = (28, 28)\n",
        "def mlp_model_initialization(parent_key, layer_widths):\n",
        "    params =[]\n",
        "    keys = jax.random.split(parent_key, num=len(layer_widths)-1)\n",
        "\n",
        "    for n_in, n_out, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weights_key, bias_key = jax.random.split(key)\n",
        "        weights = jax.random.normal(weights_key, (n_in, n_out))* jnp.sqrt(2 / n_in)\n",
        "        biases = jax.random.normal(bias_key, (n_out,))\n",
        "        params.append(dict(weights=weights, biases=biases))\n",
        "    return params\n",
        "\n",
        "# test\n",
        "key = jax.random.PRNGKey(0)\n",
        "mlp_params = mlp_model_initialization(key, [784, 512, 512, 10])\n",
        "print(jax.tree.map(lambda x: x.shape, mlp_params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U66SyG1p_ty_",
        "outputId": "e53894f9-3b79-468c-85ae-43cefa99b719"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'biases': (512,), 'weights': (784, 512)}, {'biases': (512,), 'weights': (512, 512)}, {'biases': (10,), 'weights': (512, 10)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_forward(params, x):\n",
        "    *hidden_layers, last_layer = params\n",
        "    for layer in hidden_layers:\n",
        "        x = jax.nn.relu(jnp.dot(x, layer['weights']) + layer['biases'])\n",
        "    logits = jax.nn.softmax(jnp.dot(x, last_layer['weights']) + last_layer['biases'])\n",
        "    return logits\n",
        "\n",
        "# tests\n",
        "\n",
        "# test single example\n",
        "\n",
        "dummy_img_flat = np.random.randn(np.prod(mnist_img_size))\n",
        "print(dummy_img_flat.shape)\n",
        "\n",
        "prediction = mlp_forward(mlp_params, dummy_img_flat)\n",
        "print(prediction.shape)\n",
        "\n",
        "# test batched function\n",
        "batched_MLP_predict = vmap(mlp_forward, in_axes=(None, 0))\n",
        "\n",
        "dummy_imgs_flat = np.random.randn(16, np.prod(mnist_img_size))\n",
        "print(dummy_imgs_flat.shape)\n",
        "predictions = batched_MLP_predict(mlp_params, dummy_imgs_flat)\n",
        "print(predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SpAXLUcB_ww",
        "outputId": "4bd9a4b0-a7ab-49df-a467-831f8a6376d3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784,)\n",
            "(10,)\n",
            "(16, 784)\n",
            "(16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_transform(x):\n",
        "    return np.ravel(np.array(x, dtype=np.float32))\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    transposed_data = list(zip(*batch))\n",
        "\n",
        "    labels = np.array(transposed_data[1])\n",
        "    imgs = np.array(transposed_data[0])\n",
        "\n",
        "    return imgs, labels\n",
        "\n",
        "batch_size = 128\n",
        "train_dataset = MNIST(root='train_mnist', train=True, download=True, transform=custom_transform)\n",
        "test_dataset = MNIST(root='test_mnist', train=False, download=True, transform=custom_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn, drop_last=True)\n",
        "\n",
        "# test\n",
        "batch_data = next(iter(train_loader))\n",
        "imgs = batch_data[0]\n",
        "lbls = batch_data[1]\n",
        "print(imgs.shape, imgs[0].dtype, lbls.shape, lbls[0].dtype)\n",
        "\n",
        "# Loading the whole dataset into memory\n",
        "train_images = jnp.array(train_dataset.data).reshape(len(train_dataset), -1)\n",
        "train_lbls = jnp.array(train_dataset.targets)\n",
        "\n",
        "test_images = jnp.array(test_dataset.data).reshape(len(test_dataset), -1)\n",
        "test_lbls = jnp.array(test_dataset.targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqpHgphdkbYu",
        "outputId": "062f7953-befb-4fa3-e2d3-e4f1965c3111"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 784) float32 (128,) int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(params, imgs, lbls):\n",
        "    logits = batched_MLP_predict(params, imgs)\n",
        "    logits = jnp.clip(logits, 1e-10, 1-1e-10)\n",
        "    print(logits.shape[-1])\n",
        "    labels_one_hot = jax.nn.one_hot(lbls, logits.shape[-1])\n",
        "    #q = jnp.max(logits*labels_one_hot, axis=1)\n",
        "    #print(q.shape)\n",
        "    loss = -jnp.mean((jnp.log(logits)*labels_one_hot))\n",
        "    #loss = -jnp.mean(jnp.log(jnp.max(labels_one_hot * logits)))\n",
        "    #loss = -jnp.mean(jnp.max(labels_one_hot * logits))\n",
        "    #print(loss)\n",
        "    #loss = -jnp.mean(jnp.sum(jnp.log(logits)*labels_one_hot, axis=1))\n",
        "    return loss, logits\n",
        "\n",
        "def accuracy(params, dataset_imgs, dataset_lbls):\n",
        "    pred_classes = jnp.argmax(batched_MLP_predict(params, dataset_imgs), axis=1)\n",
        "    return jnp.mean(dataset_lbls == pred_classes)\n",
        "@jit\n",
        "def update(params, imgs, lbls, lr=0.01):\n",
        "    (loss, logits), grads = value_and_grad(loss_fn, has_aux=True)(params, imgs, lbls)\n",
        "    return loss,logits, jax.tree.map(lambda p,g: p- lr*g, params, grads)\n",
        "\n",
        "# Create a MLP\n",
        "MLP_params = mlp_model_initialization(key, [np.prod(mnist_img_size), 512, 256, len(MNIST.classes)])\n",
        "\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for cnt, (imgs, lbls) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "        loss, logits, MLP_params = update(MLP_params, imgs, lbls)\n",
        "\n",
        "        #if cnt % 400 == 0:\n",
        "    print(loss)\n",
        "\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch}, train acc = {accuracy(MLP_params, train_images, train_lbls)} test acc = {accuracy(MLP_params, test_images, test_lbls)}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnEP5CswpHfV",
        "outputId": "53eef9e3-55fb-467e-e357-09694788fe33"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0.79526097\n",
            "Epoch 0, train acc = 0.613099992275238 test acc = 0.6128000020980835\n",
            "0.68373513\n",
            "Epoch 1, train acc = 0.6370333433151245 test acc = 0.6345999836921692\n",
            "0.9147736\n",
            "Epoch 2, train acc = 0.6436333656311035 test acc = 0.6358000040054321\n",
            "0.9293503\n",
            "Epoch 3, train acc = 0.6506500244140625 test acc = 0.64410001039505\n",
            "0.7195587\n",
            "Epoch 4, train acc = 0.6570667028427124 test acc = 0.6502000093460083\n",
            "0.8274972\n",
            "Epoch 5, train acc = 0.6597000360488892 test acc = 0.6488999724388123\n",
            "0.85227454\n",
            "Epoch 6, train acc = 0.6636833548545837 test acc = 0.6539999842643738\n",
            "0.7735247\n",
            "Epoch 7, train acc = 0.6659833192825317 test acc = 0.6565999984741211\n",
            "0.5037578\n",
            "Epoch 8, train acc = 0.7561833262443542 test acc = 0.7479999661445618\n",
            "0.48337212\n",
            "Epoch 9, train acc = 0.7600666880607605 test acc = 0.7487999796867371\n",
            "0.55207837\n",
            "Epoch 10, train acc = 0.7646833658218384 test acc = 0.7519999742507935\n",
            "0.59893435\n",
            "Epoch 11, train acc = 0.7681833505630493 test acc = 0.7565999627113342\n",
            "0.58268565\n",
            "Epoch 12, train acc = 0.7669000029563904 test acc = 0.7537999749183655\n",
            "0.66559094\n",
            "Epoch 13, train acc = 0.7699166536331177 test acc = 0.7549999952316284\n",
            "0.2686163\n",
            "Epoch 14, train acc = 0.859000027179718 test acc = 0.8531999588012695\n",
            "0.2274384\n",
            "Epoch 15, train acc = 0.8772166967391968 test acc = 0.864799976348877\n",
            "0.27994236\n",
            "Epoch 16, train acc = 0.8784500360488892 test acc = 0.8667999505996704\n",
            "0.28782317\n",
            "Epoch 17, train acc = 0.883983314037323 test acc = 0.8693000078201294\n",
            "0.35977894\n",
            "Epoch 18, train acc = 0.8834333419799805 test acc = 0.8685999512672424\n",
            "0.20048852\n",
            "Epoch 19, train acc = 0.8851667046546936 test acc = 0.8686999678611755\n"
          ]
        }
      ]
    }
  ]
}